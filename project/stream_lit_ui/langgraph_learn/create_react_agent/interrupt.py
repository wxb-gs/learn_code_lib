from typing import TypedDict, Annotated, Sequence
from langgraph.graph import StateGraph, END, START
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver
from langchain.tools import tool
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, BaseMessage
from langchain_ollama import ChatOllama
# ===== 1. å®šä¹‰çŠ¶æ€ç»“æ„ =====


class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], "å¯¹è¯æ¶ˆæ¯å†å²"]
    user_query: str

# ===== 2. å®šä¹‰å·¥å…·ï¼ˆæ¨¡æ‹Ÿé«˜é£é™©APIï¼‰=====


@tool
def search_weather_api(city: str) -> str:
    """è°ƒç”¨å¤–éƒ¨å¤©æ°”APIï¼ˆæ¨¡æ‹Ÿé«˜é£é™©æ“ä½œï¼‰"""
    # å®é™…åœºæ™¯éœ€æ›¿æ¢ä¸ºçœŸå®APIè°ƒç”¨
    print(f"âš ï¸ æ¨¡æ‹Ÿè°ƒç”¨å¤©æ°”API: {city}")
    return f"{city}å¤©æ°”æ™´æœ—ï¼Œ25â„ƒ"

# ===== 3. æ„å»ºä»£ç†èŠ‚ç‚¹ =====


def agent_node(state: AgentState):
    """ä»£ç†å†³ç­–èŠ‚ç‚¹"""
    llm = ChatOllama(model="qwen2.5:7b")
    response = llm.invoke(state["messages"])
    return {"messages": [response]}

# ===== 4. äººå·¥å®¡æ ¸èŠ‚ç‚¹ =====


def human_approval_node(state: AgentState):
    """åœ¨è°ƒç”¨APIå‰æš‚åœå¹¶ç­‰å¾…äººå·¥ç¡®è®¤"""
    last_msg = state["messages"][-1].content
    print(f"ğŸ” éœ€äººå·¥å®¡æ ¸çš„æ“ä½œ: {last_msg}")

    # æ¨¡æ‹Ÿäººå·¥è¾“å…¥ï¼ˆå®é™…å¯æ›¿æ¢ä¸ºWebç•Œé¢ï¼‰
    approval = input("æ˜¯å¦å…è®¸æ‰§è¡Œï¼Ÿ(y/n): ").strip().lower()
    if approval == "y":
        print("âœ… æ“ä½œå·²æ‰¹å‡†")
        return {"messages": [HumanMessage(content="ç»§ç»­æ‰§è¡Œ")]}
    else:
        print("âŒ æ“ä½œå·²æ‹’ç»")
        return {"messages": [HumanMessage(content="å–æ¶ˆæ“ä½œ")]}


# ===== 5. æ„å»ºçŠ¶æ€å›¾ =====
builder = StateGraph(AgentState)

# æ·»åŠ èŠ‚ç‚¹
builder.add_node("agent", agent_node)
builder.add_node("tools", ToolNode([search_weather_api]))  # å·¥å…·è°ƒç”¨èŠ‚ç‚¹
builder.add_node("human_approval", human_approval_node)     # äººå·¥å®¡æ ¸èŠ‚ç‚¹

# è®¾ç½®è¾¹å…³ç³»
builder.add_edge("agent", "human_approval")  # ä»£ç†å†³ç­–åè¿›å…¥å®¡æ ¸
builder.add_conditional_edges(
    "human_approval",
    lambda state: "tools" if "ç»§ç»­æ‰§è¡Œ" in state["messages"][-1].content else "agent",
)  # æ ¹æ®å®¡æ ¸ç»“æœè·³è½¬
builder.add_edge("tools", "agent")           # å·¥å…·æ‰§è¡Œåè¿”å›ä»£ç†
builder.add_edge("agent", END)              # æœ€ç»ˆç»“æŸ
builder.add_edge(START, "agent")
# å¯ç”¨æ£€æŸ¥ç‚¹ï¼ˆæ”¯æŒä¸­æ–­æ¢å¤ï¼‰
checkpointer = MemorySaver()
config = {
    "thread_id": "123"
}
graph = builder.compile(checkpointer=checkpointer,
                        interrupt_before=["tools"])

# ===== 6. æ‰§è¡Œä»£ç† =====
if __name__ == "__main__":
    # åˆå§‹åŒ–è¾“å…¥
    inputs = {
        "messages": [HumanMessage(content="æŸ¥è¯¢çº½çº¦å¤©æ°”")],
        "user_query": "çº½çº¦å¤©æ°”"
    }

    # æµå¼æ‰§è¡Œå¹¶æ‰“å°è¿‡ç¨‹
    print("ğŸš€ å¼€å§‹æ‰§è¡Œä»£ç†æµç¨‹...")
    for step in graph.stream(inputs, stream_mode="values", config=config):
        msg = step["messages"][-1]
        print(f"ğŸ“¢ {type(msg).__name__}: {msg.content}")
