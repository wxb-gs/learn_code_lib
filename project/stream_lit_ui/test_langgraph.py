import streamlit as st
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts.prompt import PromptTemplate
from langchain_ollama import ChatOllama
from langchain_core.messages import trim_messages
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, END, StateGraph, MessagesState


# ------------------ Prompt æ¨¡æ¿ ------------------
template = """
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½é—®ç­”æœºå™¨äººï¼Œè¯·ç»“åˆä»¥ä¸‹å†å²å¯¹è¯æ¶ˆæ¯å›ç­”æˆ‘çš„é—®é¢˜ï¼š
{chat_history}
é—®é¢˜ï¼š{question}

è¯·æä¾›æ­£ç¡®ã€è¯¦ç»†æ¸…æ™°çš„å›ç­”ã€‚
"""
chat_prompt_template = PromptTemplate.from_template(template)


# ------------------ æ¶ˆæ¯å¤„ç† ------------------
def process_messages(state) -> dict:
    trimmed = trim_messages(
        state["messages"],
        max_tokens=16,
        strategy="last",
        start_on="human",
        token_counter=len,
    )
    last_msg = trimmed[-1].content if trimmed else ""
    chat_history = "\n".join([m.pretty_repr() for m in trimmed[:-1]])

    prompt = chat_prompt_template.format(
        chat_history=chat_history,
        question=last_msg,
    )
    return {"messages": [HumanMessage(content=prompt)]}


# ------------------ LangGraph Agent ------------------
class ChatBot:
    def __init__(self, model_name="qwen2.5:7b"):
        self.llm = ChatOllama(model=model_name, temperature=0.7)
        self.app = self.define()

    def stream_model(self, state: MessagesState):
        for chunk in self.llm.stream(state["messages"]):
            if chunk.content:
                yield {"messages": [AIMessage(content=chunk.content)]}

    def define(self):
        graph = StateGraph(MessagesState)
        graph.add_node("before", process_messages)
        graph.add_node("model", self.stream_model)
        graph.add_edge(START, "before")
        graph.add_edge("before", "model")
        graph.add_edge("model", END)
        return graph.compile(checkpointer=MemorySaver())

    def answer_updates(self, question: str, thread_id: str = "default"):
        config = {"configurable": {"thread_id": thread_id}}
        inputs = {"messages": [HumanMessage(content=question)]}
        return self.app.stream(input=inputs, config=config, stream_mode="updates")


# ------------------ Streamlit UI ------------------

st.set_page_config(page_title="RAG èŠå¤©æœºå™¨äºº", layout="centered")
st.title("ğŸ§  RAG æ™ºèƒ½é—®ç­”æœºå™¨äºº")
st.markdown("é—®æˆ‘ä»»ä½•é—®é¢˜ï¼Œæˆ‘å°†ç»“åˆä¸Šä¸‹æ–‡ä¸ºä½ è§£ç­”ï¼")

# ä¼šè¯çŠ¶æ€åˆå§‹åŒ–
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

bot = ChatBot()

# ç”¨æˆ·è¾“å…¥
question = st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜...")

# å±•ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state.chat_history:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# æäº¤æ–°é—®é¢˜
if question:
    # æ˜¾ç¤ºç”¨æˆ·é—®é¢˜
    st.chat_message("user").markdown(question)
    st.session_state.chat_history.append({"role": "user", "content": question})

    # æ˜¾ç¤º AI å›å¤æ¡†
    with st.chat_message("assistant"):
        placeholder = st.empty()
        full_response = ""

        for update in bot.answer_updates(question, thread_id="streamlit"):
            for msg in update.get("messages", []):
                if isinstance(msg, AIMessage):
                    full_response += msg.content
                    placeholder.markdown(full_response + "â–Œ")  # åŠ¨æ€è¾“å‡º

        placeholder.markdown(full_response)  # ç§»é™¤æ¸¸æ ‡
        st.session_state.chat_history.append(
            {"role": "assistant", "content": full_response})
