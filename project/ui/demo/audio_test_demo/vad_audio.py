import sys
import time
import threading
import torch
import numpy as np
import pyaudio
import wave
from faster_whisper import WhisperModel
from PyQt5.QtWidgets import *
from PyQt5.QtCore import *
from PyQt5.QtGui import *

class VAD_Whisper(QObject):
    # å®šä¹‰ä¿¡å·
    text_recognized = pyqtSignal(str)
    recording_started = pyqtSignal()
    recording_stopped = pyqtSignal()
    
    def __init__(self, model_size="small", beam_size=5, language='zh', 
                 initial_prompt="è¿™æ˜¯ä¸€æ¡æŽ§åˆ¶æŒ‡ä»¤ï¼ŒAæœºAæœºï¼ŒBæœºBæœºï¼Œå¹³é£žï¼Œåç½®é£žè¡Œï¼Œç›´çº¿åŠ é€Ÿï¼Œå°è§’åº¦ç›˜æ—‹ï¼Œæ‹‰èµ·å‘å°„ï¼Œä¿¯ä»°è§’10åº¦ï¼Œé€Ÿåº¦100é©¬èµ«ã€‚"):
        super().__init__()
        self.model_size = model_size
        self.beam_size = beam_size
        self.language = language
        self.initial_prompt = initial_prompt
        self.condition_on_previous_text = True
        
        # åŠ è½½æ¨¡åž‹å’Œå·¥å…·å‡½æ•°
        try:
            self.model, self.utils = torch.hub.load(
                repo_or_dir='snakers4',
                model='silero_vad',
                trust_repo=None,
                source='local'
            )
        except:
            print("VADæ¨¡åž‹åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")
            self.model = None
            
        self.running = False
        self.pause = False
        self.audio_paths = "./current_speech.wav"
        self.frame_size = 512
        self.confidence_threshold = 0.8
        self.countSize = 0
        self.record_size = 0
        self.last_audio_chunk_list = []
        self.record_button_clicked = False
        self.is_recording = False
        self.is_activing = False
        self.init_finish = False
        
        # åˆå§‹åŒ–PyAudio
        self.audio = pyaudio.PyAudio()
        try:
            self.stream = self.audio.open(format=pyaudio.paInt16,
                                        channels=1,
                                        rate=16000,
                                        input=True,
                                        frames_per_buffer=self.frame_size)
        except Exception as e:
            print(f"Error opening audio stream: {e}")
            self.stream = None

    def Init_model(self):
        try:
            if self.model_size in ["small","whisper-small","whisper-medium"]:
                self.whisper_model = WhisperModel(self.model_size, device="cuda")
            elif self.model_size in ["large-v3-turbo-ct2"]:
                self.whisper_model = WhisperModel(self.model_size, device="cuda", compute_type="float16")
            elif self.model_size == "whisper-small-ct2-int8":
                self.whisper_model = WhisperModel(self.model_size, device="cuda", compute_type="int8_float16")
            else:
                # ä½¿ç”¨CPUç‰ˆæœ¬ä½œä¸ºå¤‡é€‰
                self.whisper_model = WhisperModel("base", device="cpu")
            self.init_finish = True
        except Exception as e:
            print(f"æ¨¡åž‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.init_finish = False

    def int2float(self, sound):
        abs_max = np.abs(sound).max()
        sound = sound.astype('float32')
        if abs_max > 0:
            sound /= abs_max
        sound = sound.clip(-1, 1)
        return sound
    
    def save_audio_custom(self, audio):
        with wave.open(self.audio_paths, 'wb') as wf:
            wf.setnchannels(1)
            wf.setsampwidth(2)
            wf.setframerate(16000)
            wf.writeframes(audio)

    def audio2text(self, audio):
        if not self.init_finish:
            return "æ¨¡åž‹æœªåˆå§‹åŒ–"
            
        try:
            speech_text = None
            segments, info = self.whisper_model.transcribe(
                audio, 
                beam_size=self.beam_size, 
                language=self.language, 
                initial_prompt=self.initial_prompt, 
                condition_on_previous_text=self.condition_on_previous_text
            )
            
            for segment in segments:
                text_ = segment.text
                if speech_text is None:
                    speech_text = text_
                else:
                    speech_text += ", " + text_
                    
            if speech_text is None:
                return None
            else:
                speech_text = speech_text.replace(" ", "ï¼Œ")
                return speech_text
        except Exception as e:
            print(f"è¯­éŸ³è¯†åˆ«é”™è¯¯: {e}")
            return f"è¯†åˆ«å¤±è´¥: {str(e)}"
        
    def start_recording(self):
        self.record_button_clicked = True
        self.is_recording = True
        self.recording_started.emit()
        
    def stop_recording(self):
        self.record_button_clicked = False
        self.recording_stopped.emit()
        
    def run(self):
        if not self.stream:
            return
            
        self.running = True
        self.pause = False
        self.audio_data = None
        self.is_recording = False
        self.is_activing = False
        self.start_time = 0
        self.record_size = 0
        self.Init_model()
        
        while self.running:
            if self.pause:
                time.sleep(0.1)
                continue
                
            try:
                audio_chunk = self.stream.read(self.frame_size, exception_on_overflow=False)
            except:
                continue
                
            if not self.record_button_clicked and self.model:
                audio_int16 = np.frombuffer(audio_chunk, np.int16)
                audio_float32 = self.int2float(audio_int16)
                new_confidence = self.model(torch.from_numpy(audio_float32), 16000).item()
                
                if new_confidence > self.confidence_threshold:
                    if not self.is_recording:
                        self.is_recording = True
                        self.recording_started.emit()
                    else:
                        self.countSize = 0
                elif self.is_recording:
                    if self.countSize == 0:
                        self.start_time = time.time()
                    self.countSize += 1
                    if self.audio_data is not None and self.countSize >= 20:
                        self.is_recording = False
                        self.recording_stopped.emit()

            if self.is_recording:
                if self.audio_data is None:
                    self.audio_data = b''.join(self.last_audio_chunk_list)
                    self.audio_data += audio_chunk
                else:
                    self.audio_data += audio_chunk
                self.record_size += 1
            elif self.audio_data is not None:
                if self.record_size > 25:
                    self.save_audio_custom(self.audio_data)
                    speech_text = self.audio2text(self.audio_paths)
                    if speech_text:
                        self.text_recognized.emit(speech_text)
                else:
                    self.text_recognized.emit("å½•éŸ³å¤ªçŸ­ï¼Œè¯·é‡æ–°å½•åˆ¶")
                    
                self.audio_data = None
                self.countSize = 0
                self.record_size = 0

            self.last_audio_chunk_list.append(audio_chunk)
            if len(self.last_audio_chunk_list) > 20:
                self.last_audio_chunk_list.pop(0)

    def stop(self):
        self.running = False
        if self.stream:
            self.stream.stop_stream()
            self.stream.close()
        self.audio.terminate()


class PulseButton(QPushButton):
    def __init__(self, parent=None):
        super().__init__(parent)
        self.setFixedSize(80, 80)
        self.is_recording = False
        self.pulse_timer = QTimer()
        self.pulse_timer.timeout.connect(self.update)
        self.pulse_value = 0
        self.pulse_direction = 1
        
    def start_pulse(self):
        self.is_recording = True
        self.pulse_timer.start(50)
        
    def stop_pulse(self):
        self.is_recording = False
        self.pulse_timer.stop()
        self.pulse_value = 0
        self.update()
        
    def paintEvent(self, event):
        painter = QPainter(self)
        painter.setRenderHint(QPainter.Antialiasing)
        
        # è®¡ç®—è„‰å†²æ•ˆæžœ
        if self.is_recording:
            self.pulse_value += self.pulse_direction * 5
            if self.pulse_value >= 30:
                self.pulse_direction = -1
            elif self.pulse_value <= 0:
                self.pulse_direction = 1
        
        # ç»˜åˆ¶å¤–åœˆï¼ˆè„‰å†²æ•ˆæžœï¼‰
        if self.is_recording:
            pulse_color = QColor(74, 144, 226, 100 - self.pulse_value)
            painter.setBrush(QBrush(pulse_color))
            painter.setPen(Qt.NoPen)
            painter.drawEllipse(self.rect().adjusted(-self.pulse_value//2, -self.pulse_value//2, 
                                                  self.pulse_value//2, self.pulse_value//2))
        
        # ç»˜åˆ¶ä¸»æŒ‰é’®
        main_color = QColor(239, 68, 68) if self.is_recording else QColor(74, 144, 226)
        painter.setBrush(QBrush(main_color))
        painter.setPen(Qt.NoPen)
        painter.drawEllipse(self.rect().adjusted(10, 10, -10, -10))
        
        # ç»˜åˆ¶éº¦å…‹é£Žå›¾æ ‡
        painter.setPen(QPen(Qt.white, 2))
        painter.setBrush(Qt.NoBrush)
        center = self.rect().center()
        
        # éº¦å…‹é£Žèº«ä½“
        mic_rect = QRect(center.x() - 8, center.y() - 15, 16, 20)
        painter.drawRoundedRect(mic_rect, 8, 8)
        
        # éº¦å…‹é£Žåº•åº§
        painter.drawLine(center.x(), center.y() + 8, center.x(), center.y() + 18)
        painter.drawLine(center.x() - 8, center.y() + 18, center.x() + 8, center.y() + 18)
        
        # å½•éŸ³æ—¶æ˜¾ç¤ºåœæ­¢å›¾æ ‡
        if self.is_recording:
            painter.setBrush(QBrush(Qt.white))
            stop_rect = QRect(center.x() - 6, center.y() - 6, 12, 12)
            painter.drawRect(stop_rect)


class ChatBubble(QWidget):
    def __init__(self, text, is_user=True, parent=None):
        super().__init__(parent)
        self.text = text
        self.is_user = is_user
        self.setMinimumHeight(60)
        
        # è®¾ç½®å¸ƒå±€
        layout = QHBoxLayout(self)
        layout.setContentsMargins(20, 10, 20, 10)
        
        if is_user:
            layout.addStretch()
            
        # åˆ›å»ºæ¶ˆæ¯æ°”æ³¡
        bubble = QLabel(text)
        bubble.setWordWrap(True)
        bubble.setStyleSheet(f"""
            QLabel {{
                background-color: {'#4A90E2' if is_user else '#f0f0f0'};
                color: {'white' if is_user else 'black'};
                padding: 12px 16px;
                border-radius: 18px;
                font-size: 14px;
                max-width: 300px;
            }}
        """)
        
        layout.addWidget(bubble)
        
        if not is_user:
            layout.addStretch()


class LoadingWidget(QWidget):
    def __init__(self, parent=None):
        super().__init__(parent)
        self.setFixedSize(60, 30)
        self.timer = QTimer()
        self.timer.timeout.connect(self.update)
        self.dots = 0
        
    def start_animation(self):
        self.timer.start(500)
        self.show()
        
    def stop_animation(self):
        self.timer.stop()
        self.hide()
        
    def paintEvent(self, event):
        painter = QPainter(self)
        painter.setRenderHint(QPainter.Antialiasing)
        
        # ç»˜åˆ¶ä¸‰ä¸ªè·³åŠ¨çš„ç‚¹
        colors = [QColor(150, 150, 150), QColor(100, 100, 100), QColor(150, 150, 150)]
        
        for i in range(3):
            color = colors[i] if (self.dots % 4) != i else QColor(74, 144, 226)
            painter.setBrush(QBrush(color))
            painter.setPen(Qt.NoPen)
            x = 10 + i * 15
            y = 15 + (5 if (self.dots % 4) == i else 0)
            painter.drawEllipse(x, y, 8, 8)
            
        self.dots += 1


class VoiceChatApp(QMainWindow):
    def __init__(self):
        super().__init__()
        self.init_ui()
        self.init_voice_system()
        
    def init_ui(self):
        self.setWindowTitle("è¯­éŸ³åŠ©æ‰‹ - ChatGPTé£Žæ ¼")
        self.setGeometry(100, 100, 400, 600)
        self.setStyleSheet("""
            QMainWindow {
                background-color: #ffffff;
            }
            QScrollArea {
                border: none;
                background-color: #ffffff;
            }
            QScrollBar:vertical {
                border: none;
                background: #f0f0f0;
                width: 8px;
                border-radius: 4px;
            }
            QScrollBar::handle:vertical {
                background: #c0c0c0;
                border-radius: 4px;
                min-height: 20px;
            }
            QScrollBar::handle:vertical:hover {
                background: #a0a0a0;
            }
        """)
        
        # ä¸­å¤®çª—å£
        central_widget = QWidget()
        self.setCentralWidget(central_widget)
        
        # ä¸»layout
        main_layout = QVBoxLayout(central_widget)
        main_layout.setContentsMargins(0, 0, 0, 0)
        main_layout.setSpacing(0)
        
        # æ ‡é¢˜æ 
        title_bar = QWidget()
        title_bar.setFixedHeight(60)
        title_bar.setStyleSheet("""
            QWidget {
                background: qlineargradient(x1:0, y1:0, x2:1, y2:1, 
                                          stop:0 #4A90E2, stop:1 #357ABD);
                border-bottom: 1px solid #ddd;
            }
            QLabel {
                color: white;
                font-size: 18px;
                font-weight: bold;
            }
        """)
        
        title_layout = QHBoxLayout(title_bar)
        title_layout.setContentsMargins(20, 0, 20, 0)
        title_label = QLabel("ðŸŽ¤ è¯­éŸ³åŠ©æ‰‹")
        title_layout.addWidget(title_label)
        title_layout.addStretch()
        
        # èŠå¤©åŒºåŸŸ
        self.chat_scroll = QScrollArea()
        self.chat_widget = QWidget()
        self.chat_layout = QVBoxLayout(self.chat_widget)
        self.chat_layout.setContentsMargins(0, 20, 0, 20)
        self.chat_layout.setSpacing(10)
        self.chat_layout.addStretch()
        
        self.chat_scroll.setWidget(self.chat_widget)
        self.chat_scroll.setWidgetResizable(True)
        self.chat_scroll.setVerticalScrollBarPolicy(Qt.ScrollBarAsNeeded)
        self.chat_scroll.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
        
        # åŠ è½½åŠ¨ç”»
        self.loading_widget = LoadingWidget()
        self.loading_widget.hide()
        
        # åº•éƒ¨æŽ§åˆ¶åŒºåŸŸ
        bottom_widget = QWidget()
        bottom_widget.setFixedHeight(120)
        bottom_widget.setStyleSheet("""
            QWidget {
                background-color: #f8f9fa;
                border-top: 1px solid #e9ecef;
            }
        """)
        
        bottom_layout = QVBoxLayout(bottom_widget)
        bottom_layout.setContentsMargins(20, 20, 20, 20)
        
        # çŠ¶æ€æ ‡ç­¾
        self.status_label = QLabel("ç‚¹å‡»æŒ‰é’®å¼€å§‹è¯­éŸ³è¾“å…¥")
        self.status_label.setAlignment(Qt.AlignCenter)
        self.status_label.setStyleSheet("""
            QLabel {
                color: #666;
                font-size: 14px;
                background: transparent;
            }
        """)
        
        # å½•éŸ³æŒ‰é’®
        button_layout = QHBoxLayout()
        button_layout.addStretch()
        
        self.record_button = PulseButton()
        self.record_button.clicked.connect(self.toggle_recording)
        button_layout.addWidget(self.record_button)
        button_layout.addStretch()
        
        bottom_layout.addWidget(self.status_label)
        bottom_layout.addLayout(button_layout)
        
        # æ·»åŠ åˆ°ä¸»layout
        main_layout.addWidget(title_bar)
        main_layout.addWidget(self.chat_scroll)
        main_layout.addWidget(bottom_widget)
        
        # åˆå§‹åŒ–çŠ¶æ€
        self.is_recording = False
        
        # æ·»åŠ æ¬¢è¿Žæ¶ˆæ¯
        self.add_message("ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„è¯­éŸ³åŠ©æ‰‹ï¼Œç‚¹å‡»ä¸‹æ–¹æŒ‰é’®å¼€å§‹è¯­éŸ³å¯¹è¯ã€‚", False)
        
    def init_voice_system(self):
        # åˆ›å»ºè¯­éŸ³è¯†åˆ«å¯¹è±¡
        self.vad_whisper = VAD_Whisper()
        
        # è¿žæŽ¥ä¿¡å·
        self.vad_whisper.text_recognized.connect(self.on_text_recognized)
        self.vad_whisper.recording_started.connect(self.on_recording_started)
        self.vad_whisper.recording_stopped.connect(self.on_recording_stopped)
        
        # åœ¨å•ç‹¬çº¿ç¨‹ä¸­è¿è¡Œ
        self.voice_thread = threading.Thread(target=self.vad_whisper.run, daemon=True)
        self.voice_thread.start()
        
    def toggle_recording(self):
        if not self.is_recording:
            self.start_recording()
        else:
            self.stop_recording()
            
    def start_recording(self):
        self.is_recording = True
        self.record_button.start_pulse()
        self.status_label.setText("ðŸŽ¤ æ­£åœ¨å½•éŸ³...")
        self.status_label.setStyleSheet("""
            QLabel {
                color: #ef4444;
                font-size: 14px;
                font-weight: bold;
                background: transparent;
            }
        """)
        self.vad_whisper.start_recording()
        
    def stop_recording(self):
        self.is_recording = False
        self.record_button.stop_pulse()
        self.status_label.setText("â³ æ­£åœ¨è¯†åˆ«...")
        self.status_label.setStyleSheet("""
            QLabel {
                color: #4A90E2;
                font-size: 14px;
                background: transparent;
            }
        """)
        self.vad_whisper.stop_recording()
        self.loading_widget.start_animation()
        
    @pyqtSlot()
    def on_recording_started(self):
        if not self.is_recording:  # è‡ªåŠ¨æ£€æµ‹å¼€å§‹å½•éŸ³
            self.is_recording = True
            self.record_button.start_pulse()
            self.status_label.setText("ðŸŽ¤ æ£€æµ‹åˆ°è¯­éŸ³ï¼Œæ­£åœ¨å½•éŸ³...")
            self.status_label.setStyleSheet("""
                QLabel {
                    color: #ef4444;
                    font-size: 14px;
                    font-weight: bold;
                    background: transparent;
                }
            """)
            
    @pyqtSlot()
    def on_recording_stopped(self):
        if self.is_recording:
            self.is_recording = False
            self.record_button.stop_pulse()
            self.status_label.setText("â³ æ­£åœ¨è¯†åˆ«...")
            self.status_label.setStyleSheet("""
                QLabel {
                    color: #4A90E2;
                    font-size: 14px;
                    background: transparent;
                }
            """)
        
    @pyqtSlot(str)
    def on_text_recognized(self, text):
        self.loading_widget.stop_animation()
        self.status_label.setText("ç‚¹å‡»æŒ‰é’®å¼€å§‹è¯­éŸ³è¾“å…¥")
        self.status_label.setStyleSheet("""
            QLabel {
                color: #666;
                font-size: 14px;
                background: transparent;
            }
        """)
        
        if text and text.strip():
            self.add_message(text, True)
            # è¿™é‡Œå¯ä»¥æ·»åŠ å¯¹è¯é€»è¾‘ï¼Œæ¯”å¦‚è°ƒç”¨AIæŽ¥å£
            QTimer.singleShot(1000, lambda: self.add_message(f"æ”¶åˆ°æ‚¨çš„æ¶ˆæ¯ï¼š{text}", False))
            
    def add_message(self, text, is_user=True):
        bubble = ChatBubble(text, is_user)
        self.chat_layout.insertWidget(self.chat_layout.count() - 1, bubble)
        
        # æ»šåŠ¨åˆ°åº•éƒ¨
        QTimer.singleShot(100, lambda: self.chat_scroll.verticalScrollBar().setValue(
            self.chat_scroll.verticalScrollBar().maximum()))
        
    def closeEvent(self, event):
        self.vad_whisper.stop()
        event.accept()


if __name__ == "__main__":
    app = QApplication(sys.argv)
    
    # è®¾ç½®åº”ç”¨å›¾æ ‡å’Œæ ·å¼
    app.setStyle('Fusion')
    
    window = VoiceChatApp()
    window.show()
    
    sys.exit(app.exec_())