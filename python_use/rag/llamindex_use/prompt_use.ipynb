{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from storage\\docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from storage\\index_store.json.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# 设置 LLM 模型（如 llama3.1）\n",
    "Settings.llm = Ollama(model=\"qwen2.5:7b\", request_timeout=360.0)\n",
    "\n",
    "# 设置嵌入模型（如 nomic-embed-text）\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"bge-m3\")\n",
    "\n",
    "## 上述每一次都会加载文档并构建\n",
    "## 可以从数据库中加载\n",
    "# Save the index\n",
    "\n",
    "\n",
    "# Later, load the index\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n",
    "index = load_index_from_storage(\n",
    "    storage_context,\n",
    "    # we can optionally override the embed_model here\n",
    "    # it's important to use the same embed_model as the one used to build the index\n",
    "    # embed_model=Settings.embed_model,\n",
    ")\n",
    "query_engine = index.as_query_engine(\n",
    "    # we can optionally override the llm here\n",
    "    # llm=Settings.llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看一些预设的提示词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['response_synthesizer:text_qa_template', 'response_synthesizer:refine_template'])\n",
      "('Context information is below.\\n'\n",
      " '---------------------\\n'\n",
      " '{context_str}\\n'\n",
      " '---------------------\\n'\n",
      " 'Given the context information and not prior knowledge, answer the query.\\n'\n",
      " 'Query: {query_str}\\n'\n",
      " 'Answer: ')\n",
      "------------------------------------------------------------\n",
      "('The original query is as follows: {query_str}\\n'\n",
      " 'We have provided an existing answer: {existing_answer}\\n'\n",
      " 'We have the opportunity to refine the existing answer (only if needed) with '\n",
      " 'some more context below.\\n'\n",
      " '------------\\n'\n",
      " '{context_msg}\\n'\n",
      " '------------\\n'\n",
      " 'Given the new context, refine the original answer to better answer the '\n",
      " \"query. If the context isn't useful, return the original answer.\\n\"\n",
      " 'Refined Answer: ')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "prompts = query_engine.get_prompts()\n",
    "print(prompts.keys())\n",
    "\n",
    "pprint.pprint(prompts[\"response_synthesizer:text_qa_template\"].get_template())\n",
    "print(\"---\"*20)\n",
    "pprint.pprint(prompts[\"response_synthesizer:refine_template\"].get_template())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('以下是上下文信息。\\n'\n",
      " '---------------------\\n'\n",
      " '{context_str}\\n'\n",
      " '---------------------\\n'\n",
      " '根据上下文信息回答问题，不要依赖预置知识，不要编造。\\n'\n",
      " '问题: {query_str}\\n'\n",
      " '回答: ')\n",
      "------------------------------------------------------------\n",
      "('\\n'\n",
      " '原始问题如下：{query_str}\\n'\n",
      " '\\n'\n",
      " '当前已有回答：{existing_answer}\\n'\n",
      " '\\n'\n",
      " '现提供以下补充上下文（仅在需要时用于完善原答案）：\\n'\n",
      " '------------\\n'\n",
      " '{context_msg}\\n'\n",
      " '------------\\n'\n",
      " '\\n'\n",
      " '请根据新上下文对原答案进行优化，使其更准确回答该问题。若上下文无帮助，则保留原答案。\\n'\n",
      " '\\n'\n",
      " '优化后的回答：\\n')\n"
     ]
    }
   ],
   "source": [
    "## 对预设的提示词进行更改\n",
    "\n",
    "from llama_index.core.prompts import PromptTemplate \n",
    "my_qa_prompt_tmpl_str = ( \n",
    " \"以下是上下文信息。\\n\" \n",
    " \"---------------------\\n\" \n",
    " \"{context_str}\\n\" \n",
    " \"---------------------\\n\" \n",
    " \"根据上下文信息回答问题，不要依赖预置知识，不要编造。\\n\" \n",
    " \"问题: {query_str}\\n\" \n",
    " \"回答: \" \n",
    ") \n",
    "query_engine.update_prompts( \n",
    "    {\"response_synthesizer:text_qa_template\":  PromptTemplate(my_qa_prompt_tmpl_str) } \n",
    ")\n",
    "\n",
    "my_refine_tmpl_str = \"\"\"\n",
    "原始问题如下：{query_str}\n",
    "\n",
    "当前已有回答：{existing_answer}\n",
    "\n",
    "现提供以下补充上下文（仅在需要时用于完善原答案）：\n",
    "------------\n",
    "{context_msg}\n",
    "------------\n",
    "\n",
    "请根据新上下文对原答案进行优化，使其更准确回答该问题。若上下文无帮助，则保留原答案。\n",
    "\n",
    "优化后的回答：\n",
    "\"\"\"\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:refine_template\": PromptTemplate(my_refine_tmpl_str)}\n",
    ")\n",
    "prompts = query_engine.get_prompts()\n",
    "pprint.pprint(prompts[\"response_synthesizer:text_qa_template\"].get_template())\n",
    "print(\"---\"*20)\n",
    "pprint.pprint(prompts[\"response_synthesizer:refine_template\"].get_template())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对话历史信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "user_message = ChatMessage(('user', '如何构建Agent系统？'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.18 ('agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "323adcefd241dc7b9f6af1af4db666d10a12d12d476723437e467064bc267131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
