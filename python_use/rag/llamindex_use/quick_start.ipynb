{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fc9a749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  c:\\Users\\mix\\.conda\\envs\\agents\\python.exe -m pip <command> [options]\n",
      "\n",
      "no such option: --queit\n"
     ]
    }
   ],
   "source": [
    "%pip --queit install llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d88d9a8",
   "metadata": {},
   "source": [
    "## 使用Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddae009d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet llama-index-llms-ollama llama-index-embeddings-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "124cb4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# 设置 LLM 模型（如 llama3.1）\n",
    "Settings.llm = Ollama(model=\"qwen2.5:7b\", request_timeout=360.0)\n",
    "\n",
    "# 设置嵌入模型（如 nomic-embed-text）\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"bge-m3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954d27f7",
   "metadata": {},
   "source": [
    "## 快速开始\n",
    "* 本地部署模型文档参考： https://docs.llamaindex.ai/en/latest/getting_started/starter_example_local/\n",
    "* 更多向量数据库支持文档：https://docs.llamaindex.ai/en/latest/module_guides/storing/vector_stores/\n",
    "    * 如何使用这些向量数据库：https://docs.llamaindex.ai/en/latest/community/integrations/vector_stores/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a36a60a",
   "metadata": {},
   "source": [
    "### Rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c806f409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文章内容涉及了代理人工智能（Agentic AI）的概念分类、应用以及挑战。具体包括几个实际应用场景，如智能临床决策支持系统、农业机器人协调作业、企业网络安全响应等。这些例子展示了代理人工智能在复杂动态环境中的自主任务协调能力，并强调了中央调度器管理、共享记忆模块以及反馈机制的重要性。\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"请问文章内容是关于什么的？\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70381bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 上述每一次都会加载文档并构建\n",
    "## 可以从数据库中加载\n",
    "# Save the index\n",
    "\n",
    "## 将上述的内容保存 \n",
    "index.storage_context.persist(\"storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4be62501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from storage\\docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from storage\\index_store.json.\n"
     ]
    }
   ],
   "source": [
    "# Later, load the index\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n",
    "index = load_index_from_storage(\n",
    "    storage_context,\n",
    "    # we can optionally override the embed_model here\n",
    "    # it's important to use the same embed_model as the one used to build the index\n",
    "    # embed_model=Settings.embed_model,\n",
    ")\n",
    "query_engine = index.as_query_engine(\n",
    "    # we can optionally override the llm here\n",
    "    # llm=Settings.llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bff07cf",
   "metadata": {},
   "source": [
    "### Agent Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af600e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of 1234 multiplied by 4567 is 5635678.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Define a simple calculator tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Useful for multiplying two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "# Create an agent workflow with our calculator tool\n",
    "agent = FunctionAgent(\n",
    "    tools=[multiply],\n",
    "    llm=Ollama(\n",
    "        model=\"Qwen2.5:7b\",\n",
    "        request_timeout=360.0,\n",
    "        # Manually set the context window to limit memory usage\n",
    "        context_window=8000,\n",
    "    ),\n",
    "    system_prompt=\"You are a helpful assistant that can multiply two numbers.\",\n",
    ")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    # Run the agent\n",
    "    response = await agent.run(\"What is 1234 * 4567?\")\n",
    "    print(str(response))\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de27cb",
   "metadata": {},
   "source": [
    "### 增加流式输出\n",
    "不同模型的对应的流式输出支持不同，因此可以直接设置对应的参数即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e104ec",
   "metadata": {},
   "source": [
    "#### 使用chat_engine或者llm自带\n",
    "chat_engine自动集成了向量数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80d0360c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试流式输出:\n",
      "我是Qwen，一个由阿里云开发的语言模型助手。我在这里为了提供帮助和回答问题而设计的，旨在与用户进行对话，并尽可能准确和有帮助地回应您的需求。如果您有任何问题或需要帮助，请随时告诉我！\n",
      "\n",
      "对话引擎测试:\n",
      "Agentic AI 是一种新兴的智能系统类别，它扩展了传统AI代理的能力，使其能够通过结构化的沟通、共享记忆以及动态的角色分配来实现多个智能实体之间的协作目标。与单一的AI代理模型相比，在复杂、动态、多步骤或合作场景中的应用案例日益要求上下文保留、任务依赖性和适应性。因此，Agentic AI 系统代表了一种新兴的智能化架构类型，在这种架构中，多个专业化的代理可以协同工作以实现共同目标。\n",
      "\n",
      "具体来说，与传统的AI代理相比，Agentic AI系统具有以下特点：\n",
      "1. **协作机制**：支持多个智能实体之间的协调和合作。\n",
      "2. **结构化沟通**：通过有效的信息交流来促进任务执行和目标达成。\n",
      "3. **共享记忆**：允许各智能体间共享相关信息以增强整体性能。\n",
      "4. **动态角色分配**：根据需要调整各个代理的角色和职责。\n",
      "\n",
      "这些特性使得Agentic AI系统能够更好地应对复杂多变的环境，并实现更加灵活、智能化的目标追求。"
     ]
    }
   ],
   "source": [
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# 1. 创建支持流式的LLM实例（关键修改）\n",
    "custom_llm = Ollama(\n",
    "    model=\"qwen2.5:7b\",\n",
    "    streaming=True,  # 必须启用流式标志[1,4](@ref)\n",
    "    temperature=0.7,  # 控制输出多样性[1](@ref)\n",
    "    request_timeout=120.0  # 避免超时中断[6](@ref)\n",
    ")\n",
    "\n",
    "# 2. 删除冗余调用（custom_llm.predict() 不必要）\n",
    "# 直接使用流式接口\n",
    "\n",
    "# 3. 正确的流式调用方式\n",
    "print(\"测试流式输出:\")\n",
    "for token in custom_llm.stream_complete(prompt=\"你是谁？\"):  # 使用stream_complete[1,2](@ref)\n",
    "    print(token.delta, end=\"\", flush=True)  # 使用delta获取增量内容[2](@ref)\n",
    "\n",
    "# 4. 创建带记忆的聊天引擎\n",
    "memory = ChatMemoryBuffer(token_limit=4000)  # 控制上下文长度[8](@ref)\n",
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode=\"context\",\n",
    "    streaming=True,  # 引擎层启用流式[2](@ref)\n",
    "    llm=custom_llm,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# 5. 流式对话执行\n",
    "print(\"\\n\\n对话引擎测试:\")\n",
    "streaming_response = chat_engine.stream_chat(\"什么是Agentic AI\")\n",
    "for token in streaming_response.response_gen:  # 正确访问响应生成器[2](@ref)\n",
    "    print(token, end=\"\", flush=True)  # 实时输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c245122a",
   "metadata": {},
   "source": [
    "### 记忆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4017256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_store=SimpleChatStore(store={'chat_history': [ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='什么是Agentic AI')]), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='Agentic AI 是一种新兴的智能系统类别，它扩展了传统AI代理的能力，使其能够通过结构化的沟通、共享记忆以及动态的角色分配来实现多个智能实体之间的协作目标。与单一的AI代理模型相比，在复杂、动态、多步骤或合作场景中的应用案例日益要求上下文保留、任务依赖性和适应性。因此，Agentic AI 系统代表了一种新兴的智能化架构类型，在这种架构中，多个专业化的代理可以协同工作以实现共同目标。\\n\\n具体来说，与传统的AI代理相比，Agentic AI系统具有以下特点：\\n1. **协作机制**：支持多个智能实体之间的协调和合作。\\n2. **结构化沟通**：通过有效的信息交流来促进任务执行和目标达成。\\n3. **共享记忆**：允许各智能体间共享相关信息以增强整体性能。\\n4. **动态角色分配**：根据需要调整各个代理的角色和职责。\\n\\n这些特性使得Agentic AI系统能够更好地应对复杂多变的环境，并实现更加灵活、智能化的目标追求。')])]}) chat_store_key='chat_history' token_limit=4000 tokenizer_fn=functools.partial(<bound method Encoding.encode of <Encoding 'cl100k_base'>>, allowed_special='all')\n"
     ]
    }
   ],
   "source": [
    "# # 4. 创建带记忆的聊天引擎\n",
    "# memory = ChatMemoryBuffer(token_limit=4000)  # 控制上下文长度[8](@ref)\n",
    "print(memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc11e976",
   "metadata": {},
   "source": [
    "## 结构化数据提取\n",
    "https://docs.llamaindex.ai/en/latest/use_cases/extraction/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.18 ('agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "323adcefd241dc7b9f6af1af4db666d10a12d12d476723437e467064bc267131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
