{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip  install llama-index\n",
    "%pip install llama-index-llms-ollama llama-index-embeddings-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试流式输出:\n",
      "我是Qwen，一个由阿里云开发的语言模型助手。我在这里为了提供帮助和回答问题而设计的，希望能成为你与知识之间的桥梁！如果你有任何问题或需要帮助，请随时告诉我。"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# 设置 LLM 模型（如 llama3.1）\n",
    "Settings.llm = Ollama(model=\"qwen2.5:7b\", request_timeout=360.0)\n",
    "\n",
    "# 设置嵌入模型（如 nomic-embed-text）\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"bge-m3\")\n",
    "\n",
    "# 1. 创建支持流式的LLM实例（关键修改）\n",
    "custom_llm = Ollama(\n",
    "    model=\"qwen2.5:7b\",\n",
    "    streaming=True,  # 必须启用流式标志[1,4](@ref)\n",
    "    temperature=0.7,  # 控制输出多样性[1](@ref)\n",
    "    request_timeout=120.0  # 避免超时中断[6](@ref)\n",
    ")\n",
    "\n",
    "# 2. 删除冗余调用（custom_llm.predict() 不必要）\n",
    "# 直接使用流式接口\n",
    "\n",
    "# 3. 正确的流式调用方式\n",
    "print(\"测试流式输出:\")\n",
    "for token in custom_llm.stream_complete(prompt=\"你是谁？\"):  # 使用stream_complete[1,2](@ref)\n",
    "    print(token.delta, end=\"\", flush=True)  # 使用delta获取增量内容[2](@ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这篇文章内容主要涉及代理人工智能（Agentic AI）及其在不同领域的应用，如自动化科研论文写作、果园机器人协同作业、医院ICU中的临床决策支持以及企业网络安全响应等。同时提到了多智能体游戏AI和自适应工作流的应用。\n"
     ]
    }
   ],
   "source": [
    "# Later, load the index\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"请问文章内容是关于什么的？\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 借助LlamaDebugHandler进行\n",
    "* 记录所有的调用过程，一些历史记录，时间，token消耗，输入输出等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler\n",
    "\n",
    "# 初始化调试处理器\n",
    "llama_debug = LlamaDebugHandler(\n",
    "    print_trace_on_end=True,    # 执行结束后自动打印摘要\n",
    ")\n",
    "\n",
    "# 集成到回调系统\n",
    "callback_manager = CallbackManager([llama_debug])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from storage\\docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from storage\\index_store.json.\n"
     ]
    }
   ],
   "source": [
    "# 设置为某个组件级别的跟踪器\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n",
    "index = load_index_from_storage(\n",
    "    storage_context,\n",
    "    # we can optionally override the embed_model here\n",
    "    # it's important to use the same embed_model as the one used to build the index\n",
    ")\n",
    "\n",
    "# 设置为全局的\n",
    "Settings.callback_manager = callback_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_query -> 2.670604 seconds\n",
      "      |_synthesize -> 2.264138 seconds\n",
      "        |_templating -> 0.0 seconds\n",
      "        |_llm -> 2.259139 seconds\n",
      "**********\n",
      "**********\n",
      "Trace: chat\n",
      "    |_agent_step -> 6.26025 seconds\n",
      "      |_llm -> 0.949324 seconds\n",
      "      |_function_call -> 0.79055 seconds\n",
      "        |_query -> 0.789524 seconds\n",
      "          |_synthesize -> 0.598661 seconds\n",
      "            |_templating -> 0.0 seconds\n",
      "            |_llm -> 0.589502 seconds\n",
      "      |_llm -> 0.875682 seconds\n",
      "      |_function_call -> 1.734953 seconds\n",
      "        |_query -> 1.733937 seconds\n",
      "          |_synthesize -> 1.567575 seconds\n",
      "            |_templating -> 0.0 seconds\n",
      "            |_llm -> 1.562576 seconds\n",
      "      |_llm -> 1.905706 seconds\n",
      "**********\n",
      "这篇文章主要探讨了AI代理和生成性人工智能（Agentic AI）的相关领域，包括工具增强的大型语言模型如Toolfive、协同平台Gentopia及Metta GPT等多智能体框架的应用与挑战；还涉及到了Web代理、查询解析与分析以及科研辅助系统等方面的研究。这些研究旨在从基础理论到实际应用的各种探索，以推动下一代AI技术的发展。\n",
      "\n",
      "===== 事件时间线 =====\n",
      "06/10/2025, 16:05:37.931706 _ 06/10/2025, 16:05:35.444762\n",
      "06/10/2025, 16:05:37.931706 _ 06/10/2025, 16:05:35.707769\n",
      "06/10/2025, 16:05:35.711286 _ 06/10/2025, 16:05:35.709280\n",
      "06/10/2025, 16:05:35.714289 _ 06/10/2025, 16:05:35.712291\n",
      "06/10/2025, 16:05:35.714289 _ 06/10/2025, 16:05:35.714289\n",
      "06/10/2025, 16:05:37.931706 _ 06/10/2025, 16:05:35.714289\n",
      "==========LLM事件==========\n",
      "  Prompt tokens: 0\n",
      "  Completion tokens: 0\n",
      "06/10/2025, 16:05:57.119963 _ 06/10/2025, 16:05:54.506800\n",
      "06/10/2025, 16:05:57.119963 _ 06/10/2025, 16:05:54.657612\n",
      "06/10/2025, 16:05:54.659619 _ 06/10/2025, 16:05:54.658620\n",
      "06/10/2025, 16:05:54.661611 _ 06/10/2025, 16:05:54.660619\n",
      "06/10/2025, 16:05:54.662615 _ 06/10/2025, 16:05:54.662615\n",
      "06/10/2025, 16:05:57.119963 _ 06/10/2025, 16:05:54.662615\n",
      "==========LLM事件==========\n",
      "  Prompt tokens: 0\n",
      "  Completion tokens: 0\n",
      "06/10/2025, 16:08:11.011470 _ 06/10/2025, 16:08:09.007660\n",
      "06/10/2025, 16:08:11.011470 _ 06/10/2025, 16:08:09.274398\n",
      "06/10/2025, 16:08:09.276903 _ 06/10/2025, 16:08:09.275391\n",
      "06/10/2025, 16:08:09.278912 _ 06/10/2025, 16:08:09.277912\n",
      "06/10/2025, 16:08:09.278912 _ 06/10/2025, 16:08:09.278912\n",
      "06/10/2025, 16:08:11.011470 _ 06/10/2025, 16:08:09.279915\n",
      "==========LLM事件==========\n",
      "  Prompt tokens: 0\n",
      "  Completion tokens: 0\n",
      "06/10/2025, 16:08:34.241098 _ 06/10/2025, 16:08:30.907149\n",
      "06/10/2025, 16:08:34.241098 _ 06/10/2025, 16:08:31.119981\n",
      "06/10/2025, 16:08:31.122498 _ 06/10/2025, 16:08:31.120984\n",
      "06/10/2025, 16:08:31.125526 _ 06/10/2025, 16:08:31.123512\n",
      "06/10/2025, 16:08:31.125526 _ 06/10/2025, 16:08:31.125526\n",
      "06/10/2025, 16:08:34.241098 _ 06/10/2025, 16:08:31.125526\n",
      "==========LLM事件==========\n",
      "  Prompt tokens: 0\n",
      "  Completion tokens: 0\n",
      "06/10/2025, 16:08:48.346256 _ 06/10/2025, 16:08:45.280028\n",
      "06/10/2025, 16:08:48.346256 _ 06/10/2025, 16:08:45.453781\n",
      "06/10/2025, 16:08:45.456766 _ 06/10/2025, 16:08:45.455771\n",
      "06/10/2025, 16:08:45.458374 _ 06/10/2025, 16:08:45.456766\n",
      "06/10/2025, 16:08:45.459331 _ 06/10/2025, 16:08:45.459331\n",
      "06/10/2025, 16:08:48.345259 _ 06/10/2025, 16:08:45.459331\n",
      "==========LLM事件==========\n",
      "  Prompt tokens: 0\n",
      "  Completion tokens: 0\n",
      "06/10/2025, 16:23:32.427486 _ 06/10/2025, 16:23:21.133916\n",
      "06/10/2025, 16:23:32.427486 _ 06/10/2025, 16:23:25.870804\n",
      "06/10/2025, 16:23:25.873306 _ 06/10/2025, 16:23:25.871801\n",
      "06/10/2025, 16:23:25.875313 _ 06/10/2025, 16:23:25.874314\n",
      "06/10/2025, 16:23:25.875313 _ 06/10/2025, 16:23:25.875313\n",
      "06/10/2025, 16:23:32.426475 _ 06/10/2025, 16:23:25.876311\n",
      "==========LLM事件==========\n",
      "  Prompt tokens: 0\n",
      "  Completion tokens: 0\n",
      "06/10/2025, 16:24:53.709589 _ 06/10/2025, 16:24:51.685446\n",
      "06/10/2025, 16:24:53.709589 _ 06/10/2025, 16:24:51.990017\n",
      "06/10/2025, 16:24:51.992012 _ 06/10/2025, 16:24:51.991011\n",
      "06/10/2025, 16:24:51.994013 _ 06/10/2025, 16:24:51.993012\n",
      "06/10/2025, 16:24:51.995013 _ 06/10/2025, 16:24:51.995013\n",
      "06/10/2025, 16:24:53.709589 _ 06/10/2025, 16:24:51.996012\n",
      "==========LLM事件==========\n",
      "  Prompt tokens: 0\n",
      "  Completion tokens: 0\n",
      "06/10/2025, 16:25:00.163197 _ 06/10/2025, 16:24:55.218995\n",
      "06/10/2025, 16:24:55.942276 _ 06/10/2025, 16:24:55.219992\n",
      "==========LLM事件==========\n",
      "  Prompt tokens: 0\n",
      "  Completion tokens: 0\n",
      "06/10/2025, 16:24:58.573541 _ 06/10/2025, 16:24:55.953910\n",
      "06/10/2025, 16:24:58.573541 _ 06/10/2025, 16:24:55.953910\n",
      "06/10/2025, 16:24:58.573541 _ 06/10/2025, 16:24:56.161000\n",
      "06/10/2025, 16:24:56.163996 _ 06/10/2025, 16:24:56.163007\n",
      "06/10/2025, 16:24:56.164996 _ 06/10/2025, 16:24:56.163996\n",
      "06/10/2025, 16:24:56.165997 _ 06/10/2025, 16:24:56.165997\n",
      "06/10/2025, 16:24:58.572521 _ 06/10/2025, 16:24:56.165997\n",
      "==========LLM事件==========\n",
      "  Prompt tokens: 0\n",
      "  Completion tokens: 0\n",
      "06/10/2025, 16:25:00.161185 _ 06/10/2025, 16:24:58.574544\n",
      "==========LLM事件==========\n",
      "  Prompt tokens: 0\n",
      "  Completion tokens: 0\n",
      "06/10/2025, 16:25:32.726558 _ 06/10/2025, 16:25:30.055954\n",
      "06/10/2025, 16:25:32.726558 _ 06/10/2025, 16:25:30.462420\n",
      "06/10/2025, 16:25:30.464416 _ 06/10/2025, 16:25:30.463416\n",
      "06/10/2025, 16:25:30.467419 _ 06/10/2025, 16:25:30.465415\n",
      "06/10/2025, 16:25:30.467419 _ 06/10/2025, 16:25:30.467419\n",
      "06/10/2025, 16:25:32.726558 _ 06/10/2025, 16:25:30.467419\n",
      "==========LLM事件==========\n",
      "  Prompt tokens: 0\n",
      "  Completion tokens: 0\n",
      "06/10/2025, 16:25:38.995350 _ 06/10/2025, 16:25:32.735100\n",
      "06/10/2025, 16:25:33.685418 _ 06/10/2025, 16:25:32.736094\n",
      "==========LLM事件==========\n",
      "  Prompt tokens: 0\n",
      "  Completion tokens: 0\n",
      "06/10/2025, 16:25:34.475968 _ 06/10/2025, 16:25:33.685418\n",
      "06/10/2025, 16:25:34.475968 _ 06/10/2025, 16:25:33.686444\n",
      "06/10/2025, 16:25:34.475968 _ 06/10/2025, 16:25:33.877307\n",
      "06/10/2025, 16:25:33.881298 _ 06/10/2025, 16:25:33.878315\n",
      "06/10/2025, 16:25:33.885468 _ 06/10/2025, 16:25:33.882298\n",
      "06/10/2025, 16:25:33.885468 _ 06/10/2025, 16:25:33.885468\n",
      "06/10/2025, 16:25:34.475968 _ 06/10/2025, 16:25:33.886466\n",
      "==========LLM事件==========\n",
      "  Prompt tokens: 0\n",
      "  Completion tokens: 0\n",
      "06/10/2025, 16:25:35.352673 _ 06/10/2025, 16:25:34.476991\n",
      "==========LLM事件==========\n",
      "  Prompt tokens: 0\n",
      "  Completion tokens: 0\n",
      "06/10/2025, 16:25:37.087626 _ 06/10/2025, 16:25:35.352673\n",
      "06/10/2025, 16:25:37.087626 _ 06/10/2025, 16:25:35.353689\n",
      "06/10/2025, 16:25:37.087626 _ 06/10/2025, 16:25:35.520051\n",
      "06/10/2025, 16:25:35.522056 _ 06/10/2025, 16:25:35.521049\n",
      "06/10/2025, 16:25:35.525050 _ 06/10/2025, 16:25:35.523058\n",
      "06/10/2025, 16:25:35.525050 _ 06/10/2025, 16:25:35.525050\n",
      "06/10/2025, 16:25:37.087626 _ 06/10/2025, 16:25:35.525050\n",
      "==========LLM事件==========\n",
      "  Prompt tokens: 0\n",
      "  Completion tokens: 0\n",
      "06/10/2025, 16:25:38.994338 _ 06/10/2025, 16:25:37.088632\n",
      "==========LLM事件==========\n",
      "  Prompt tokens: 0\n",
      "  Completion tokens: 0\n"
     ]
    }
   ],
   "source": [
    "# 4. 查询并获取调试信息\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"文档的主要内容是什么?\")\n",
    "\n",
    "chat_engine =  index.as_chat_engine()\n",
    "response = chat_engine.chat(\"请问文章内容关于什么？\")\n",
    "print(response)\n",
    "# 5. 分析调试数据\n",
    "print(\"\\n===== 事件时间线 =====\")\n",
    "\n",
    "# 获取所有事件的时间线\n",
    "event_pairs = llama_debug.get_event_pairs()\n",
    "\n",
    "# 遍历事件并打印信息\n",
    "for event_pair in event_pairs:\n",
    "    start_event, end_event = event_pair\n",
    "    if end_event:  # 确保有结束事件\n",
    "        print(f\"{end_event.time} _ {start_event.time}\")\n",
    "        \n",
    "        # 对于LLM事件，可以获取token使用情况\n",
    "        if start_event.event_type == \"llm\":\n",
    "            print(\"=\"*10 + \"LLM事件\" + \"=\"*10)\n",
    "            token_usage = end_event.payload.get(\"token_usage\", {})\n",
    "            prompt_tokens = token_usage.get(\"prompt_tokens\", 0)\n",
    "            completion_tokens = token_usage.get(\"completion_tokens\", 0)\n",
    "            print(f\"  Prompt tokens: {prompt_tokens}\")\n",
    "            print(f\"  Completion tokens: {completion_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 借助第三方的跟踪和调试平台"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### langfuse\n",
    "* 可以使用在线平台， 或者在本地搭建平台\n",
    "* 与上述相同设置全局callback_manager相似，直接直接集成\n",
    "* https://langfuse.com/docs/integrations/llama-index/get-started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet langfuse openinference-instrumentation-llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langfuse client is authenticated and ready!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 旧版，无法使用 languse.callback\n",
    "# from langfuse import LlamaIndexCallbackHandler \n",
    "\n",
    "import os\n",
    "#设置 Langfuse 平台的 API Key，参考上方申请\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-84d45f77-cea6-46dc-90d9-b52cf6135cc6\" \n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-19583d27-37c4-46f6-958c-ff70e48c6260\" \n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" \n",
    "\n",
    "from langfuse import get_client\n",
    "\n",
    "langfuse = get_client()\n",
    "\n",
    "# Verify connection\n",
    "if langfuse.auth_check():\n",
    "    print(\"Langfuse client is authenticated and ready!\")\n",
    "else:\n",
    "    print(\"Authentication failed. Please check your credentials and host.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to instrument while already instrumented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: chat\n",
      "    |_agent_step -> 0.475552 seconds\n",
      "      |_llm -> 0.0 seconds\n",
      "**********\n",
      " 文章中提到，AI Agent 和 Agentic AI 的主要区别在于它们处理复杂任务的能力不同。AI Agents 是能够通过自然语言理解和生成来执行特定任务的智能体，但当面对需要持续上下文保持和动态环境适应能力的复杂任务时，单一的 AI Agent 可能会遇到挑战。而 Agentic AI 则是由多个具有特定功能的代理组成的架构，这些代理能够通过共享信息、协作以及在必要时角色互换来共同完成复杂的多步骤任务或目标。这意味着 Agentic AI 能够更好地适应复杂和动态的变化，提供更高的灵活性和可扩展性。\n",
      "\n",
      "因此，简而言之：\n",
      "- **AI Agents** 是独立执行任务的个体智能体。\n",
      "- **Agentic AI** 则是通过多个代理间的协作来应对更为复杂的问题和环境变化。"
     ]
    }
   ],
   "source": [
    "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
    "\n",
    "# Initialize LlamaIndex instrumentation\n",
    "LlamaIndexInstrumentor().instrument()\n",
    "\n",
    "\n",
    "## 使用llama_index封装好的llm等，比如custom_llm\n",
    "with langfuse.start_as_current_span(name=\"llama-index-trace\"):\n",
    "    # response = custom_llm.complete(\"Hello, world!\")\n",
    "    # print(response)\n",
    "\n",
    "    stream_res = chat_engine.stream_chat(\"文中关于Ai Agentic的区别与AI agent之间？\")\n",
    "    for chunk in stream_res.response_gen:\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "        \n",
    "\n",
    "langfuse.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deepeval\n",
    "* 快速结合：https://deepeval.com/integrations/frameworks/llamaindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet -U deepeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "#!deepeval login 自行在控制台登录"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试用例\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\28597\\.conda\\envs\\agents\\lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for Jupyter\n",
       "support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\28597\\.conda\\envs\\agents\\lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for Jupyter\n",
       "support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "The score is 0.00 because the output contains no relevant information about LlamaIndex and includes unrelated discussions on RAG and Tool-Augmented Reasoning, which do not help in defining what LlamaIndex is.\n"
     ]
    }
   ],
   "source": [
    "# 新建测试用例\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# 设置deepEval支持的Ollama\n",
    "from deepeval.models import OllamaModel\n",
    "eval_model = OllamaModel(\n",
    "        model=\"qwen2.5:32b\"\n",
    "    )\n",
    "# An example input to your RAG application\n",
    "user_input = \"What is LlamaIndex?\"\n",
    "\n",
    "# LlamaIndex returns a response object that contains\n",
    "# both the output string and retrieved nodes\n",
    "response_object = query_engine.query(user_input)\n",
    "\n",
    "# Process the response object to get the output string\n",
    "# and retrieved nodes\n",
    "if response_object is not None:\n",
    "    actual_output = response_object.response\n",
    "    retrieval_context = [node.get_content() for node in response_object.source_nodes]\n",
    "\n",
    "\n",
    "\n",
    "# Create a test case and metric as usual\n",
    "test_case = LLMTestCase(\n",
    "    input=user_input,\n",
    "    actual_output=actual_output,\n",
    "    retrieval_context=retrieval_context\n",
    ")\n",
    "\n",
    "# 设置不同的模型作为评估模型\n",
    "answer_relevancy_metric = AnswerRelevancyMetric(\n",
    "    model = eval_model\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "answer_relevancy_metric.measure(test_case)\n",
    "print(answer_relevancy_metric.score)\n",
    "print(answer_relevancy_metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 单元测试\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from deepeval import assert_test\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.dataset import EvaluationDataset, Golden\n",
    "\n",
    "rag_application = index.as_chat_engine()\n",
    "\n",
    "example_golden = Golden(input=\"What is Agentic?\")\n",
    "\n",
    "dataset = EvaluationDataset(goldens=[example_golden])\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"golden\",\n",
    "    dataset.goldens,\n",
    ")\n",
    "def test_rag(golden: Golden):\n",
    "    # LlamaIndex returns a response object that contains\n",
    "    # both the output string and retrieved nodes\n",
    "    response_object = rag_application.query(golden.input)\n",
    "\n",
    "    # Process the response object to get the output string\n",
    "    # and retrieved nodes\n",
    "    if response_object is not None:\n",
    "        actual_output = response_object.response\n",
    "        retrieval_context = [node.get_content() for node in response_object.source_nodes]\n",
    "\n",
    "    test_case = LLMTestCase(\n",
    "        input=golden.input,\n",
    "        actual_output=actual_output,\n",
    "        retrieval_context=retrieval_context\n",
    "    )\n",
    "    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5, model=eval_model)\n",
    "    assert_test(test_case, [answer_relevancy_metric])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 整合llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#该部分源代码有问题，建议使用上述的替代即可\n",
    "\n",
    "# from deepeval.integrations.llama_index import DeepEvalFaithfulnessEvaluator\n",
    "\n",
    "# # An example input to your RAG application\n",
    "# user_input = \"What is LlamaIndex?\"\n",
    "\n",
    "# # LlamaIndex returns a response object that contains\n",
    "# # both the output string and retrieved nodes\n",
    "# response_object = rag_application.query(user_input)\n",
    "\n",
    "# evaluator = DeepEvalFaithfulnessEvaluator()\n",
    "# evaluation_result = evaluator.evaluate_response(\n",
    "#     query=user_input, response=response_object\n",
    "# )\n",
    "# print(evaluation_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 远端拉取数据集测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "#!deepeval login\n",
    "# 建议使用.py命令行运行"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.18 ('agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "323adcefd241dc7b9f6af1af4db666d10a12d12d476723437e467064bc267131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
