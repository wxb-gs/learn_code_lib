import pytest
import logging
from deepeval.test_case import LLMTestCase
from deepeval.dataset import EvaluationDataset
from deepeval import assert_test
from llama_index.core import StorageContext, load_index_from_storage
from deepeval.models.llms.ollama_model import OllamaModel
from deepeval.dataset import EvaluationDataset, Golden
from deepeval.metrics import AnswerRelevancyMetric

from llama_index.core import Settings
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.ollama import OllamaEmbedding

# è®¾ç½® LLM æ¨¡å‹ï¼ˆå¦‚ llama3.1ï¼‰
Settings.llm = Ollama(model="qwen2.5:7b", request_timeout=360.0)

# è®¾ç½®åµŒå…¥æ¨¡å‹ï¼ˆå¦‚ nomic-embed-textï¼‰
Settings.embed_model = OllamaEmbedding(model_name="bge-m3")

storage_context = StorageContext.from_defaults(persist_dir="storage")
index = load_index_from_storage(
    storage_context,
    # we can optionally override the embed_model here
    # it's important to use the same embed_model as the one used to build the index
)
eval_model = OllamaModel(model="qwen2.5:32b")

dataset = EvaluationDataset()
dataset.pull(alias="af")


rag_chat = index.as_chat_engine()
def your_llm_app(input: str):
    response = ""
    streaming_response = rag_chat.stream_chat(input)
    for token in streaming_response.response_gen:  # æ­£ç¡®è®¿é—®å“åº”ç”Ÿæˆå™¨[2](@ref)
        print(token, end="", flush=True)  # å®æ—¶è¾“å‡º
        response += token
    return response


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler("evaluation.log"),  # è¾“å‡ºåˆ°æ–‡ä»¶
        logging.StreamHandler()  # è¾“å‡ºåˆ°æ§åˆ¶å°
    ]
)

@pytest.mark.parametrize("golden", dataset.goldens)
def test_llm_app(golden: Golden):
    logging.info(f"ğŸš€ å¼€å§‹æµ‹è¯•ç”¨ä¾‹: {golden.input}")
    
    # æ‰§è¡ŒLLMè°ƒç”¨
    response = your_llm_app(golden.input)  
    logging.info(f"âœ… LLMå“åº”: {response[:100]}...")  # è®°å½•å‰100å­—ç¬¦
    
    test_case = LLMTestCase(
        input=golden.input, 
        actual_output=response
    )
    
    # æ‰§è¡Œè¯„ä¼°
    metric = AnswerRelevancyMetric(model=eval_model)
    metric.measure(test_case)
    logging.info(f"ğŸ“Š è¯„ä¼°å¾—åˆ†: {metric.score} | åŸå› : {metric.reason}")
    
    assert_test(test_case=test_case, metrics=[metric])
    logging.info("âœ… æµ‹è¯•å®Œæˆ\n" + "-"*50)

