{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Muti-Query\n",
    "利用大模型将原本的单一查询转变为多个子查询，然后将这些子查询分别做rag，将结果进行汇总，排序。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ！表示在jupter中运行shell命令\n",
    "# % 更安全，自动安装到当前的内核环境中\n",
    "%pip install langchain langchain_ollama\n",
    "%pip install chromadb langchain_chormadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"bge-m3\")\n",
    "\n",
    "llm = ChatOllama(model=\"huihui_ai/deepseek-r1-abliterated:7b\")\n",
    "result = llm.invoke(\"你好？\")\n",
    "print(result.content)\n",
    "\n",
    "for chunk in llm.stream(\"你是谁？\"):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量化存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用chromadb，将本地文件“chineseJH.txt”进行向量化存储\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "import os\n",
    "\n",
    "# 判断向量数据库目录是否存在\n",
    "db = None\n",
    "if not os.path.exists(\"chineseJH_chroma_db\") or not os.listdir(\"chineseJH_chroma_db\"):\n",
    "    # 加载文本文件\n",
    "    loader = TextLoader(\"./chineseJH.txt\", encoding=\"utf-8\")\n",
    "    documents = loader.load()\n",
    "\n",
    "    # 文本切分\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    # 创建Chroma向量数据库并存储\n",
    "    db = Chroma.from_documents(docs, embeddings, persist_directory=\"chineseJH_chroma_db\")\n",
    "    db.persist()\n",
    "    print(\"已成功将chineseJH.txt向量化并存储到chineseJH_chroma_db目录。\")\n",
    "else:\n",
    "    db = Chroma(persist_directory=\"chineseJH_chroma_db\", embedding_function=embeddings)\n",
    "    print(\"chineseJH_chroma_db 已存在，无需重复向量化。\")\n",
    "\n",
    "\n",
    "# if not os.path.exists(\"55_db\") or not os.listdir(\"55_db\"):\n",
    "#     # 加载文本文件\n",
    "#     loader = TextLoader(\"./1-55.txt\", encoding=\"utf-8\")\n",
    "#     documents = loader.load()\n",
    "\n",
    "#     # 文本切分\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "#     docs = text_splitter.split_documents(documents)\n",
    "\n",
    "#     # 创建Chroma向量数据库并存储\n",
    "#     db = Chroma.from_documents(docs, embeddings, persist_directory=\"55_db\")\n",
    "#     db.persist()\n",
    "#     print(\"已成功将chineseJH.txt向量化并存储到55_db目录。\")\n",
    "# else:\n",
    "#     db = Chroma(persist_directory=\"55_db\", embedding_function=embeddings)\n",
    "#     print(\"55_db 已存在，无需重复向量化。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用大模型生成同义问答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "template = \"\"\"\n",
    "请为下面的问题生成3个不同表达方式的同义句,并且每个问题占一行，问题之间没有空行，严格按照上述格式：\n",
    "问题：{query}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"query\"])\n",
    "\n",
    "\n",
    "# class LineListOutputParser(BaseOutputParser):\n",
    "#     def parse(self, text: str):\n",
    "#         # 按行分割，去除空行和首尾空白\n",
    "#         return [line.strip() for line in text.strip().split('\\n') if line.strip()]\n",
    "\n",
    "user_query = \"令狐冲最后怎么样了？\"\n",
    "get_multi_query_chain = prompt | llm | StrOutputParser() | (lambda x: [str.strip() for str in x.split('\\n')])\n",
    "\n",
    "multi_queries = get_multi_query_chain.invoke({\"query\":user_query})\n",
    "print(multi_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多查询检索合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps\n",
    "\n",
    "print(\"=\"*20 + \"测试查询\" + \"=\"*20)\n",
    "\n",
    "# 多query分别检索后合并\n",
    "retrieves = []\n",
    "for query in multi_queries:\n",
    "    similar_docs = db.similarity_search(query, k = 5)\n",
    "    retrieves.extend(similar_docs)\n",
    "print(retrieves)\n",
    "\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建链\n",
    "retriver = db.as_retriever()\n",
    "\n",
    "# 将docs列表[[],[],[],...]转化为list并去重\n",
    "def get_unique_doc(doc_lists):\n",
    "    final_list = []\n",
    "    final_set = set()\n",
    "    for doc_list in doc_lists:\n",
    "        for doc in doc_list:\n",
    "            if doc.page_content not in final_set:\n",
    "                final_set.add(doc.page_content)\n",
    "                final_list.append(doc)\n",
    "    return final_list\n",
    "\n",
    "\n",
    "retriver_chain = get_multi_query_chain | retriver.map() | get_unique_doc\n",
    "\n",
    "docs = retriver_chain.invoke({\"query\": \"令狐冲会的武功有哪些？\"})\n",
    "print(docs)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 整合生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "# 用于创造提取函数，从dict中提取字段的函数\n",
    "question_extractor = itemgetter(\"question\")  # 创建提取函数\n",
    "\n",
    "\n",
    "template = \"\"\"请根据以下【上下文】内容，认真回答【问题】。  \n",
    "【上下文】\n",
    "{context}\n",
    "\n",
    "【问题】\n",
    "{question}\n",
    "\n",
    "【请在下方作答】\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {  \n",
    "        \"context\": {\"query\" : itemgetter(\"question\") } | retriver_chain | StrOutputParser(),\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"=\"*50 + \"RAG效果\")\n",
    "\n",
    "# str = input(\"输入问题:\")\n",
    "# print(str)\n",
    "\n",
    "str = \"\"\n",
    "\n",
    "question = \"辟邪剑法都被谁拿到过？\" if str == \"\" else str\n",
    "for chunk in final_rag_chain.stream({\"question\": question}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"无RAG效果\")\n",
    "simple_chain = llm | StrOutputParser()\n",
    "for chunk in simple_chain.stream(question):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
