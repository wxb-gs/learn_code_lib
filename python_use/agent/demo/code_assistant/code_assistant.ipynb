{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain_community langchain-openai langchain-ollama langchain langgraph bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "\n",
    "# LCEL docs\n",
    "url = \"https://python.langchain.com/docs/concepts/lcel/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Sort the list based on the URLs and get the text\n",
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")\n",
    "print(concatenated_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试写代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【问题解析】\n",
      "\n",
      "To build a Retrieval-Augmented Generation (RAG) chain using LangChain Expression Language (LCEL), you need to combine several Runnables that work together to retrieve relevant documents and generate text based on those documents. Here's a step-by-step guide on how to do this:\n",
      "\n",
      "========================================\n",
      "\n",
      "【所需导入】\n",
      "\n",
      "from langchain.chains import RetrievalAugmentedGenerationChain\n",
      "from langchain.embeddings import Embeddings\n",
      "from langchain.vectorstores import VectorStore\n",
      "\n",
      "========================================\n",
      "\n",
      "【完整代码】\n",
      "\n",
      "```python\n",
      "# Step 1: Define the embeddings and vector store\n",
      "embeddings = Embeddings()\n",
      "vector_store = VectorStore(embeddings=embeddings)\n",
      "\n",
      "# Step 2: Create a RetrievalAugmentedGenerationChain\n",
      "rag_chain = RetrievalAugmentedGenerationChain(\n",
      "    retriever=vector_store.as_retriever(),\n",
      "    generator=generator,  # Replace with your LLM or chat model\n",
      "    prompt=prompt_template,  # Replace with your prompt template\n",
      ")\n",
      "\n",
      "# Step 3: Invoke the chain with input\n",
      "input_text = 'What is the capital of France?'  # Example query\n",
      "output = rag_chain.invoke(input_text)\n",
      "print(output)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_ollama import ChatOllama\n",
    "### OpenAI\n",
    "\n",
    "# Grader prompt\n",
    "code_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a coding assistant with expertise in LCEL, LangChain expression language. \\n \n",
    "    Here is a full set of LCEL documentation:  \\n ------- \\n  {context} \\n ------- \\n Answer the user \n",
    "    question based on the above provided documentation. Ensure any code you provide can be executed \\n \n",
    "    with all required imports and variables defined. Structure your answer with a description of the code solution. \\n\n",
    "    Then list the imports. And finally list the functioning code block. Here is the user question:\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Data model\n",
    "class code(BaseModel):\n",
    "    \"\"\"Schema for code solutions to questions about LCEL.\"\"\"\n",
    "\n",
    "    prefix: str = Field(description=\"Description of the problem and approach\")\n",
    "    imports: str = Field(description=\"Code block import statements\")\n",
    "    code: str = Field(description=\"Code block not including import statements\")\n",
    "\n",
    "\n",
    "expt_llm = \"qwen2.5:7b\"\n",
    "llm = ChatOllama(temperature=0, model=expt_llm)\n",
    "\n",
    "# 将输出结果结构化为对象\n",
    "code_gen_chain_oai = code_gen_prompt | llm.with_structured_output(code)\n",
    "question = \"How do I build a RAG chain in LCEL?\"\n",
    "solution = code_gen_chain_oai.invoke(\n",
    "    {\"context\": concatenated_content, \"messages\": [(\"user\", question)]}\n",
    ")\n",
    "# 美化输出\n",
    "print(\"【问题解析】\\n\")\n",
    "print(solution.prefix)\n",
    "print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "print(\"【所需导入】\\n\")\n",
    "print(solution.imports)\n",
    "print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "print(\"【完整代码】\\n\")\n",
    "print(solution.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试纠正代码\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "### Anthropic\n",
    "\n",
    "# Prompt to enforce tool use\n",
    "code_gen_prompt_ollama = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"<instructions> You are a coding assistant with expertise in LCEL, LangChain expression language. \\n \n",
    "    Here is the LCEL documentation:  \\n ------- \\n  {context} \\n ------- \\n Answer the user  question based on the \\n \n",
    "    above provided documentation. Ensure any code you provide can be executed with all required imports and variables \\n\n",
    "    defined. Structure your answer: 1) a prefix describing the code solution, 2) the imports, 3) the functioning code block. \\n\n",
    "    Invoke the code tool to structure the output correctly. </instructions> \\n Here is the user question:\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# LLM\n",
    "expt_llm = \"qwen2.5:7b\"  # 或 \"llama3\"、\"phi3\" 等\n",
    "llm = ChatOllama(\n",
    "    model=expt_llm,\n",
    "    temperature=0,\n",
    ")\n",
    "structured_llm_ollama = llm.with_structured_output(code, include_raw=True)\n",
    "\n",
    "\n",
    "# Optional: Check for errors in case tool use is flaky\n",
    "def check_claude_output(tool_output):\n",
    "    \"\"\"Check for parse error or failure to call the tool\"\"\"\n",
    "\n",
    "    # Error with parsing\n",
    "    if tool_output[\"parsing_error\"]:\n",
    "        # Report back output and parsing errors\n",
    "        print(\"Parsing error!\")\n",
    "        raw_output = str(tool_output[\"raw\"].content)\n",
    "        error = tool_output[\"parsing_error\"]\n",
    "        raise ValueError(\n",
    "            f\"Error parsing your output! Be sure to invoke the tool. Output: {raw_output}. \\n Parse error: {error}\"\n",
    "        )\n",
    "\n",
    "    # Tool was not invoked\n",
    "    elif not tool_output[\"parsed\"]:\n",
    "        print(\"Failed to invoke tool!\")\n",
    "        raise ValueError(\n",
    "            \"You did not use the provided tool! Be sure to invoke the tool to structure the output.\"\n",
    "        )\n",
    "    return tool_output\n",
    "\n",
    "\n",
    "# Chain with output check\n",
    "code_chain_claude_raw = (\n",
    "    code_gen_prompt_ollama | structured_llm_ollama | check_claude_output\n",
    ")\n",
    "\n",
    "\n",
    "def insert_errors(inputs):\n",
    "    \"\"\"Insert errors for tool parsing in the messages\"\"\"\n",
    "\n",
    "    # Get errors\n",
    "    error = inputs[\"error\"]\n",
    "    messages = inputs[\"messages\"]\n",
    "    messages += [\n",
    "        (\n",
    "            \"assistant\",\n",
    "            f\"Retry. You are required to fix the parsing errors: {error} \\n\\n You must invoke the provided tool.\",\n",
    "        )\n",
    "    ]\n",
    "    return {\n",
    "        \"messages\": messages,\n",
    "        \"context\": inputs[\"context\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# This will be run as a fallback chain\n",
    "fallback_chain = insert_errors | code_chain_claude_raw\n",
    "N = 3  # Max re-tries\n",
    "code_gen_chain_re_try = code_chain_claude_raw.with_fallbacks(\n",
    "    fallbacks=[fallback_chain] * N, exception_key=\"error\"\n",
    ")\n",
    "\n",
    "\n",
    "def parse_output(solution):\n",
    "    \"\"\"When we add 'include_raw=True' to structured output,\n",
    "    it will return a dict w 'raw', 'parsed', 'parsing_error'.\"\"\"\n",
    "\n",
    "    return solution[\"parsed\"]\n",
    "\n",
    "\n",
    "# Optional: With re-try to correct for failure to invoke tool\n",
    "code_gen_chain = code_gen_chain_re_try | parse_output\n",
    "\n",
    "# No re-try\n",
    "code_gen_chain = code_gen_prompt_ollama | structured_llm_ollama | parse_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【问题解析】\n",
      "\n",
      "To build a Retrieval-Augmented Generation (RAG) chain using LangChain Expression Language (LCEL), you can follow these steps. This example will demonstrate how to create a simple RAG chain that retrieves documents from a vector store and uses them to augment the input before passing it through an LLM.\n",
      "\n",
      "========================================\n",
      "\n",
      "【所需导入】\n",
      "\n",
      "from langchain.chains import RetrievalQA\n",
      "from langchain.embeddings import Embeddings\n",
      "from langchain.vectorstores import VectorStore\n",
      "\n",
      "========================================\n",
      "\n",
      "【完整代码】\n",
      "\n",
      "class SimpleRagChain(RetrievalQA):\n",
      "    def __init__(self, embeddings: Embeddings, vectorstore: VectorStore, llm, **kwargs):\n",
      "        super().__init__(embeddings=embeddings, retriever=vectorstore.as_retriever(), llm=llm, **kwargs)\n",
      "\n",
      "# Example usage\n",
      "from langchain.embeddings import HuggingFaceEmbeddings\n",
      "from langchain.vectorstores import FAISS\n",
      "from langchain.llms import OpenAI\n",
      "\n",
      "embeddings = HuggingFaceEmbeddings()\n",
      "vectorstore = FAISS.load_local('path/to/vectorstore', embeddings)\n",
      "llm = OpenAI()\n",
      "rag_chain = SimpleRagChain(embeddings=embeddings, vectorstore=vectorstore, llm=llm)\n",
      "\n",
      "response = rag_chain.run(input='What is the capital of France?')\n",
      "print(response)\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "question = \"How do I build a RAG chain in LCEL?\"\n",
    "solution = code_gen_chain.invoke(\n",
    "    {\"context\": concatenated_content, \"messages\": [(\"user\", question)]}\n",
    ")\n",
    "# 美化输出\n",
    "print(\"【问题解析】\\n\")\n",
    "print(solution.prefix)\n",
    "print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "print(\"【所需导入】\\n\")\n",
    "print(solution.imports)\n",
    "print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "print(\"【完整代码】\\n\")\n",
    "print(solution.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.16 ('agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "323adcefd241dc7b9f6af1af4db666d10a12d12d476723437e467064bc267131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
