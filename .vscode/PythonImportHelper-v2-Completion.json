[
    {
        "label": "StateGraph",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "END",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "START",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "MessagesState",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "StateGraph",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "OllamaLLM",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "AIMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "BaseMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "SystemMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "trim_messages",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio",
        "description": "asyncio",
        "detail": "asyncio",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "sleep",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "sleep",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "MessagesPlaceholder",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "MessagesPlaceholder",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "StrOutputParser",
        "importPath": "langchain_core.output_parsers",
        "description": "langchain_core.output_parsers",
        "isExtraImport": true,
        "detail": "langchain_core.output_parsers",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "uuid",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uuid",
        "description": "uuid",
        "detail": "uuid",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "importPath": "deepeval",
        "description": "deepeval",
        "isExtraImport": true,
        "detail": "deepeval",
        "documentation": {}
    },
    {
        "label": "LLMTestCase",
        "importPath": "deepeval.test_case",
        "description": "deepeval.test_case",
        "isExtraImport": true,
        "detail": "deepeval.test_case",
        "documentation": {}
    },
    {
        "label": "AnswerRelevancyMetric",
        "importPath": "deepeval.metrics",
        "description": "deepeval.metrics",
        "isExtraImport": true,
        "detail": "deepeval.metrics",
        "documentation": {}
    },
    {
        "label": "HallucinationMetric",
        "importPath": "deepeval.metrics",
        "description": "deepeval.metrics",
        "isExtraImport": true,
        "detail": "deepeval.metrics",
        "documentation": {}
    },
    {
        "label": "DeepEvalCallbackHandler",
        "importPath": "langchain_community.callbacks.confident_callback",
        "description": "langchain_community.callbacks.confident_callback",
        "isExtraImport": true,
        "detail": "langchain_community.callbacks.confident_callback",
        "documentation": {}
    },
    {
        "label": "OllamaModel",
        "importPath": "deepeval.models",
        "description": "deepeval.models",
        "isExtraImport": true,
        "detail": "deepeval.models",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "langgraph_use",
        "description": "langgraph_use",
        "isExtraImport": true,
        "detail": "langgraph_use",
        "documentation": {}
    },
    {
        "label": "tool",
        "importPath": "langchain_core.tools",
        "description": "langchain_core.tools",
        "isExtraImport": true,
        "detail": "langchain_core.tools",
        "documentation": {}
    },
    {
        "label": "tool",
        "importPath": "langchain_core.tools",
        "description": "langchain_core.tools",
        "isExtraImport": true,
        "detail": "langchain_core.tools",
        "documentation": {}
    },
    {
        "label": "MemorySaver",
        "importPath": "langgraph.checkpoint.memory",
        "description": "langgraph.checkpoint.memory",
        "isExtraImport": true,
        "detail": "langgraph.checkpoint.memory",
        "documentation": {}
    },
    {
        "label": "MemorySaver",
        "importPath": "langgraph.checkpoint.memory",
        "description": "langgraph.checkpoint.memory",
        "isExtraImport": true,
        "detail": "langgraph.checkpoint.memory",
        "documentation": {}
    },
    {
        "label": "create_react_agent",
        "importPath": "langgraph.prebuilt",
        "description": "langgraph.prebuilt",
        "isExtraImport": true,
        "detail": "langgraph.prebuilt",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "IPython.display",
        "description": "IPython.display",
        "isExtraImport": true,
        "detail": "IPython.display",
        "documentation": {}
    },
    {
        "label": "display",
        "importPath": "IPython.display",
        "description": "IPython.display",
        "isExtraImport": true,
        "detail": "IPython.display",
        "documentation": {}
    },
    {
        "label": "Memory",
        "importPath": "mem0",
        "description": "mem0",
        "isExtraImport": true,
        "detail": "mem0",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "TextLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "TextLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "TextLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "TextLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "PyPDFLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "load_summarize_chain",
        "importPath": "langchain.chains.summarize",
        "description": "langchain.chains.summarize",
        "isExtraImport": true,
        "detail": "langchain.chains.summarize",
        "documentation": {}
    },
    {
        "label": "load_summarize_chain",
        "importPath": "langchain.chains.summarize",
        "description": "langchain.chains.summarize",
        "isExtraImport": true,
        "detail": "langchain.chains.summarize",
        "documentation": {}
    },
    {
        "label": "load_summarize_chain",
        "importPath": "langchain.chains.summarize",
        "description": "langchain.chains.summarize",
        "isExtraImport": true,
        "detail": "langchain.chains.summarize",
        "documentation": {}
    },
    {
        "label": "RecursiveCharacterTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "RecursiveCharacterTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "RecursiveCharacterTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "TokenTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "RecursiveCharacterTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "PythonREPL",
        "importPath": "langchain_experimental.utilities",
        "description": "langchain_experimental.utilities",
        "isExtraImport": true,
        "detail": "langchain_experimental.utilities",
        "documentation": {}
    },
    {
        "label": "RunnableLambda",
        "importPath": "langchain_core.runnables",
        "description": "langchain_core.runnables",
        "isExtraImport": true,
        "detail": "langchain_core.runnables",
        "documentation": {}
    },
    {
        "label": "RunnableConfig",
        "importPath": "langchain_core.runnables",
        "description": "langchain_core.runnables",
        "isExtraImport": true,
        "detail": "langchain_core.runnables",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "getModel",
        "importPath": "chatbot.core.load_model.get_model",
        "description": "chatbot.core.load_model.get_model",
        "isExtraImport": true,
        "detail": "chatbot.core.load_model.get_model",
        "documentation": {}
    },
    {
        "label": "getModel",
        "importPath": "chatbot.core.load_model.get_model",
        "description": "chatbot.core.load_model.get_model",
        "isExtraImport": true,
        "detail": "chatbot.core.load_model.get_model",
        "documentation": {}
    },
    {
        "label": "militory",
        "importPath": "chatbot.utils.prompts",
        "description": "chatbot.utils.prompts",
        "isExtraImport": true,
        "detail": "chatbot.utils.prompts",
        "documentation": {}
    },
    {
        "label": "getBg",
        "importPath": "chatbot.bg.bg",
        "description": "chatbot.bg.bg",
        "isExtraImport": true,
        "detail": "chatbot.bg.bg",
        "documentation": {}
    },
    {
        "label": "fg",
        "importPath": "chatbot.fg.fg",
        "description": "chatbot.fg.fg",
        "isExtraImport": true,
        "detail": "chatbot.fg.fg",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "get_model",
        "importPath": "chatbot.core.load_model",
        "description": "chatbot.core.load_model",
        "isExtraImport": true,
        "detail": "chatbot.core.load_model",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "QtCore",
        "importPath": "PyQt5",
        "description": "PyQt5",
        "isExtraImport": true,
        "detail": "PyQt5",
        "documentation": {}
    },
    {
        "label": "QtGui",
        "importPath": "PyQt5",
        "description": "PyQt5",
        "isExtraImport": true,
        "detail": "PyQt5",
        "documentation": {}
    },
    {
        "label": "QtWidgets",
        "importPath": "PyQt5",
        "description": "PyQt5",
        "isExtraImport": true,
        "detail": "PyQt5",
        "documentation": {}
    },
    {
        "label": "QtCore",
        "importPath": "PyQt5",
        "description": "PyQt5",
        "isExtraImport": true,
        "detail": "PyQt5",
        "documentation": {}
    },
    {
        "label": "QtGui",
        "importPath": "PyQt5",
        "description": "PyQt5",
        "isExtraImport": true,
        "detail": "PyQt5",
        "documentation": {}
    },
    {
        "label": "QtWidgets",
        "importPath": "PyQt5",
        "description": "PyQt5",
        "isExtraImport": true,
        "detail": "PyQt5",
        "documentation": {}
    },
    {
        "label": "QSpacerItem",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSizePolicy",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QWidget",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QApplication",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSizePolicy",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QDesktopWidget",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QGraphicsDropShadowEffect",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSpacerItem",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QPixmap",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QColor",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "pyqtSignal",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "QThread",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "QBasicTimer",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "Ui_loadwin",
        "importPath": "loadwin_ui",
        "description": "loadwin_ui",
        "isExtraImport": true,
        "detail": "loadwin_ui",
        "documentation": {}
    },
    {
        "label": "Ui_mainwin",
        "importPath": "Ui_mainwin",
        "description": "Ui_mainwin",
        "isExtraImport": true,
        "detail": "Ui_mainwin",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "find_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain_core.documents",
        "description": "langchain_core.documents",
        "isExtraImport": true,
        "detail": "langchain_core.documents",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "load_pdf_documents",
        "importPath": "load_file",
        "description": "load_file",
        "isExtraImport": true,
        "detail": "load_file",
        "documentation": {}
    },
    {
        "label": "textwrap",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "textwrap",
        "description": "textwrap",
        "detail": "textwrap",
        "documentation": {}
    },
    {
        "label": "gradio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gradio",
        "description": "gradio",
        "detail": "gradio",
        "documentation": {}
    },
    {
        "label": "ChatMessageHistory",
        "importPath": "langchain_community.chat_message_histories",
        "description": "langchain_community.chat_message_histories",
        "isExtraImport": true,
        "detail": "langchain_community.chat_message_histories",
        "documentation": {}
    },
    {
        "label": "ChatMessageHistory",
        "importPath": "langchain_community.chat_message_histories",
        "description": "langchain_community.chat_message_histories",
        "isExtraImport": true,
        "detail": "langchain_community.chat_message_histories",
        "documentation": {}
    },
    {
        "label": "RunnableWithMessageHistory",
        "importPath": "langchain_core.runnables.history",
        "description": "langchain_core.runnables.history",
        "isExtraImport": true,
        "detail": "langchain_core.runnables.history",
        "documentation": {}
    },
    {
        "label": "RunnableWithMessageHistory",
        "importPath": "langchain_core.runnables.history",
        "description": "langchain_core.runnables.history",
        "isExtraImport": true,
        "detail": "langchain_core.runnables.history",
        "documentation": {}
    },
    {
        "label": "json5",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json5",
        "description": "json5",
        "detail": "json5",
        "documentation": {}
    },
    {
        "label": "InternLM2Chat",
        "importPath": "tinyAgent.LLM",
        "description": "tinyAgent.LLM",
        "isExtraImport": true,
        "detail": "tinyAgent.LLM",
        "documentation": {}
    },
    {
        "label": "Tools",
        "importPath": "tinyAgent.tool",
        "description": "tinyAgent.tool",
        "isExtraImport": true,
        "detail": "tinyAgent.tool",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "LlamaTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "LlamaForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "LlamaTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "LlamaForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "CLIPVisionModelWithProjection",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "os,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.",
        "description": "os.",
        "detail": "os.",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "torchvision",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision",
        "description": "torchvision",
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "TensorDataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "torchvision.models",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.models",
        "description": "torchvision.models",
        "detail": "torchvision.models",
        "documentation": {}
    },
    {
        "label": "linalg",
        "importPath": "scipy",
        "description": "scipy",
        "isExtraImport": true,
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "SimpleUnet",
        "importPath": "unet",
        "description": "unet",
        "isExtraImport": true,
        "detail": "unet",
        "documentation": {}
    },
    {
        "label": "SimpleUnet",
        "importPath": "unet",
        "description": "unet",
        "isExtraImport": true,
        "detail": "unet",
        "documentation": {}
    },
    {
        "label": "NoiseScheduler",
        "importPath": "diffusion",
        "description": "diffusion",
        "isExtraImport": true,
        "detail": "diffusion",
        "documentation": {}
    },
    {
        "label": "NoiseScheduler",
        "importPath": "diffusion",
        "description": "diffusion",
        "isExtraImport": true,
        "detail": "diffusion",
        "documentation": {}
    },
    {
        "label": "torch.optim",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.optim",
        "description": "torch.optim",
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "load_transformed_dataset",
        "importPath": "dataloader",
        "description": "dataloader",
        "isExtraImport": true,
        "detail": "dataloader",
        "documentation": {}
    },
    {
        "label": "sample",
        "importPath": "sample",
        "description": "sample",
        "isExtraImport": true,
        "detail": "sample",
        "documentation": {}
    },
    {
        "label": "plot",
        "importPath": "sample",
        "description": "sample",
        "isExtraImport": true,
        "detail": "sample",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "PeftModel",
        "importPath": "peft",
        "description": "peft",
        "isExtraImport": true,
        "detail": "peft",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "string",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "string",
        "description": "string",
        "detail": "string",
        "documentation": {}
    },
    {
        "label": "jieba",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "jieba",
        "description": "jieba",
        "detail": "jieba",
        "documentation": {}
    },
    {
        "label": "Rouge",
        "importPath": "rouge",
        "description": "rouge",
        "isExtraImport": true,
        "detail": "rouge",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "qa_f1_score",
        "importPath": "Eval.metrics",
        "description": "Eval.metrics",
        "isExtraImport": true,
        "detail": "Eval.metrics",
        "documentation": {}
    },
    {
        "label": "qa_f1_zh_score",
        "importPath": "Eval.metrics",
        "description": "Eval.metrics",
        "isExtraImport": true,
        "detail": "Eval.metrics",
        "documentation": {}
    },
    {
        "label": "rouge_score",
        "importPath": "Eval.metrics",
        "description": "Eval.metrics",
        "isExtraImport": true,
        "detail": "Eval.metrics",
        "documentation": {}
    },
    {
        "label": "classification_score",
        "importPath": "Eval.metrics",
        "description": "Eval.metrics",
        "isExtraImport": true,
        "detail": "Eval.metrics",
        "documentation": {}
    },
    {
        "label": "rouge_zh_score",
        "importPath": "Eval.metrics",
        "description": "Eval.metrics",
        "isExtraImport": true,
        "detail": "Eval.metrics",
        "documentation": {}
    },
    {
        "label": "GAOKAO_math",
        "importPath": "Eval.metrics",
        "description": "Eval.metrics",
        "isExtraImport": true,
        "detail": "Eval.metrics",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "internlm2Chat",
        "importPath": "Eval.model.LLM",
        "description": "Eval.model.LLM",
        "isExtraImport": true,
        "detail": "Eval.model.LLM",
        "documentation": {}
    },
    {
        "label": "Qwen2Chat",
        "importPath": "Eval.model.LLM",
        "description": "Eval.model.LLM",
        "isExtraImport": true,
        "detail": "Eval.model.LLM",
        "documentation": {}
    },
    {
        "label": "base64",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "base64",
        "description": "base64",
        "detail": "base64",
        "documentation": {}
    },
    {
        "label": "modelscope",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "modelscope",
        "description": "modelscope",
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "Qwen2_5_VLForConditionalGeneration",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoProcessor",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "snapshot_download",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoModel",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "process_vision_info",
        "importPath": "qwen_vl_utils",
        "description": "qwen_vl_utils",
        "isExtraImport": true,
        "detail": "qwen_vl_utils",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "AutoPipelineForText2Image",
        "importPath": "diffusers",
        "description": "diffusers",
        "isExtraImport": true,
        "detail": "diffusers",
        "documentation": {}
    },
    {
        "label": "clip",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "clip",
        "description": "clip",
        "detail": "clip",
        "documentation": {}
    },
    {
        "label": "hashlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hashlib",
        "description": "hashlib",
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "urllib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "urllib",
        "description": "urllib",
        "detail": "urllib",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "version",
        "importPath": "packaging",
        "description": "packaging",
        "isExtraImport": true,
        "detail": "packaging",
        "documentation": {}
    },
    {
        "label": "Compose",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Resize",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "CenterCrop",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "ToTensor",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Normalize",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "gzip",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gzip",
        "description": "gzip",
        "detail": "gzip",
        "documentation": {}
    },
    {
        "label": "html",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "html",
        "description": "html",
        "detail": "html",
        "documentation": {}
    },
    {
        "label": "lru_cache",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "ftfy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ftfy",
        "description": "ftfy",
        "detail": "ftfy",
        "documentation": {}
    },
    {
        "label": "regex",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "regex",
        "description": "regex",
        "detail": "regex",
        "documentation": {}
    },
    {
        "label": "pytest",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pytest",
        "description": "pytest",
        "detail": "pytest",
        "documentation": {}
    },
    {
        "label": "tokenize",
        "importPath": "clip.clip",
        "description": "clip.clip",
        "isExtraImport": true,
        "detail": "clip.clip",
        "documentation": {}
    },
    {
        "label": "load",
        "importPath": "clip.clip",
        "description": "clip.clip",
        "isExtraImport": true,
        "detail": "clip.clip",
        "documentation": {}
    },
    {
        "label": "available_models",
        "importPath": "clip.clip",
        "description": "clip.clip",
        "isExtraImport": true,
        "detail": "clip.clip",
        "documentation": {}
    },
    {
        "label": "pkg_resources",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pkg_resources",
        "description": "pkg_resources",
        "detail": "pkg_resources",
        "documentation": {}
    },
    {
        "label": "huggingface_hub",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "huggingface_hub",
        "description": "huggingface_hub",
        "detail": "huggingface_hub",
        "documentation": {}
    },
    {
        "label": "SDXLGenerator",
        "importPath": "IMGRAG.ImgGenerator",
        "description": "IMGRAG.ImgGenerator",
        "isExtraImport": true,
        "detail": "IMGRAG.ImgGenerator",
        "documentation": {}
    },
    {
        "label": "load_qwen_vlm",
        "importPath": "IMGRAG.ImgEvaluator",
        "description": "IMGRAG.ImgEvaluator",
        "isExtraImport": true,
        "detail": "IMGRAG.ImgEvaluator",
        "documentation": {}
    },
    {
        "label": "run_qwen_vl",
        "importPath": "IMGRAG.ImgEvaluator",
        "description": "IMGRAG.ImgEvaluator",
        "isExtraImport": true,
        "detail": "IMGRAG.ImgEvaluator",
        "documentation": {}
    },
    {
        "label": "load_qwen_llm",
        "importPath": "IMGRAG.RewritePrompt",
        "description": "IMGRAG.RewritePrompt",
        "isExtraImport": true,
        "detail": "IMGRAG.RewritePrompt",
        "documentation": {}
    },
    {
        "label": "run_qwen_llm",
        "importPath": "IMGRAG.RewritePrompt",
        "description": "IMGRAG.RewritePrompt",
        "isExtraImport": true,
        "detail": "IMGRAG.RewritePrompt",
        "documentation": {}
    },
    {
        "label": "get_clip_similarities",
        "importPath": "IMGRAG.ImgRetrieval",
        "description": "IMGRAG.ImgRetrieval",
        "isExtraImport": true,
        "detail": "IMGRAG.ImgRetrieval",
        "documentation": {}
    },
    {
        "label": "struct",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "struct",
        "description": "struct",
        "detail": "struct",
        "documentation": {}
    },
    {
        "label": "inspect",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "inspect",
        "description": "inspect",
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "ProcessPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "sentencepiece",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sentencepiece",
        "description": "sentencepiece",
        "detail": "sentencepiece",
        "documentation": {}
    },
    {
        "label": "SentencePieceProcessor",
        "importPath": "sentencepiece",
        "description": "sentencepiece",
        "isExtraImport": true,
        "detail": "sentencepiece",
        "documentation": {}
    },
    {
        "label": "torch.distributed",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.distributed",
        "description": "torch.distributed",
        "detail": "torch.distributed",
        "documentation": {}
    },
    {
        "label": "Tokenizer",
        "importPath": "tokenizer",
        "description": "tokenizer",
        "isExtraImport": true,
        "detail": "tokenizer",
        "documentation": {}
    },
    {
        "label": "Tokenizer",
        "importPath": "tokenizer",
        "description": "tokenizer",
        "isExtraImport": true,
        "detail": "tokenizer",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "nullcontext",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "nullcontext",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "ModelArgs",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "ModelArgs",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "Task",
        "importPath": "preprocess",
        "description": "preprocess",
        "isExtraImport": true,
        "detail": "preprocess",
        "documentation": {}
    },
    {
        "label": "copy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "PyPDF2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "PyPDF2",
        "description": "PyPDF2",
        "detail": "PyPDF2",
        "documentation": {}
    },
    {
        "label": "markdown",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "markdown",
        "description": "markdown",
        "detail": "markdown",
        "documentation": {}
    },
    {
        "label": "html2text",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "html2text",
        "description": "html2text",
        "detail": "html2text",
        "documentation": {}
    },
    {
        "label": "tiktoken",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tiktoken",
        "description": "tiktoken",
        "detail": "tiktoken",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "BaseEmbeddings",
        "importPath": "RAG.Embeddings",
        "description": "RAG.Embeddings",
        "isExtraImport": true,
        "detail": "RAG.Embeddings",
        "documentation": {}
    },
    {
        "label": "OpenAIEmbedding",
        "importPath": "RAG.Embeddings",
        "description": "RAG.Embeddings",
        "isExtraImport": true,
        "detail": "RAG.Embeddings",
        "documentation": {}
    },
    {
        "label": "JinaEmbedding",
        "importPath": "RAG.Embeddings",
        "description": "RAG.Embeddings",
        "isExtraImport": true,
        "detail": "RAG.Embeddings",
        "documentation": {}
    },
    {
        "label": "ZhipuEmbedding",
        "importPath": "RAG.Embeddings",
        "description": "RAG.Embeddings",
        "isExtraImport": true,
        "detail": "RAG.Embeddings",
        "documentation": {}
    },
    {
        "label": "GraphState",
        "kind": 6,
        "importPath": "python_use.agent.langchain_use.demo.test",
        "description": "python_use.agent.langchain_use.demo.test",
        "peekOfCode": "class GraphState(dict):\n    pass\n# 1 LLM\ndef answer_question(state: GraphState) -> GraphState:\n    messages = state[\"messages\"]\n    response = llm(messages)\n    state[\"messages\"].append({\"role\": \"assistant\", \"content\": response.content})\n    return state\n# 1. \ndef should_search(state: GraphState) -> str:",
        "detail": "python_use.agent.langchain_use.demo.test",
        "documentation": {}
    },
    {
        "label": "answer_question",
        "kind": 2,
        "importPath": "python_use.agent.langchain_use.demo.test",
        "description": "python_use.agent.langchain_use.demo.test",
        "peekOfCode": "def answer_question(state: GraphState) -> GraphState:\n    messages = state[\"messages\"]\n    response = llm(messages)\n    state[\"messages\"].append({\"role\": \"assistant\", \"content\": response.content})\n    return state\n# 1. \ndef should_search(state: GraphState) -> str:\n    question = state[\"messages\"][-1][\"content\"]\n    if \"latest\" in question or \"current\" in question:\n        return \"search\"",
        "detail": "python_use.agent.langchain_use.demo.test",
        "documentation": {}
    },
    {
        "label": "should_search",
        "kind": 2,
        "importPath": "python_use.agent.langchain_use.demo.test",
        "description": "python_use.agent.langchain_use.demo.test",
        "peekOfCode": "def should_search(state: GraphState) -> str:\n    question = state[\"messages\"][-1][\"content\"]\n    if \"latest\" in question or \"current\" in question:\n        return \"search\"\n    return \"answer\"\n# 2. \ndef search_web(state: GraphState) -> GraphState:\n    # \n    result = \"According to recent sources, ...\"\n    state[\"messages\"].append({\"role\": \"function\", \"name\": \"search_web\", \"content\": result})",
        "detail": "python_use.agent.langchain_use.demo.test",
        "documentation": {}
    },
    {
        "label": "search_web",
        "kind": 2,
        "importPath": "python_use.agent.langchain_use.demo.test",
        "description": "python_use.agent.langchain_use.demo.test",
        "peekOfCode": "def search_web(state: GraphState) -> GraphState:\n    # \n    result = \"According to recent sources, ...\"\n    state[\"messages\"].append({\"role\": \"function\", \"name\": \"search_web\", \"content\": result})\n    return state\n# \ngraph = StateGraph(GraphState)\n# \ngraph.add_node(\"answer\", answer_question)\ngraph.add_node(\"search\", search_web)",
        "detail": "python_use.agent.langchain_use.demo.test",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "python_use.agent.langchain_use.demo.test",
        "description": "python_use.agent.langchain_use.demo.test",
        "peekOfCode": "llm = ChatOllama(model=\"qwen2.5:7b\")\n# \nclass GraphState(dict):\n    pass\n# 1 LLM\ndef answer_question(state: GraphState) -> GraphState:\n    messages = state[\"messages\"]\n    response = llm(messages)\n    state[\"messages\"].append({\"role\": \"assistant\", \"content\": response.content})\n    return state",
        "detail": "python_use.agent.langchain_use.demo.test",
        "documentation": {}
    },
    {
        "label": "graph",
        "kind": 5,
        "importPath": "python_use.agent.langchain_use.demo.test",
        "description": "python_use.agent.langchain_use.demo.test",
        "peekOfCode": "graph = StateGraph(GraphState)\n# \ngraph.add_node(\"answer\", answer_question)\ngraph.add_node(\"search\", search_web)\n# \ngraph.add_conditional_edges(\"router\", should_search, {\n    \"answer\": \"answer\",\n    \"search\": \"search\",\n})\n# ",
        "detail": "python_use.agent.langchain_use.demo.test",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "python_use.agent.langchain_use.demo.test",
        "description": "python_use.agent.langchain_use.demo.test",
        "peekOfCode": "app = graph.compile()\n# \nstate = GraphState(messages=[HumanMessage(content=\"What is the latest news about AI?\")])\nresult = app.invoke(state)\nprint(result[\"messages\"][-1][\"content\"])",
        "detail": "python_use.agent.langchain_use.demo.test",
        "documentation": {}
    },
    {
        "label": "state",
        "kind": 5,
        "importPath": "python_use.agent.langchain_use.demo.test",
        "description": "python_use.agent.langchain_use.demo.test",
        "peekOfCode": "state = GraphState(messages=[HumanMessage(content=\"What is the latest news about AI?\")])\nresult = app.invoke(state)\nprint(result[\"messages\"][-1][\"content\"])",
        "detail": "python_use.agent.langchain_use.demo.test",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "python_use.agent.langchain_use.demo.test",
        "description": "python_use.agent.langchain_use.demo.test",
        "peekOfCode": "result = app.invoke(state)\nprint(result[\"messages\"][-1][\"content\"])",
        "detail": "python_use.agent.langchain_use.demo.test",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "python_use.agent.langchain_use.chat",
        "description": "python_use.agent.langchain_use.chat",
        "peekOfCode": "model = ChatOpenAI(model=\"qwen2:7b\", base_url=\"127.0.0.1:10087\")  # \nasync def run():\n    # 1: \n    print(\"\\n--- 1 ---\")\n    async for chunk in model.astream(\"\"):\n        print(chunk.content, end=\"\", flush=True)\n    print(\"next???????\")\n    # 2: LCEL\n    print(\"\\n\\n--- 2 ---\")\n    prompt = ChatPromptTemplate.from_template(\"{topic}\")",
        "detail": "python_use.agent.langchain_use.chat",
        "documentation": {}
    },
    {
        "label": "evaluate_agent",
        "kind": 2,
        "importPath": "python_use.agent.langchain_use.eval_langgraph_agent",
        "description": "python_use.agent.langchain_use.eval_langgraph_agent",
        "peekOfCode": "def evaluate_agent():\n    test_results = []\n    thread_id = uuid.uuid4()\n    config = {\"configurable\": {\"thread_id\": thread_id}}\n    for case in test_cases:\n        # \n        messages = []\n        input_msg = HumanMessage(content=case[\"input\"])\n        for event in app.stream(\n            {\"messages\": [input_msg]}, ",
        "detail": "python_use.agent.langchain_use.eval_langgraph_agent",
        "documentation": {}
    },
    {
        "label": "print_results",
        "kind": 2,
        "importPath": "python_use.agent.langchain_use.eval_langgraph_agent",
        "description": "python_use.agent.langchain_use.eval_langgraph_agent",
        "peekOfCode": "def print_results(results):\n    print(\"=\"*50)\n    print(\"LangGraph\")\n    print(\"=\"*50)\n    for i, res in enumerate(results, 1):\n        print(f\"\\n #{i}: {res['description']}\")\n        print(f\": {res['input']}\")\n        print(f\": {res['expected']}\")\n        print(f\": {res['actual']}\")\n        print(\"\\n:\")",
        "detail": "python_use.agent.langchain_use.eval_langgraph_agent",
        "documentation": {}
    },
    {
        "label": "eval_model",
        "kind": 5,
        "importPath": "python_use.agent.langchain_use.eval_langgraph_agent",
        "description": "python_use.agent.langchain_use.eval_langgraph_agent",
        "peekOfCode": "eval_model = OllamaModel(model='qwen2.5:7b')\n# \nmetrics = [\n    AnswerRelevancyMetric(threshold=0.7, model=eval_model),\n    HallucinationMetric(threshold=0.8, model=eval_model),\n]\n# # \n# # \n# callback_handler = DeepEvalCallbackHandler(metrics=metrics)\n# ",
        "detail": "python_use.agent.langchain_use.eval_langgraph_agent",
        "documentation": {}
    },
    {
        "label": "metrics",
        "kind": 5,
        "importPath": "python_use.agent.langchain_use.eval_langgraph_agent",
        "description": "python_use.agent.langchain_use.eval_langgraph_agent",
        "peekOfCode": "metrics = [\n    AnswerRelevancyMetric(threshold=0.7, model=eval_model),\n    HallucinationMetric(threshold=0.8, model=eval_model),\n]\n# # \n# # \n# callback_handler = DeepEvalCallbackHandler(metrics=metrics)\n# \ntest_cases = [\n    {",
        "detail": "python_use.agent.langchain_use.eval_langgraph_agent",
        "documentation": {}
    },
    {
        "label": "test_cases",
        "kind": 5,
        "importPath": "python_use.agent.langchain_use.eval_langgraph_agent",
        "description": "python_use.agent.langchain_use.eval_langgraph_agent",
        "peekOfCode": "test_cases = [\n    {\n        \"input\": \"hi! I'm bob. What is my age?\",\n        \"expected_output\": \"42 years old\",\n        \"context\": [\"User name is Bob\"],\n        \"description\": \"\"\n    },\n    {\n        \"input\": \"do you remember my name?\",\n        \"expected_output\": \"Bob\",",
        "detail": "python_use.agent.langchain_use.eval_langgraph_agent",
        "documentation": {}
    },
    {
        "label": "get_user_age",
        "kind": 2,
        "importPath": "python_use.agent.langchain_use.langgraph_use",
        "description": "python_use.agent.langchain_use.langgraph_use",
        "peekOfCode": "def get_user_age(name: str) -> str:\n    \"\"\"Use this tool to find the user's age.\"\"\"\n    # This is a placeholder for the actual implementation\n    if \"bob\" in name.lower():\n        return \"42 years old\"\n    return \"41 years old\"\nmemory = MemorySaver()\nmodel = ChatOllama(model=\"Qwen2.5-7B-Instruct\")\n# \ndef prompt(state) -> list[BaseMessage]:",
        "detail": "python_use.agent.langchain_use.langgraph_use",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 2,
        "importPath": "python_use.agent.langchain_use.langgraph_use",
        "description": "python_use.agent.langchain_use.langgraph_use",
        "peekOfCode": "def prompt(state) -> list[BaseMessage]:\n    \"\"\"Given the agent state, return a list of messages for the chat model.\"\"\"\n    # We're using the message processor defined above.\n    return trim_messages(\n        state[\"messages\"],\n        token_counter=len,  # <-- len will simply count the number of messages rather than tokens\n        max_tokens=5,  # <-- allow up to 5 messages.\n        strategy=\"last\",\n        # Most chat models expect that chat history starts with either:\n        # (1) a HumanMessage or",
        "detail": "python_use.agent.langchain_use.langgraph_use",
        "documentation": {}
    },
    {
        "label": "memory",
        "kind": 5,
        "importPath": "python_use.agent.langchain_use.langgraph_use",
        "description": "python_use.agent.langchain_use.langgraph_use",
        "peekOfCode": "memory = MemorySaver()\nmodel = ChatOllama(model=\"Qwen2.5-7B-Instruct\")\n# \ndef prompt(state) -> list[BaseMessage]:\n    \"\"\"Given the agent state, return a list of messages for the chat model.\"\"\"\n    # We're using the message processor defined above.\n    return trim_messages(\n        state[\"messages\"],\n        token_counter=len,  # <-- len will simply count the number of messages rather than tokens\n        max_tokens=5,  # <-- allow up to 5 messages.",
        "detail": "python_use.agent.langchain_use.langgraph_use",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "python_use.agent.langchain_use.langgraph_use",
        "description": "python_use.agent.langchain_use.langgraph_use",
        "peekOfCode": "model = ChatOllama(model=\"Qwen2.5-7B-Instruct\")\n# \ndef prompt(state) -> list[BaseMessage]:\n    \"\"\"Given the agent state, return a list of messages for the chat model.\"\"\"\n    # We're using the message processor defined above.\n    return trim_messages(\n        state[\"messages\"],\n        token_counter=len,  # <-- len will simply count the number of messages rather than tokens\n        max_tokens=5,  # <-- allow up to 5 messages.\n        strategy=\"last\",",
        "detail": "python_use.agent.langchain_use.langgraph_use",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "python_use.agent.langchain_use.langgraph_use",
        "description": "python_use.agent.langchain_use.langgraph_use",
        "peekOfCode": "app = create_react_agent(\n    model,\n    tools=[get_user_age],\n    checkpointer=memory,\n    prompt=prompt,\n)\nif __name__ == \"__main__\":\n    # The thread id is a unique key that identifies\n    # this particular conversation.\n    # We'll just generate a random uuid here.",
        "detail": "python_use.agent.langchain_use.langgraph_use",
        "documentation": {}
    },
    {
        "label": "call_model",
        "kind": 2,
        "importPath": "python_use.agent.memory.langchain_memory",
        "description": "python_use.agent.memory.langchain_memory",
        "peekOfCode": "def call_model(state: MessagesState):\n    response = model.invoke(state[\"messages\"])\n    # We return a list, because this will get added to the existing list\n    return {\"messages\": response}\n# Define the two nodes we will cycle between\nworkflow.add_edge(START, \"model\")\nworkflow.add_node(\"model\", call_model)\n# Adding memory is straight forward in langgraph!\nmemory = MemorySaver()\napp = workflow.compile(",
        "detail": "python_use.agent.memory.langchain_memory",
        "documentation": {}
    },
    {
        "label": "workflow",
        "kind": 5,
        "importPath": "python_use.agent.memory.langchain_memory",
        "description": "python_use.agent.memory.langchain_memory",
        "peekOfCode": "workflow = StateGraph(state_schema=MessagesState)\n# Define a chat model\n### \nos.environ[\"OPENAI_API_KEY\"] =  'sk-d58877221cec4f208dc353259ca9c8bc' # Qwen\nos.environ[\"OPENAI_BASE_URL\"] = 'https://dashscope.aliyuncs.com/compatible-mode/v1'   # https://dashscope.aliyuncs.com/compatible-mode/v1\n# \nmodel = ChatOpenAI(\n    # model=os.getenv('DEEPSEEK_MODEL'),\n    # api_key=os.getenv('DEEPSEEK_API_KEY'),\n    # base_url=os.getenv('DEEPSEEK_URL')",
        "detail": "python_use.agent.memory.langchain_memory",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "python_use.agent.memory.langchain_memory",
        "description": "python_use.agent.memory.langchain_memory",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] =  'sk-d58877221cec4f208dc353259ca9c8bc' # Qwen\nos.environ[\"OPENAI_BASE_URL\"] = 'https://dashscope.aliyuncs.com/compatible-mode/v1'   # https://dashscope.aliyuncs.com/compatible-mode/v1\n# \nmodel = ChatOpenAI(\n    # model=os.getenv('DEEPSEEK_MODEL'),\n    # api_key=os.getenv('DEEPSEEK_API_KEY'),\n    # base_url=os.getenv('DEEPSEEK_URL')\n    model = \"qwen-plus\",\n    api_key =  os.getenv('OPENAI_API_KEY'),\n    base_url = os.getenv('OPENAI_BASE_URL')",
        "detail": "python_use.agent.memory.langchain_memory",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_BASE_URL\"]",
        "kind": 5,
        "importPath": "python_use.agent.memory.langchain_memory",
        "description": "python_use.agent.memory.langchain_memory",
        "peekOfCode": "os.environ[\"OPENAI_BASE_URL\"] = 'https://dashscope.aliyuncs.com/compatible-mode/v1'   # https://dashscope.aliyuncs.com/compatible-mode/v1\n# \nmodel = ChatOpenAI(\n    # model=os.getenv('DEEPSEEK_MODEL'),\n    # api_key=os.getenv('DEEPSEEK_API_KEY'),\n    # base_url=os.getenv('DEEPSEEK_URL')\n    model = \"qwen-plus\",\n    api_key =  os.getenv('OPENAI_API_KEY'),\n    base_url = os.getenv('OPENAI_BASE_URL')\n)",
        "detail": "python_use.agent.memory.langchain_memory",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "python_use.agent.memory.langchain_memory",
        "description": "python_use.agent.memory.langchain_memory",
        "peekOfCode": "model = ChatOpenAI(\n    # model=os.getenv('DEEPSEEK_MODEL'),\n    # api_key=os.getenv('DEEPSEEK_API_KEY'),\n    # base_url=os.getenv('DEEPSEEK_URL')\n    model = \"qwen-plus\",\n    api_key =  os.getenv('OPENAI_API_KEY'),\n    base_url = os.getenv('OPENAI_BASE_URL')\n)\n# Define the function that calls the model\ndef call_model(state: MessagesState):",
        "detail": "python_use.agent.memory.langchain_memory",
        "documentation": {}
    },
    {
        "label": "memory",
        "kind": 5,
        "importPath": "python_use.agent.memory.langchain_memory",
        "description": "python_use.agent.memory.langchain_memory",
        "peekOfCode": "memory = MemorySaver()\napp = workflow.compile(\n    checkpointer=memory\n)\n# The thread id is a unique key that identifies\n# this particular conversation.\n# We'll just generate a random uuid here.\n# This enables a single application to manage conversations among multiple users.\nthread_id = uuid.uuid4()\nconfig = {\"configurable\": {\"thread_id\": thread_id}}",
        "detail": "python_use.agent.memory.langchain_memory",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "python_use.agent.memory.langchain_memory",
        "description": "python_use.agent.memory.langchain_memory",
        "peekOfCode": "app = workflow.compile(\n    checkpointer=memory\n)\n# The thread id is a unique key that identifies\n# this particular conversation.\n# We'll just generate a random uuid here.\n# This enables a single application to manage conversations among multiple users.\nthread_id = uuid.uuid4()\nconfig = {\"configurable\": {\"thread_id\": thread_id}}\ninput_message = HumanMessage(content=\"hi! I'm bob\")",
        "detail": "python_use.agent.memory.langchain_memory",
        "documentation": {}
    },
    {
        "label": "thread_id",
        "kind": 5,
        "importPath": "python_use.agent.memory.langchain_memory",
        "description": "python_use.agent.memory.langchain_memory",
        "peekOfCode": "thread_id = uuid.uuid4()\nconfig = {\"configurable\": {\"thread_id\": thread_id}}\ninput_message = HumanMessage(content=\"hi! I'm bob\")\nfor event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n    event[\"messages\"][-1].pretty_print()\n# Here, let's confirm that the AI remembers our name!\ninput_message = HumanMessage(content=\"what was my name?\")\nfor event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n    event[\"messages\"][-1].pretty_print()\nsnapstate = app.get_state(config)",
        "detail": "python_use.agent.memory.langchain_memory",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "python_use.agent.memory.langchain_memory",
        "description": "python_use.agent.memory.langchain_memory",
        "peekOfCode": "config = {\"configurable\": {\"thread_id\": thread_id}}\ninput_message = HumanMessage(content=\"hi! I'm bob\")\nfor event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n    event[\"messages\"][-1].pretty_print()\n# Here, let's confirm that the AI remembers our name!\ninput_message = HumanMessage(content=\"what was my name?\")\nfor event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n    event[\"messages\"][-1].pretty_print()\nsnapstate = app.get_state(config)\nprint(snapstate.values)",
        "detail": "python_use.agent.memory.langchain_memory",
        "documentation": {}
    },
    {
        "label": "input_message",
        "kind": 5,
        "importPath": "python_use.agent.memory.langchain_memory",
        "description": "python_use.agent.memory.langchain_memory",
        "peekOfCode": "input_message = HumanMessage(content=\"hi! I'm bob\")\nfor event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n    event[\"messages\"][-1].pretty_print()\n# Here, let's confirm that the AI remembers our name!\ninput_message = HumanMessage(content=\"what was my name?\")\nfor event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n    event[\"messages\"][-1].pretty_print()\nsnapstate = app.get_state(config)\nprint(snapstate.values)",
        "detail": "python_use.agent.memory.langchain_memory",
        "documentation": {}
    },
    {
        "label": "input_message",
        "kind": 5,
        "importPath": "python_use.agent.memory.langchain_memory",
        "description": "python_use.agent.memory.langchain_memory",
        "peekOfCode": "input_message = HumanMessage(content=\"what was my name?\")\nfor event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n    event[\"messages\"][-1].pretty_print()\nsnapstate = app.get_state(config)\nprint(snapstate.values)",
        "detail": "python_use.agent.memory.langchain_memory",
        "documentation": {}
    },
    {
        "label": "snapstate",
        "kind": 5,
        "importPath": "python_use.agent.memory.langchain_memory",
        "description": "python_use.agent.memory.langchain_memory",
        "peekOfCode": "snapstate = app.get_state(config)\nprint(snapstate.values)",
        "detail": "python_use.agent.memory.langchain_memory",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "python_use.agent.memory._mem0",
        "description": "python_use.agent.memory._mem0",
        "peekOfCode": "llm = ChatOllama(model=\"qwen2.5:7b\")\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"qdrant\",\n        \"config\": {\n            \"collection_name\": \"test\",\n            \"host\": \"localhost\",\n            \"port\": 6333,\n            \"embedding_model_dims\": 1024  # \n        },",
        "detail": "python_use.agent.memory._mem0",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "python_use.agent.memory._mem0",
        "description": "python_use.agent.memory._mem0",
        "peekOfCode": "config = {\n    \"vector_store\": {\n        \"provider\": \"qdrant\",\n        \"config\": {\n            \"collection_name\": \"test\",\n            \"host\": \"localhost\",\n            \"port\": 6333,\n            \"embedding_model_dims\": 1024  # \n        },\n    },",
        "detail": "python_use.agent.memory._mem0",
        "documentation": {}
    },
    {
        "label": "m",
        "kind": 5,
        "importPath": "python_use.agent.memory._mem0",
        "description": "python_use.agent.memory._mem0",
        "peekOfCode": "m = Memory.from_config(config)\nprint(\"\")\nmessages = [\n    {\"role\": \"user\", \"content\": \"Could you tell me what 1 plus 1111 equals?\"},\n    {\"role\": \"assistant\", \"content\": \"1112\"},\n    {\"role\": \"user\", \"content\": \"What is that answer plus 1?\"},\n    {\"role\": \"assistant\", \"content\": \"1113\"}\n]\nprompt = PromptTemplate(\n    input_variables=[\"history\", \"question\"],",
        "detail": "python_use.agent.memory._mem0",
        "documentation": {}
    },
    {
        "label": "messages",
        "kind": 5,
        "importPath": "python_use.agent.memory._mem0",
        "description": "python_use.agent.memory._mem0",
        "peekOfCode": "messages = [\n    {\"role\": \"user\", \"content\": \"Could you tell me what 1 plus 1111 equals?\"},\n    {\"role\": \"assistant\", \"content\": \"1112\"},\n    {\"role\": \"user\", \"content\": \"What is that answer plus 1?\"},\n    {\"role\": \"assistant\", \"content\": \"1113\"}\n]\nprompt = PromptTemplate(\n    input_variables=[\"history\", \"question\"],\n    template = \"\"\"\n:{history}",
        "detail": "python_use.agent.memory._mem0",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "python_use.agent.memory._mem0",
        "description": "python_use.agent.memory._mem0",
        "peekOfCode": "prompt = PromptTemplate(\n    input_variables=[\"history\", \"question\"],\n    template = \"\"\"\n:{history}\n:{question}\"\"\"\n)\nquestion = \"\"\nmyPrompt = prompt.format_prompt(\n    history=messages,\n    question=question",
        "detail": "python_use.agent.memory._mem0",
        "documentation": {}
    },
    {
        "label": "question",
        "kind": 5,
        "importPath": "python_use.agent.memory._mem0",
        "description": "python_use.agent.memory._mem0",
        "peekOfCode": "question = \"\"\nmyPrompt = prompt.format_prompt(\n    history=messages,\n    question=question\n)\nres = llm.invoke(myPrompt)\nprint(res.content)\nmessages.append({\"role\": \"user\", \"content\": question })\nmessages.append({\"role\": \"assistant\", \"content\": res.content})\n#  user_id ",
        "detail": "python_use.agent.memory._mem0",
        "documentation": {}
    },
    {
        "label": "myPrompt",
        "kind": 5,
        "importPath": "python_use.agent.memory._mem0",
        "description": "python_use.agent.memory._mem0",
        "peekOfCode": "myPrompt = prompt.format_prompt(\n    history=messages,\n    question=question\n)\nres = llm.invoke(myPrompt)\nprint(res.content)\nmessages.append({\"role\": \"user\", \"content\": question })\nmessages.append({\"role\": \"assistant\", \"content\": res.content})\n#  user_id \nm.add(messages, user_id=\"damn\", metadata={\"type\": \"math_qa\"})",
        "detail": "python_use.agent.memory._mem0",
        "documentation": {}
    },
    {
        "label": "res",
        "kind": 5,
        "importPath": "python_use.agent.memory._mem0",
        "description": "python_use.agent.memory._mem0",
        "peekOfCode": "res = llm.invoke(myPrompt)\nprint(res.content)\nmessages.append({\"role\": \"user\", \"content\": question })\nmessages.append({\"role\": \"assistant\", \"content\": res.content})\n#  user_id \nm.add(messages, user_id=\"damn\", metadata={\"type\": \"math_qa\"})\nrelated_memories = m.search(query=\"waht's the last questions\", user_id=\"damn\", filters={\"type\": \"math_qa\"})\nprint(related_memories )",
        "detail": "python_use.agent.memory._mem0",
        "documentation": {}
    },
    {
        "label": "related_memories",
        "kind": 5,
        "importPath": "python_use.agent.memory._mem0",
        "description": "python_use.agent.memory._mem0",
        "peekOfCode": "related_memories = m.search(query=\"waht's the last questions\", user_id=\"damn\", filters={\"type\": \"math_qa\"})\nprint(related_memories )",
        "detail": "python_use.agent.memory._mem0",
        "documentation": {}
    },
    {
        "label": "loader",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_map_reduce",
        "description": "python_use.agent.split.summary_map_reduce",
        "peekOfCode": "loader = TextLoader(\"./chineseJH.txt\", encoding=\"utf-8\")\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\ndocs = loader.load_and_split(text_splitter)\n# \nllm = ChatOpenAI(model=\"qwen:7b\", base_url=\"http://127.0.0.1:10087/v1\", api_key=\"vllm\")\n# \nchain = load_summarize_chain(\n    llm,\n    chain_type=\"refine\",\n    verbose=True,",
        "detail": "python_use.agent.split.summary_map_reduce",
        "documentation": {}
    },
    {
        "label": "text_splitter",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_map_reduce",
        "description": "python_use.agent.split.summary_map_reduce",
        "peekOfCode": "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\ndocs = loader.load_and_split(text_splitter)\n# \nllm = ChatOpenAI(model=\"qwen:7b\", base_url=\"http://127.0.0.1:10087/v1\", api_key=\"vllm\")\n# \nchain = load_summarize_chain(\n    llm,\n    chain_type=\"refine\",\n    verbose=True,\n    map_prompt=PromptTemplate.from_template(\"\\n{text}\"),  # Map",
        "detail": "python_use.agent.split.summary_map_reduce",
        "documentation": {}
    },
    {
        "label": "docs",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_map_reduce",
        "description": "python_use.agent.split.summary_map_reduce",
        "peekOfCode": "docs = loader.load_and_split(text_splitter)\n# \nllm = ChatOpenAI(model=\"qwen:7b\", base_url=\"http://127.0.0.1:10087/v1\", api_key=\"vllm\")\n# \nchain = load_summarize_chain(\n    llm,\n    chain_type=\"refine\",\n    verbose=True,\n    map_prompt=PromptTemplate.from_template(\"\\n{text}\"),  # Map\n    combine_prompt=PromptTemplate.from_template(\"\\n{text}\") # Reduce",
        "detail": "python_use.agent.split.summary_map_reduce",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_map_reduce",
        "description": "python_use.agent.split.summary_map_reduce",
        "peekOfCode": "llm = ChatOpenAI(model=\"qwen:7b\", base_url=\"http://127.0.0.1:10087/v1\", api_key=\"vllm\")\n# \nchain = load_summarize_chain(\n    llm,\n    chain_type=\"refine\",\n    verbose=True,\n    map_prompt=PromptTemplate.from_template(\"\\n{text}\"),  # Map\n    combine_prompt=PromptTemplate.from_template(\"\\n{text}\") # Reduce\n)\n# 5",
        "detail": "python_use.agent.split.summary_map_reduce",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_map_reduce",
        "description": "python_use.agent.split.summary_map_reduce",
        "peekOfCode": "chain = load_summarize_chain(\n    llm,\n    chain_type=\"refine\",\n    verbose=True,\n    map_prompt=PromptTemplate.from_template(\"\\n{text}\"),  # Map\n    combine_prompt=PromptTemplate.from_template(\"\\n{text}\") # Reduce\n)\n# 5\nresult = chain.invoke(docs[:2])\nprint(result)",
        "detail": "python_use.agent.split.summary_map_reduce",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_map_reduce",
        "description": "python_use.agent.split.summary_map_reduce",
        "peekOfCode": "result = chain.invoke(docs[:2])\nprint(result)\nprint(\"===\"*50)\nprint(docs[:2])",
        "detail": "python_use.agent.split.summary_map_reduce",
        "documentation": {}
    },
    {
        "label": "loader",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_refine",
        "description": "python_use.agent.split.summary_refine",
        "peekOfCode": "loader = TextLoader(\"./chineseJH.txt\", encoding=\"utf-8\")\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\ndocs = loader.load_and_split(text_splitter)\n# \nllm = ChatOpenAI(model=\"qwen:7b\", base_url=\"http://127.0.0.1:10087/v1\", api_key=\"vllm\")\n# \nquestion_prompt = PromptTemplate(\n    template=\"{text}\",\n    input_variables=[\"text\"]\n)",
        "detail": "python_use.agent.split.summary_refine",
        "documentation": {}
    },
    {
        "label": "text_splitter",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_refine",
        "description": "python_use.agent.split.summary_refine",
        "peekOfCode": "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\ndocs = loader.load_and_split(text_splitter)\n# \nllm = ChatOpenAI(model=\"qwen:7b\", base_url=\"http://127.0.0.1:10087/v1\", api_key=\"vllm\")\n# \nquestion_prompt = PromptTemplate(\n    template=\"{text}\",\n    input_variables=[\"text\"]\n)\nrefine_prompt = PromptTemplate(",
        "detail": "python_use.agent.split.summary_refine",
        "documentation": {}
    },
    {
        "label": "docs",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_refine",
        "description": "python_use.agent.split.summary_refine",
        "peekOfCode": "docs = loader.load_and_split(text_splitter)\n# \nllm = ChatOpenAI(model=\"qwen:7b\", base_url=\"http://127.0.0.1:10087/v1\", api_key=\"vllm\")\n# \nquestion_prompt = PromptTemplate(\n    template=\"{text}\",\n    input_variables=[\"text\"]\n)\nrefine_prompt = PromptTemplate(\n    template=\"{existing_answer}\\n{text}\\n\",",
        "detail": "python_use.agent.split.summary_refine",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_refine",
        "description": "python_use.agent.split.summary_refine",
        "peekOfCode": "llm = ChatOpenAI(model=\"qwen:7b\", base_url=\"http://127.0.0.1:10087/v1\", api_key=\"vllm\")\n# \nquestion_prompt = PromptTemplate(\n    template=\"{text}\",\n    input_variables=[\"text\"]\n)\nrefine_prompt = PromptTemplate(\n    template=\"{existing_answer}\\n{text}\\n\",\n    input_variables=[\"existing_answer\", \"text\"]\n)",
        "detail": "python_use.agent.split.summary_refine",
        "documentation": {}
    },
    {
        "label": "question_prompt",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_refine",
        "description": "python_use.agent.split.summary_refine",
        "peekOfCode": "question_prompt = PromptTemplate(\n    template=\"{text}\",\n    input_variables=[\"text\"]\n)\nrefine_prompt = PromptTemplate(\n    template=\"{existing_answer}\\n{text}\\n\",\n    input_variables=[\"existing_answer\", \"text\"]\n)\n# \nrefine_chain = load_summarize_chain(",
        "detail": "python_use.agent.split.summary_refine",
        "documentation": {}
    },
    {
        "label": "refine_prompt",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_refine",
        "description": "python_use.agent.split.summary_refine",
        "peekOfCode": "refine_prompt = PromptTemplate(\n    template=\"{existing_answer}\\n{text}\\n\",\n    input_variables=[\"existing_answer\", \"text\"]\n)\n# \nrefine_chain = load_summarize_chain(\n    llm,\n    chain_type=\"refine\",\n    verbose=True,\n    question_prompt=question_prompt,",
        "detail": "python_use.agent.split.summary_refine",
        "documentation": {}
    },
    {
        "label": "refine_chain",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_refine",
        "description": "python_use.agent.split.summary_refine",
        "peekOfCode": "refine_chain = load_summarize_chain(\n    llm,\n    chain_type=\"refine\",\n    verbose=True,\n    question_prompt=question_prompt,\n    refine_prompt=refine_prompt\n)\n# \ninputs = {\n    \"input_documents\": docs[:3],",
        "detail": "python_use.agent.split.summary_refine",
        "documentation": {}
    },
    {
        "label": "inputs",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_refine",
        "description": "python_use.agent.split.summary_refine",
        "peekOfCode": "inputs = {\n    \"input_documents\": docs[:3],\n    \"question\": \"\"  # \n}\nrefined_summary = refine_chain.invoke(inputs)\nprint(refined_summary)",
        "detail": "python_use.agent.split.summary_refine",
        "documentation": {}
    },
    {
        "label": "refined_summary",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_refine",
        "description": "python_use.agent.split.summary_refine",
        "peekOfCode": "refined_summary = refine_chain.invoke(inputs)\nprint(refined_summary)",
        "detail": "python_use.agent.split.summary_refine",
        "documentation": {}
    },
    {
        "label": "loader",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_stuff",
        "description": "python_use.agent.split.summary_stuff",
        "peekOfCode": "loader = TextLoader(\"./chineseJH.txt\", encoding=\"utf-8\")\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\ndocs = loader.load_and_split(text_splitter)\n# \nllm = ChatOpenAI(model=\"qwen:7b\", base_url=\"http://127.0.0.1:10087/v1\", api_key=\"vllm\")\n# \nchain = load_summarize_chain(\n    llm,\n    verbose=True,\n)",
        "detail": "python_use.agent.split.summary_stuff",
        "documentation": {}
    },
    {
        "label": "text_splitter",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_stuff",
        "description": "python_use.agent.split.summary_stuff",
        "peekOfCode": "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\ndocs = loader.load_and_split(text_splitter)\n# \nllm = ChatOpenAI(model=\"qwen:7b\", base_url=\"http://127.0.0.1:10087/v1\", api_key=\"vllm\")\n# \nchain = load_summarize_chain(\n    llm,\n    verbose=True,\n)\n# 5",
        "detail": "python_use.agent.split.summary_stuff",
        "documentation": {}
    },
    {
        "label": "docs",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_stuff",
        "description": "python_use.agent.split.summary_stuff",
        "peekOfCode": "docs = loader.load_and_split(text_splitter)\n# \nllm = ChatOpenAI(model=\"qwen:7b\", base_url=\"http://127.0.0.1:10087/v1\", api_key=\"vllm\")\n# \nchain = load_summarize_chain(\n    llm,\n    verbose=True,\n)\n# 5\nresult = chain.invoke(docs[:2])",
        "detail": "python_use.agent.split.summary_stuff",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_stuff",
        "description": "python_use.agent.split.summary_stuff",
        "peekOfCode": "llm = ChatOpenAI(model=\"qwen:7b\", base_url=\"http://127.0.0.1:10087/v1\", api_key=\"vllm\")\n# \nchain = load_summarize_chain(\n    llm,\n    verbose=True,\n)\n# 5\nresult = chain.invoke(docs[:2])\nprint(result)\nprint(\"===\"*50)",
        "detail": "python_use.agent.split.summary_stuff",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_stuff",
        "description": "python_use.agent.split.summary_stuff",
        "peekOfCode": "chain = load_summarize_chain(\n    llm,\n    verbose=True,\n)\n# 5\nresult = chain.invoke(docs[:2])\nprint(result)\nprint(\"===\"*50)\nprint(docs[:2])",
        "detail": "python_use.agent.split.summary_stuff",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_stuff",
        "description": "python_use.agent.split.summary_stuff",
        "peekOfCode": "result = chain.invoke(docs[:2])\nprint(result)\nprint(\"===\"*50)\nprint(docs[:2])",
        "detail": "python_use.agent.split.summary_stuff",
        "documentation": {}
    },
    {
        "label": "python_execute_tool",
        "kind": 2,
        "importPath": "python_use.agent.tools.code_run",
        "description": "python_use.agent.tools.code_run",
        "peekOfCode": "def python_execute_tool(code: str):\n    \"\"\"Python\"\"\"\n    repl = PythonREPL()\n    try:\n        # \n        output = repl.run(code)\n        return {\n            \"status\": \"success\",\n            \"output\": output,\n            \"error\": None",
        "detail": "python_use.agent.tools.code_run",
        "documentation": {}
    },
    {
        "label": "getBg",
        "kind": 2,
        "importPath": "python_use.import_use.src.chatbot.bg.bg",
        "description": "python_use.import_use.src.chatbot.bg.bg",
        "peekOfCode": "def getBg():\n    print(\"bg\")",
        "detail": "python_use.import_use.src.chatbot.bg.bg",
        "documentation": {}
    },
    {
        "label": "getModel",
        "kind": 2,
        "importPath": "python_use.import_use.src.chatbot.core.load_model.get_model",
        "description": "python_use.import_use.src.chatbot.core.load_model.get_model",
        "peekOfCode": "def getModel():\n    print(\"getmodel\")\n    return \"Model\"",
        "detail": "python_use.import_use.src.chatbot.core.load_model.get_model",
        "documentation": {}
    },
    {
        "label": "Neo4j_rag",
        "kind": 6,
        "importPath": "python_use.import_use.src.chatbot.core.rag.neo4j_rag",
        "description": "python_use.import_use.src.chatbot.core.rag.neo4j_rag",
        "peekOfCode": "class Neo4j_rag:\n    def __init__(self):\n        getModel()\n        getBg()\n        fg()\n        pass\n    def process(self):\n        return True\nif __name__ == \"__main__\":\n    neo4j_processor = Neo4j_rag()",
        "detail": "python_use.import_use.src.chatbot.core.rag.neo4j_rag",
        "documentation": {}
    },
    {
        "label": "Rag",
        "kind": 6,
        "importPath": "python_use.import_use.src.chatbot.core.rag.rag",
        "description": "python_use.import_use.src.chatbot.core.rag.rag",
        "peekOfCode": "class Rag:\n    def __init__(self):\n        pass\n    def getItem(self):\n        return True",
        "detail": "python_use.import_use.src.chatbot.core.rag.rag",
        "documentation": {}
    },
    {
        "label": "fg",
        "kind": 2,
        "importPath": "python_use.import_use.src.chatbot.fg.fg",
        "description": "python_use.import_use.src.chatbot.fg.fg",
        "peekOfCode": "def fg():\n    print(\"fg\")",
        "detail": "python_use.import_use.src.chatbot.fg.fg",
        "documentation": {}
    },
    {
        "label": "MILITORY_PROMPT",
        "kind": 5,
        "importPath": "python_use.import_use.src.chatbot.utils.prompts.militory",
        "description": "python_use.import_use.src.chatbot.utils.prompts.militory",
        "peekOfCode": "MILITORY_PROMPT = \"militory_prompt\"\nprint(f\"load_model2 {getModel()}\")",
        "detail": "python_use.import_use.src.chatbot.utils.prompts.militory",
        "documentation": {}
    },
    {
        "label": "cal",
        "kind": 2,
        "importPath": "python_use.import_use.src.chatbot.utils.utils",
        "description": "python_use.import_use.src.chatbot.utils.utils",
        "peekOfCode": "def cal():\n    return 1 * 3",
        "detail": "python_use.import_use.src.chatbot.utils.utils",
        "documentation": {}
    },
    {
        "label": "Ui_loadwin",
        "kind": 6,
        "importPath": "python_use.pyqt_use.loading_screen.loadwin_ui",
        "description": "python_use.pyqt_use.loading_screen.loadwin_ui",
        "peekOfCode": "class Ui_loadwin(object):\n    def setupUi(self, loadwin):\n        loadwin.setObjectName(\"loadwin\")\n        # loadwin.resize(455, 349)\n        self.gridLayout = QtWidgets.QGridLayout(loadwin)\n        self.gridLayout.setObjectName(\"gridLayout\")\n        # # \n        # self.gridLayout.setRowStretch(0, 1)  # \n        # self.gridLayout.setRowStretch(1, 0)  # \n        # self.gridLayout.setRowStretch(2, 0)  # ",
        "detail": "python_use.pyqt_use.loading_screen.loadwin_ui",
        "documentation": {}
    },
    {
        "label": "LoadWin",
        "kind": 6,
        "importPath": "python_use.pyqt_use.loading_screen.main",
        "description": "python_use.pyqt_use.loading_screen.main",
        "peekOfCode": "class LoadWin(QWidget, Ui_loadwin):  # \n    def __init__(self):\n        super(LoadWin, self).__init__()\n        self.setupUi(self)\n        self.setWindowTitle(\"\")\n        screen = QDesktopWidget().screenGeometry()\n        # 60%40%\n        self.resize(int(screen.width() * 0.5), int(screen.height() * 0.6))\n        # \n        self.setMinimumSize(400, 300)  # ",
        "detail": "python_use.pyqt_use.loading_screen.main",
        "documentation": {}
    },
    {
        "label": "LoadThread",
        "kind": 6,
        "importPath": "python_use.pyqt_use.loading_screen.main",
        "description": "python_use.pyqt_use.loading_screen.main",
        "peekOfCode": "class LoadThread(QThread):  #  -----------\n    part_signal = pyqtSignal(int)  # \n    data_signal = pyqtSignal(str)  # \n    show_signal = pyqtSignal()\n    def __init__(self):\n        super().__init__()\n        self.ret = None\n    # \n    def run(self):\n        # process_set_part",
        "detail": "python_use.pyqt_use.loading_screen.main",
        "documentation": {}
    },
    {
        "label": "MainWin",
        "kind": 6,
        "importPath": "python_use.pyqt_use.loading_screen.main",
        "description": "python_use.pyqt_use.loading_screen.main",
        "peekOfCode": "class MainWin(QWidget, Ui_mainwin):  #  -----------\n    def __init__(self):\n        super(MainWin, self).__init__()\n        self.setupUi(self)\n        self.setWindowTitle(\"\")\n    def set_data(self, mes=\"xxxxx\"):\n        self.lineEdit.setText(mes)\n# \ndef show_main_win():\n    w.close()",
        "detail": "python_use.pyqt_use.loading_screen.main",
        "documentation": {}
    },
    {
        "label": "show_main_win",
        "kind": 2,
        "importPath": "python_use.pyqt_use.loading_screen.main",
        "description": "python_use.pyqt_use.loading_screen.main",
        "peekOfCode": "def show_main_win():\n    w.close()\n    zhu.set_data(\"xxxxx\")\n    zhu.show()\nif __name__ == \"__main__\":\n    app = QApplication(sys.argv)\n    w = LoadWin()\n    zhu = MainWin()\n    w.show()\n    sys.exit(app.exec())",
        "detail": "python_use.pyqt_use.loading_screen.main",
        "documentation": {}
    },
    {
        "label": "BASE_DIR",
        "kind": 5,
        "importPath": "python_use.pyqt_use.loading_screen.main",
        "description": "python_use.pyqt_use.loading_screen.main",
        "peekOfCode": "BASE_DIR = os.getenv(\"BASE_DIR\")\nclass LoadWin(QWidget, Ui_loadwin):  # \n    def __init__(self):\n        super(LoadWin, self).__init__()\n        self.setupUi(self)\n        self.setWindowTitle(\"\")\n        screen = QDesktopWidget().screenGeometry()\n        # 60%40%\n        self.resize(int(screen.width() * 0.5), int(screen.height() * 0.6))\n        # ",
        "detail": "python_use.pyqt_use.loading_screen.main",
        "documentation": {}
    },
    {
        "label": "Ui_mainwin",
        "kind": 6,
        "importPath": "python_use.pyqt_use.loading_screen.Ui_mainwin",
        "description": "python_use.pyqt_use.loading_screen.Ui_mainwin",
        "peekOfCode": "class Ui_mainwin(object):\n    def setupUi(self, mainwin):\n        mainwin.setObjectName(\"mainwin\")\n        mainwin.resize(609, 432)\n        self.calendarWidget = QtWidgets.QCalendarWidget(mainwin)\n        self.calendarWidget.setGeometry(QtCore.QRect(170, 80, 248, 197))\n        self.calendarWidget.setObjectName(\"calendarWidget\")\n        self.lineEdit = QtWidgets.QLineEdit(mainwin)\n        self.lineEdit.setGeometry(QtCore.QRect(180, 320, 231, 21))\n        self.lineEdit.setObjectName(\"lineEdit\")",
        "detail": "python_use.pyqt_use.loading_screen.Ui_mainwin",
        "documentation": {}
    },
    {
        "label": "split_documents",
        "kind": 2,
        "importPath": "python_use.rag.text_split.base",
        "description": "python_use.rag.text_split.base",
        "peekOfCode": "def split_documents(raw_documents, chunk_size=256, chunk_overlap=24):\n    \"\"\"\"\"\"\n    if not raw_documents:\n        raise ValueError(\"\")\n    print(\"...\")\n    text_splitter = TokenTextSplitter(\n        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n    )\n    chunks = text_splitter.split_documents(raw_documents)\n    print(f\" {len(chunks)} \")",
        "detail": "python_use.rag.text_split.base",
        "documentation": {}
    },
    {
        "label": "build",
        "kind": 2,
        "importPath": "python_use.rag.text_split.build_chromadb",
        "description": "python_use.rag.text_split.build_chromadb",
        "peekOfCode": "def build(self, folder):\n    try:\n        new_docs = load_pdf_documents(folder)\n        vector_store = self.client.get_or_create_collection(\n            name=CHROMA_KEY, embedding_function=self.embedding\n        )\n        vector_store.add(\n            documents=[doc.page_content for doc in new_docs],\n            metadatas=[doc.metadata for doc in new_docs],\n            ids=[f\"doc_{i}\" for i in range(len(new_docs))],",
        "detail": "python_use.rag.text_split.build_chromadb",
        "documentation": {}
    },
    {
        "label": "CHROMA_KEY",
        "kind": 5,
        "importPath": "python_use.rag.text_split.build_chromadb",
        "description": "python_use.rag.text_split.build_chromadb",
        "peekOfCode": "CHROMA_KEY = \"langchain\"\ndef build(self, folder):\n    try:\n        new_docs = load_pdf_documents(folder)\n        vector_store = self.client.get_or_create_collection(\n            name=CHROMA_KEY, embedding_function=self.embedding\n        )\n        vector_store.add(\n            documents=[doc.page_content for doc in new_docs],\n            metadatas=[doc.metadata for doc in new_docs],",
        "detail": "python_use.rag.text_split.build_chromadb",
        "documentation": {}
    },
    {
        "label": "load_pdf_documents",
        "kind": 2,
        "importPath": "python_use.rag.text_split.load_file",
        "description": "python_use.rag.text_split.load_file",
        "peekOfCode": "def load_pdf_documents(pdf_folder):\n    \"\"\"PDF\"\"\"\n    raw_documents = []\n    if not os.path.isdir(pdf_folder):\n        raise FileNotFoundError(f\" PDF : {pdf_folder}\")\n    pdf_files = glob.glob(os.path.join(pdf_folder, \"*.pdf\"), recursive=True)\n    if not pdf_files:\n        print(f\" {pdf_folder}  PDF \")\n        return []\n    print(f\" {len(pdf_files)}  PDF ...\")",
        "detail": "python_use.rag.text_split.load_file",
        "documentation": {}
    },
    {
        "label": "text_splitter",
        "kind": 5,
        "importPath": "python_use.rag.text_split.split",
        "description": "python_use.rag.text_split.split",
        "peekOfCode": "text_splitter = RecursiveCharacterTextSplitter(\n    separators=[\n        \"\\n\\n\",        # \n        \"\", \"!\", \"?\",  # \n        \"\\n\",          # \n        \"\", \";\",      # \n        \" \", \"\",        # \n    ],\n    chunk_size=800,     # 400-500\n    chunk_overlap=150,  # 15%",
        "detail": "python_use.rag.text_split.split",
        "documentation": {}
    },
    {
        "label": "raw_docs",
        "kind": 5,
        "importPath": "python_use.rag.text_split.split",
        "description": "python_use.rag.text_split.split",
        "peekOfCode": "raw_docs = load_pdf_documents(\"./pdfs\")\nprint(\"\\n\" + \"*\" * 50 + \"  \" + \"*\" * 50)\nprint(f\"PDF{len(raw_docs)}\")\nprint(f\"{raw_docs[0].metadata}\\n\")\n# \nchunks = text_splitter.split_documents(raw_docs)\nprint(\"\\n\" + \"=\" * 50 + \"  \" + \"=\" * 50)\nprint(f\"{len(chunks)}\")\nprint(f\"{sum(len(c.page_content) for c in chunks)//len(chunks)}\")\nprint(f\"{max(len(c.page_content) for c in chunks)}\")",
        "detail": "python_use.rag.text_split.split",
        "documentation": {}
    },
    {
        "label": "chunks",
        "kind": 5,
        "importPath": "python_use.rag.text_split.split",
        "description": "python_use.rag.text_split.split",
        "peekOfCode": "chunks = text_splitter.split_documents(raw_docs)\nprint(\"\\n\" + \"=\" * 50 + \"  \" + \"=\" * 50)\nprint(f\"{len(chunks)}\")\nprint(f\"{sum(len(c.page_content) for c in chunks)//len(chunks)}\")\nprint(f\"{max(len(c.page_content) for c in chunks)}\")\nprint(f\"{min(len(c.page_content) for c in chunks)}\\n\")\n# \nprint(\"=\" * 50 + \"  \" + \"=\" * 50)\nfor i, chunk in enumerate(chunks[:3]):  # 3\n    metadata = chunk.metadata",
        "detail": "python_use.rag.text_split.split",
        "documentation": {}
    },
    {
        "label": "get_session_history",
        "kind": 2,
        "importPath": "python_use.role_bot.src.history",
        "description": "python_use.role_bot.src.history",
        "peekOfCode": "def get_session_history(session_id: str) -> ChatMessageHistory:\n    if session_id not in store:\n        store[session_id] = ChatMessageHistory()\n    return store[session_id]\n# 3. \nrunnable = RunnableWithMessageHistory(\n    chain,\n    get_session_history,\n    input_messages_key=\"input\",\n    history_messages_key=\"history\"",
        "detail": "python_use.role_bot.src.history",
        "documentation": {}
    },
    {
        "label": "respond",
        "kind": 2,
        "importPath": "python_use.role_bot.src.history",
        "description": "python_use.role_bot.src.history",
        "peekOfCode": "def respond(message: str, history: list):\n    session_id = \"default_session\"\n    response = \"\"\n    for chunk in runnable.stream(\n        {\"input\": message},\n        config={\"configurable\": {\"session_id\": session_id}}\n    ):\n        response += chunk.content\n        yield [{\"role\": \"user\", \"content\": message},\n               {\"role\": \"assistant\", \"content\": response}]",
        "detail": "python_use.role_bot.src.history",
        "documentation": {}
    },
    {
        "label": "get_history_blocks",
        "kind": 2,
        "importPath": "python_use.role_bot.src.history",
        "description": "python_use.role_bot.src.history",
        "peekOfCode": "def get_history_blocks(expand_states=None):\n    session_id = \"default_session\"\n    history_obj = get_session_history(session_id)\n    messages = history_obj.messages\n    if not messages:\n        return [gr.Markdown(\"\")]\n    blocks = []\n    if expand_states is None or len(expand_states) != len(messages):\n        expand_states = [False] * len(messages)\n    for i, msg in enumerate(messages):",
        "detail": "python_use.role_bot.src.history",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "python_use.role_bot.src.history",
        "description": "python_use.role_bot.src.history",
        "peekOfCode": "llm = ChatOllama(model=\"Qwen2.5-7B-Instruct:latest\")\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"AI\"),\n    MessagesPlaceholder(variable_name=\"history\"),\n    (\"human\", \"{input}\")\n])\nchain = prompt | llm\n# 2. \nstore = {}\ndef get_session_history(session_id: str) -> ChatMessageHistory:",
        "detail": "python_use.role_bot.src.history",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "python_use.role_bot.src.history",
        "description": "python_use.role_bot.src.history",
        "peekOfCode": "prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"AI\"),\n    MessagesPlaceholder(variable_name=\"history\"),\n    (\"human\", \"{input}\")\n])\nchain = prompt | llm\n# 2. \nstore = {}\ndef get_session_history(session_id: str) -> ChatMessageHistory:\n    if session_id not in store:",
        "detail": "python_use.role_bot.src.history",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "python_use.role_bot.src.history",
        "description": "python_use.role_bot.src.history",
        "peekOfCode": "chain = prompt | llm\n# 2. \nstore = {}\ndef get_session_history(session_id: str) -> ChatMessageHistory:\n    if session_id not in store:\n        store[session_id] = ChatMessageHistory()\n    return store[session_id]\n# 3. \nrunnable = RunnableWithMessageHistory(\n    chain,",
        "detail": "python_use.role_bot.src.history",
        "documentation": {}
    },
    {
        "label": "store",
        "kind": 5,
        "importPath": "python_use.role_bot.src.history",
        "description": "python_use.role_bot.src.history",
        "peekOfCode": "store = {}\ndef get_session_history(session_id: str) -> ChatMessageHistory:\n    if session_id not in store:\n        store[session_id] = ChatMessageHistory()\n    return store[session_id]\n# 3. \nrunnable = RunnableWithMessageHistory(\n    chain,\n    get_session_history,\n    input_messages_key=\"input\",",
        "detail": "python_use.role_bot.src.history",
        "documentation": {}
    },
    {
        "label": "runnable",
        "kind": 5,
        "importPath": "python_use.role_bot.src.history",
        "description": "python_use.role_bot.src.history",
        "peekOfCode": "runnable = RunnableWithMessageHistory(\n    chain,\n    get_session_history,\n    input_messages_key=\"input\",\n    history_messages_key=\"history\"\n)\n# 4. \ndef respond(message: str, history: list):\n    session_id = \"default_session\"\n    response = \"\"",
        "detail": "python_use.role_bot.src.history",
        "documentation": {}
    },
    {
        "label": "get_session_history",
        "kind": 2,
        "importPath": "python_use.role_bot.src.main",
        "description": "python_use.role_bot.src.main",
        "peekOfCode": "def get_session_history(session_id: str) -> ChatMessageHistory:\n    if session_id not in store:\n        store[session_id] = ChatMessageHistory()\n    return store[session_id]\n# 3. \nrunnable = RunnableWithMessageHistory(\n    chain,\n    get_session_history,\n    input_messages_key=\"input\",\n    history_messages_key=\"history\"",
        "detail": "python_use.role_bot.src.main",
        "documentation": {}
    },
    {
        "label": "respond",
        "kind": 2,
        "importPath": "python_use.role_bot.src.main",
        "description": "python_use.role_bot.src.main",
        "peekOfCode": "def respond(message: str, history: list):\n    session_id = \"default_session\"\n    response = \"\"\n    for chunk in runnable.stream(\n        {\"input\": message},\n        config={\"configurable\": {\"session_id\": session_id}}\n    ):\n        response += chunk.content\n        yield [{\"role\": \"user\", \"content\": message},\n               {\"role\": \"assistant\", \"content\": response}]",
        "detail": "python_use.role_bot.src.main",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "python_use.role_bot.src.main",
        "description": "python_use.role_bot.src.main",
        "peekOfCode": "llm = ChatOllama(model=\"Qwen2.5-7B-Instruct:latest\")\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"AI\"),\n    MessagesPlaceholder(variable_name=\"history\"),\n    (\"human\", \"{input}\")\n])\nchain = prompt | llm\n# 2. \nstore = {}\ndef get_session_history(session_id: str) -> ChatMessageHistory:",
        "detail": "python_use.role_bot.src.main",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "python_use.role_bot.src.main",
        "description": "python_use.role_bot.src.main",
        "peekOfCode": "prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"AI\"),\n    MessagesPlaceholder(variable_name=\"history\"),\n    (\"human\", \"{input}\")\n])\nchain = prompt | llm\n# 2. \nstore = {}\ndef get_session_history(session_id: str) -> ChatMessageHistory:\n    if session_id not in store:",
        "detail": "python_use.role_bot.src.main",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "python_use.role_bot.src.main",
        "description": "python_use.role_bot.src.main",
        "peekOfCode": "chain = prompt | llm\n# 2. \nstore = {}\ndef get_session_history(session_id: str) -> ChatMessageHistory:\n    if session_id not in store:\n        store[session_id] = ChatMessageHistory()\n    return store[session_id]\n# 3. \nrunnable = RunnableWithMessageHistory(\n    chain,",
        "detail": "python_use.role_bot.src.main",
        "documentation": {}
    },
    {
        "label": "store",
        "kind": 5,
        "importPath": "python_use.role_bot.src.main",
        "description": "python_use.role_bot.src.main",
        "peekOfCode": "store = {}\ndef get_session_history(session_id: str) -> ChatMessageHistory:\n    if session_id not in store:\n        store[session_id] = ChatMessageHistory()\n    return store[session_id]\n# 3. \nrunnable = RunnableWithMessageHistory(\n    chain,\n    get_session_history,\n    input_messages_key=\"input\",",
        "detail": "python_use.role_bot.src.main",
        "documentation": {}
    },
    {
        "label": "runnable",
        "kind": 5,
        "importPath": "python_use.role_bot.src.main",
        "description": "python_use.role_bot.src.main",
        "peekOfCode": "runnable = RunnableWithMessageHistory(\n    chain,\n    get_session_history,\n    input_messages_key=\"input\",\n    history_messages_key=\"history\"\n)\n# 4. \ndef respond(message: str, history: list):\n    session_id = \"default_session\"\n    response = \"\"",
        "detail": "python_use.role_bot.src.main",
        "documentation": {}
    },
    {
        "label": "Agent",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyAgent.tinyAgent.Agent",
        "description": "tiny-universe-main.content.TinyAgent.tinyAgent.Agent",
        "peekOfCode": "class Agent:\n    def __init__(self, path: str = '') -> None:\n        self.path = path\n        self.tool = Tools()\n        self.system_prompt = self.build_system_input()\n        self.model = InternLM2Chat(path)\n    def build_system_input(self):\n        tool_descs, tool_names = [], []\n        for tool in self.tool.toolConfig:\n            tool_descs.append(TOOL_DESC.format(**tool))",
        "detail": "tiny-universe-main.content.TinyAgent.tinyAgent.Agent",
        "documentation": {}
    },
    {
        "label": "TOOL_DESC",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyAgent.tinyAgent.Agent",
        "description": "tiny-universe-main.content.TinyAgent.tinyAgent.Agent",
        "peekOfCode": "TOOL_DESC = \"\"\"{name_for_model}: Call this tool to interact with the {name_for_human} API. What is the {name_for_human} API useful for? {description_for_model} Parameters: {parameters} Format the arguments as a JSON object.\"\"\"\nREACT_PROMPT = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n{tool_descs}\nUse the following format:\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tool_names}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can be repeated zero or more times)",
        "detail": "tiny-universe-main.content.TinyAgent.tinyAgent.Agent",
        "documentation": {}
    },
    {
        "label": "REACT_PROMPT",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyAgent.tinyAgent.Agent",
        "description": "tiny-universe-main.content.TinyAgent.tinyAgent.Agent",
        "peekOfCode": "REACT_PROMPT = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n{tool_descs}\nUse the following format:\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tool_names}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can be repeated zero or more times)\nThought: I now know the final answer",
        "detail": "tiny-universe-main.content.TinyAgent.tinyAgent.Agent",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyAgent.tinyAgent.LLM",
        "description": "tiny-universe-main.content.TinyAgent.tinyAgent.LLM",
        "peekOfCode": "class BaseModel:\n    def __init__(self, path: str = '') -> None:\n        self.path = path\n    def chat(self, prompt: str, history: List[dict]):\n        pass\n    def load_model(self):\n        pass\nclass InternLM2Chat(BaseModel):\n    def __init__(self, path: str = '') -> None:\n        super().__init__(path)",
        "detail": "tiny-universe-main.content.TinyAgent.tinyAgent.LLM",
        "documentation": {}
    },
    {
        "label": "InternLM2Chat",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyAgent.tinyAgent.LLM",
        "description": "tiny-universe-main.content.TinyAgent.tinyAgent.LLM",
        "peekOfCode": "class InternLM2Chat(BaseModel):\n    def __init__(self, path: str = '') -> None:\n        super().__init__(path)\n        self.load_model()\n    def load_model(self):\n        print('================ Loading model ================')\n        self.tokenizer = AutoTokenizer.from_pretrained(self.path, trust_remote_code=True)\n        self.model = AutoModelForCausalLM.from_pretrained(self.path, torch_dtype=torch.float16, trust_remote_code=True).cuda().eval()\n        print('================ Model loaded ================')\n    def chat(self, prompt: str, history: List[dict], meta_instruction:str ='') -> str:",
        "detail": "tiny-universe-main.content.TinyAgent.tinyAgent.LLM",
        "documentation": {}
    },
    {
        "label": "Tools",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyAgent.tinyAgent.tool",
        "description": "tiny-universe-main.content.TinyAgent.tinyAgent.tool",
        "peekOfCode": "class Tools:\n    def __init__(self) -> None:\n        self.toolConfig = self._tools()\n    def _tools(self):\n        tools = [\n            {\n                'name_for_human': '',\n                'name_for_model': 'google_search',\n                'description_for_model': '',\n                'parameters': [",
        "detail": "tiny-universe-main.content.TinyAgent.tinyAgent.tool",
        "documentation": {}
    },
    {
        "label": "load_transformed_dataset",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.dataloader",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.dataloader",
        "peekOfCode": "def load_transformed_dataset(img_size=32, batch_size=128) -> DataLoader:\n    \"\"\"CIFAR10\"\"\"\n    train_data_transform = transforms.Compose([\n        transforms.Resize((img_size, img_size)),\n        transforms.RandomHorizontalFlip(),  # \n        transforms.ToTensor(),  # [0, 1]\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # [-1, 1]\n    ])\n    test_data_transform = transforms.Compose([\n        transforms.Resize((img_size, img_size)),",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.dataloader",
        "documentation": {}
    },
    {
        "label": "show_tensor_image",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.dataloader",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.dataloader",
        "peekOfCode": "def show_tensor_image(image):\n    reverse_transforms = transforms.Compose([\n        transforms.Lambda(lambda t: (t + 1) / 2),  # [-1, 1][0, 1]\n        transforms.Lambda(lambda t: t.permute(1, 2, 0)),  # CHWHWC\n        transforms.Lambda(lambda t: t * 255.),  # [0, 255]\n        transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),  # uint8\n        transforms.ToPILImage(),  # PIL\n    ])\n    # ,\n    if len(image.shape) == 4:",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.dataloader",
        "documentation": {}
    },
    {
        "label": "NoiseScheduler",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.diffusion",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.diffusion",
        "peekOfCode": "class NoiseScheduler(nn.Module):\n    def __init__(self, beta_start=0.0001, beta_end=0.02, num_steps=1000):\n        \"\"\"\n        Args:\n            beta_start: 1,\n            beta_end: T,  \n            num_steps: T,\n            device: \n        \"\"\"\n        super().__init__()",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.diffusion",
        "documentation": {}
    },
    {
        "label": "plot_diffusion_steps",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.diffusion",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.diffusion",
        "peekOfCode": "def plot_diffusion_steps(image, noise_scheduler, step_size=100):\n    \"\"\"\n    Args:\n        image: \n        noise_scheduler: \n        step_size: \n    Returns:\n        fig: \n    \"\"\"\n    num_images = noise_scheduler.num_steps // step_size",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.diffusion",
        "documentation": {}
    },
    {
        "label": "InceptionStatistics",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.metrics",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.metrics",
        "peekOfCode": "class InceptionStatistics:\n    def __init__(self, device='cuda'):\n        self.device = device\n        # Inception v3\n        self.model = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1, transform_input=False)\n        self.model.fc = nn.Identity()  # \n        self.model = self.model.to(device)\n        self.model.eval()\n        # \n        self.preprocess = transforms.Compose([",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.metrics",
        "documentation": {}
    },
    {
        "label": "calculate_inception_score",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.metrics",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.metrics",
        "peekOfCode": "def calculate_inception_score(probs, splits=10):\n    \"\"\"Inception Score\n    IS = exp(E[KL(p(y|x) || p(y))])\n    :\n    - p(y|x) Inception(probs)\n    - p(y) ,p(y|x)\n    - KLKL,\n    - E\n    :\n    1. splits",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.metrics",
        "documentation": {}
    },
    {
        "label": "calculate_fid",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.metrics",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.metrics",
        "peekOfCode": "def calculate_fid(real_features, fake_features):\n    \"\"\"Frchet Inception Distance (FID)\n    FID = ||_r - _f||^2 + Tr(_r + _f - 2(_r _f)^(1/2))\n    :\n    - _r, _f \n    - _r, _f \n    - Tr ()\n    - ||||^2 \n    FID,\n    \"\"\"",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.metrics",
        "documentation": {}
    },
    {
        "label": "evaluate_model",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.metrics",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.metrics",
        "peekOfCode": "def evaluate_model(model, scheduler, train_loader, num_samples, batch_size, image_size, device=\"cuda\"):\n    \"\"\"ISFID\"\"\"\n    # \n    fake_images = []\n    num_batches = num_samples // batch_size  # batch_size\n    print(f\"{num_samples}...\")\n    for _ in tqdm(range(num_batches)):\n        fake_batch = sample(model, scheduler, batch_size, (3, image_size, image_size), device)\n        fake_batch = ((fake_batch + 1) / 2)  # [0,1]\n        fake_images.append(fake_batch.cpu())",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.metrics",
        "documentation": {}
    },
    {
        "label": "sample",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.sample",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.sample",
        "peekOfCode": "def sample(model, scheduler, num_samples, size, device=\"cpu\"):\n    \"\"\"\n    Args:\n        model: UNet,\n        scheduler: ,\n        num_samples: \n        size: ,(3,32,32)\n        device: \n    Returns:\n        ",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.sample",
        "documentation": {}
    },
    {
        "label": "plot",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.sample",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.sample",
        "peekOfCode": "def plot(images):\n    fig = plt.figure(figsize=(12, 8))\n    plt.axis(\"off\")\n    plt.imshow(torchvision.utils.make_grid(images, nrow=5).permute(1, 2, 0))\n    plt.tight_layout(pad=1)\n    return fig\nif __name__ == \"__main__\":\n    image_size = 32\n    model = SimpleUnet()\n    model.load_state_dict(torch.load(f\"simple-unet-ddpm-{image_size}.pth\", weights_only=True))",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.sample",
        "documentation": {}
    },
    {
        "label": "test_step",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.train",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.train",
        "peekOfCode": "def test_step(model, dataloader, noise_scheduler, criterion, epoch, num_epochs, device):\n    \"\"\",\"\"\"\n    model.eval()\n    with torch.no_grad():\n        loss_sum = 0\n        num_batches = 0\n        pbar = tqdm(dataloader)\n        for batch in pbar:\n            images, _ = batch\n            images = images.to(device)",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.train",
        "documentation": {}
    },
    {
        "label": "train_step",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.train",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.train",
        "peekOfCode": "def train_step(model, dataloader, noise_scheduler, criterion, optimizer, epoch, num_epochs, device):\n    \"\"\",\"\"\"\n    # \n    model.train()\n    loss_sum = 0\n    num_batches = 0\n    pbar = tqdm(dataloader)\n    for batch in pbar:\n        # batch\n        images, _ = batch",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.train",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.train",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.train",
        "peekOfCode": "def train(model, train_loader, test_loader, noise_scheduler, criterion, optimizer, device, num_epochs=100, img_size=32):\n    \"\"\"\"\"\"\n    for epoch in range(num_epochs):\n        train_loss = train_step(model, train_loader, noise_scheduler, criterion, optimizer, epoch, num_epochs, device)\n        test_loss = test_step(model, test_loader, noise_scheduler, criterion, epoch, num_epochs, device)\n        if epoch % 10 == 0:\n            # 10\n            images = sample(model, noise_scheduler, 10, (3, img_size, img_size), device)\n            # [-1, 1][0, 1],\n            images = ((images + 1) / 2).detach().cpu()",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.train",
        "documentation": {}
    },
    {
        "label": "Block",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.unet",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.unet",
        "peekOfCode": "class Block(nn.Module):\n    def __init__(self, in_channels, out_channels, time_emb_dim, up=False):\n        \"\"\"UNetBlock,/\"\"\"\n        super().__init__()\n        self.time_mlp = nn.Linear(time_emb_dim, out_channels)\n        if up:\n            self.conv1 = nn.Conv2d(2 * in_channels, out_channels, kernel_size=3, padding=1)\n            self.transform = nn.ConvTranspose2d(out_channels, out_channels, kernel_size=4, stride=2, padding=1)\n        else:\n            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.unet",
        "documentation": {}
    },
    {
        "label": "SinusoidalPositionEmbeddings",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.unet",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.unet",
        "peekOfCode": "class SinusoidalPositionEmbeddings(nn.Module):\n    \"\"\",Transformer,\"\"\"\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n    def forward(self, time):\n        device = time.device\n        # ,sincos\n        half_dim = self.dim // 2\n        # ",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.unet",
        "documentation": {}
    },
    {
        "label": "SimpleUnet",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.unet",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.unet",
        "peekOfCode": "class SimpleUnet(nn.Module):\n    \"\"\"UNet,\"\"\"\n    def __init__(self):\n        super().__init__()\n        image_channels = 3\n        down_channels = (64, 128, 256, 512, 1024)\n        up_channels = (1024, 512, 256, 128, 64)\n        out_dim = 3\n        time_emb_dim = 32\n        # ",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.unet",
        "documentation": {}
    },
    {
        "label": "print_shapes",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.unet",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.unet",
        "peekOfCode": "def print_shapes(model, x, time_step):\n    print(\"Input shape:\", x.shape)\n    # \n    t = model.time_embed(time_step)\n    print(\"Time embedding shape:\", t.shape)\n    # \n    x = model.input(x)\n    print(\"After input conv shape:\", x.shape)\n    #\n    residual_stack = []",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.unet",
        "documentation": {}
    },
    {
        "label": "BaseLLM",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyEval.Eval.model.LLM",
        "description": "tiny-universe-main.content.TinyEval.Eval.model.LLM",
        "peekOfCode": "class BaseLLM:\n    def __init__(self, path: str, model_name: str, adapter_path: str) -> None:\n        self.path = path\n        self.model_name = model_name\n        self.adapter_path = adapter_path\n    def build_chat(self, tokenizer, prompt, model_name):\n        pass\n    def load_model_and_tokenizer(self, path, model_name, device):\n        pass\n    def post_process(self, response, model_name):",
        "detail": "tiny-universe-main.content.TinyEval.Eval.model.LLM",
        "documentation": {}
    },
    {
        "label": "internlm2Chat",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyEval.Eval.model.LLM",
        "description": "tiny-universe-main.content.TinyEval.Eval.model.LLM",
        "peekOfCode": "class internlm2Chat(BaseLLM):\n    def __init__(self, path: str, model_name: str = '', adapter_path: str = '') -> None:\n        super().__init__(path, model_name, adapter_path)  # \n    def build_chat(self, prompt):\n        prompt = f'<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n'\n        return prompt\n    def post_process(self, response):\n        response = response.split(\"<|im_end|>\")[0]\n        return response\n    def load_model_and_tokenizer(self, path, device, adapter_path):",
        "detail": "tiny-universe-main.content.TinyEval.Eval.model.LLM",
        "documentation": {}
    },
    {
        "label": "Qwen2Chat",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyEval.Eval.model.LLM",
        "description": "tiny-universe-main.content.TinyEval.Eval.model.LLM",
        "peekOfCode": "class Qwen2Chat(BaseLLM):\n    def __init__(self, path: str, model_name: str = '', adapter_path: str = '') -> None:\n        super().__init__(path, model_name, adapter_path)  # \n    def build_chat(self, prompt, instruct=None):\n        if instruct is None:\n            instruct = 'You are a helpful assistant.'\n        prompt = f'<|im_start|>system\\n{instruct}<im_end>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n'\n        return prompt\n    def load_model_and_tokenizer(self, path, device, adapter_path):\n        model = AutoModelForCausalLM.from_pretrained(path, trust_remote_code=True, torch_dtype=torch.bfloat16).to(device)",
        "detail": "tiny-universe-main.content.TinyEval.Eval.model.LLM",
        "documentation": {}
    },
    {
        "label": "normalize_zh_aswer",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "description": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "peekOfCode": "def normalize_zh_aswer(s):\n    \"\"\",,\"\"\"\n    def white_space_fix(text):\n        return \"\".join(text.split())\n    def remove_punc(text):\n        cn_punctuation = \".\"\n        all_punctuation = set(string.punctuation + cn_punctuation)\n        return ''.join(ch for ch in text if ch not in all_punctuation)\n    def lower(text):\n        return text.lower()",
        "detail": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "documentation": {}
    },
    {
        "label": "normalize_en_answer",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "description": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "peekOfCode": "def normalize_en_answer(s):\n    \"\"\",,.\"\"\"\n    def remove_articles(text):\n        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n    def white_space_fix(text):\n        return \" \".join(text.split())\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \"\".join(ch for ch in text if ch not in exclude)\n    def lower(text):",
        "detail": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "documentation": {}
    },
    {
        "label": "classification_score",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "description": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "peekOfCode": "def classification_score(prediction, ground_truth, **kwargs):\n    em_match_list = []\n    all_classes = kwargs[\"all_classes\"]\n    for class_name in all_classes:\n        if class_name in prediction:                                   # \n            em_match_list.append(class_name)\n    for match_term in em_match_list:\n        if match_term in ground_truth and match_term != ground_truth:  #   'two step'--'step'\n            em_match_list.remove(match_term)\n    if ground_truth in em_match_list:",
        "detail": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "documentation": {}
    },
    {
        "label": "rouge_score",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "description": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "peekOfCode": "def rouge_score(prediction, ground_truth, **kwargs):\n    rouge = Rouge()\n    try:\n        scores = rouge.get_scores([prediction], [ground_truth], avg=True)\n    except:\n        return 0.0\n    return scores[\"rouge-l\"][\"f\"]\ndef rouge_zh_score(prediction, ground_truth, **kwargs):\n    prediction = \" \".join(list(jieba.cut(prediction, cut_all=False)))\n    ground_truth = \" \".join(list(jieba.cut(ground_truth, cut_all=False))) ",
        "detail": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "documentation": {}
    },
    {
        "label": "rouge_zh_score",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "description": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "peekOfCode": "def rouge_zh_score(prediction, ground_truth, **kwargs):\n    prediction = \" \".join(list(jieba.cut(prediction, cut_all=False)))\n    ground_truth = \" \".join(list(jieba.cut(ground_truth, cut_all=False))) \n    score = rouge_score(prediction, ground_truth)\n    return score\ndef f1_score(prediction, ground_truth, **kwargs):\n    # Counterdict,&Counter\n    common = Counter(prediction) & Counter(ground_truth)  \n    num_same = sum(common.values())                       # predictiongt\n    if num_same == 0:",
        "detail": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "description": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "peekOfCode": "def f1_score(prediction, ground_truth, **kwargs):\n    # Counterdict,&Counter\n    common = Counter(prediction) & Counter(ground_truth)  \n    num_same = sum(common.values())                       # predictiongt\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(prediction)          # \n    recall = 1.0 * num_same / len(ground_truth)           # \n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1",
        "detail": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "documentation": {}
    },
    {
        "label": "qa_f1_score",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "description": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "peekOfCode": "def qa_f1_score(prediction, ground_truth, **kwargs):\n    normalized_prediction = normalize_en_answer(prediction)\n    normalized_ground_truth = normalize_en_answer(ground_truth)\n    prediction_tokens = normalized_prediction.split()\n    ground_truth_tokens = normalized_ground_truth.split()\n    return f1_score(prediction_tokens, ground_truth_tokens)\ndef qa_f1_zh_score(prediction, ground_truth, **kwargs):\n    prediction_tokens = list(jieba.cut(prediction, cut_all=False))\n    ground_truth_tokens = list(jieba.cut(ground_truth, cut_all=False))\n    prediction_tokens_norm = [normalize_zh_aswer(t) for t in prediction_tokens]",
        "detail": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "documentation": {}
    },
    {
        "label": "qa_f1_zh_score",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "description": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "peekOfCode": "def qa_f1_zh_score(prediction, ground_truth, **kwargs):\n    prediction_tokens = list(jieba.cut(prediction, cut_all=False))\n    ground_truth_tokens = list(jieba.cut(ground_truth, cut_all=False))\n    prediction_tokens_norm = [normalize_zh_aswer(t) for t in prediction_tokens]\n    ground_truth_tokens_norm = [normalize_zh_aswer(t) for t in ground_truth_tokens]\n    prediction_tokens = [t for t in prediction_tokens_norm if len(t) > 0]\n    ground_truth_tokens = [t for t in ground_truth_tokens_norm if len(t) > 0]\n    return f1_score(prediction_tokens, ground_truth_tokens)\ndef GAOKAO_math(prediction, ground_truth, **kwargs):\n    score = 0",
        "detail": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "documentation": {}
    },
    {
        "label": "GAOKAO_math",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "description": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "peekOfCode": "def GAOKAO_math(prediction, ground_truth, **kwargs):\n    score = 0\n    # \n    if len(ground_truth) > 1:\n        # \n        pattern = r\"[A-D]\"\n        matches = re.findall(pattern, prediction)\n        predicted_answer = ''\n        if matches:\n            # 10",
        "detail": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyEval.eval",
        "description": "tiny-universe-main.content.TinyEval.eval",
        "peekOfCode": "def parse_args(args=None):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model', type=str, default='Qwen2')\n    return parser.parse_args(args)\ndataset2metric = {\n    'multifieldqa_zh': qa_f1_zh_score,\n    'multi_news': rouge_score,\n    'trec': classification_score,\n    'custom_zh': rouge_zh_score,\n    \"GAOKAO_math\": GAOKAO_math",
        "detail": "tiny-universe-main.content.TinyEval.eval",
        "documentation": {}
    },
    {
        "label": "scorer",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyEval.eval",
        "description": "tiny-universe-main.content.TinyEval.eval",
        "peekOfCode": "def scorer(dataset, predictions, answers, all_classes):\n    total_score = 0.\n    for (prediction, ground_truths) in zip(predictions, answers):\n        score = 0.\n        if dataset in [\"trec\"]:\n            prediction = prediction.lstrip('\\n').split('\\n')[0]  # \n        if dataset in ['custom_zh', 'custom_en']:\n            score = max(score, dataset2metric[dataset](prediction, ground_truths, all_classes=all_classes))\n        else:\n            score = max(score, dataset2metric.get(dataset, dataset2metric[dataset])(prediction, ground_truths, all_classes=all_classes))",
        "detail": "tiny-universe-main.content.TinyEval.eval",
        "documentation": {}
    },
    {
        "label": "dataset2metric",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyEval.eval",
        "description": "tiny-universe-main.content.TinyEval.eval",
        "peekOfCode": "dataset2metric = {\n    'multifieldqa_zh': qa_f1_zh_score,\n    'multi_news': rouge_score,\n    'trec': classification_score,\n    'custom_zh': rouge_zh_score,\n    \"GAOKAO_math\": GAOKAO_math\n}\n# \ndef scorer(dataset, predictions, answers, all_classes):\n    total_score = 0.",
        "detail": "tiny-universe-main.content.TinyEval.eval",
        "documentation": {}
    },
    {
        "label": "seed_everything",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyEval.inference",
        "description": "tiny-universe-main.content.TinyEval.inference",
        "peekOfCode": "def seed_everything(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n    torch.cuda.manual_seed_all(seed)\ndef parse_args(args=None):\n    parser = argparse.ArgumentParser()",
        "detail": "tiny-universe-main.content.TinyEval.inference",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyEval.inference",
        "description": "tiny-universe-main.content.TinyEval.inference",
        "peekOfCode": "def parse_args(args=None):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model', type=str, default='Qwen2')\n    return parser.parse_args(args)\nif __name__ == '__main__':\n    seed_everything(42)\n    args = parse_args()\n    model2path = json.load(open(\"Eval/config/model2path.json\", \"r\"))\n    model2maxlen = json.load(open(\"Eval/config/model2maxlen.json\", \"r\"))\n    adapter2path = json.load(open(\"Eval/config/adapter2path.json\", \"r\"))",
        "detail": "tiny-universe-main.content.TinyEval.inference",
        "documentation": {}
    },
    {
        "label": "load_qwen_vlm",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.ImgEvaluator",
        "description": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.ImgEvaluator",
        "peekOfCode": "def load_qwen_vlm(pretrained_model=\"./model/Qwen/Qwen2.5-VL-3B-Instruct\"):\n    min_pixels = 256 * 28 * 28\n    max_pixels = 1280 * 28 * 28\n    # default: Load the model on the available device(s)\n    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n        pretrained_model, torch_dtype=\"auto\", device_map=\"auto\"\n    )\n    # default processer\n    processor = AutoProcessor.from_pretrained(pretrained_model, min_pixels=min_pixels,\n                                              max_pixels=max_pixels)",
        "detail": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.ImgEvaluator",
        "documentation": {}
    },
    {
        "label": "run_qwen_vl",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.ImgEvaluator",
        "description": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.ImgEvaluator",
        "peekOfCode": "def run_qwen_vl(image_path,prompt,model,processor):\n    # \n    with open(image_path, \"rb\") as image_file:\n        base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image\",",
        "detail": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.ImgEvaluator",
        "documentation": {}
    },
    {
        "label": "SDXLGenerator",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.ImgGenerator",
        "description": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.ImgGenerator",
        "peekOfCode": "class SDXLGenerator:\n    def __init__(self, prompt, output_path, steps=50, seed=0,\n                 use_image_guidance=False, image_path=None, ip_scale=0.5,\n                 sd_path=\"./model/stabilityai/stable-diffusion-xl-base-1.0\",\n                 adapter_path=\"./model/h94/IP-Adapter\"):\n        self.prompt = prompt\n        self.output_path = output_path\n        self.steps = steps\n        self.seed = seed\n        self.use_image_guidance = use_image_guidance",
        "detail": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.ImgGenerator",
        "documentation": {}
    },
    {
        "label": "get_clip_similarities",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.ImgRetrieval",
        "description": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.ImgRetrieval",
        "peekOfCode": "def get_clip_similarities(prompts, image_paths, embeddings_path=\"./datasets/vector_bases\", bs=2, k=5, device='cuda:0',\n                          model_path=\"./model/ViT-B-32.pt\"):\n    \"\"\"\n    Calculate similarity between text prompts and images using CLIP model.\n    Args:\n        prompts: List of text prompts to compare against images\n        image_paths: List of paths to images\n        embeddings_path: Directory to save/load precomputed image embeddings\n        bs: Batch size for processing images\n        k: Number of top similar images to return",
        "detail": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.ImgRetrieval",
        "documentation": {}
    },
    {
        "label": "load_qwen_llm",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.RewritePrompt",
        "description": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.RewritePrompt",
        "peekOfCode": "def load_qwen_llm(model_name = \"./model/Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\"):\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        torch_dtype=\"auto\",\n        device_map=\"auto\"\n    )\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    return model,tokenizer\ndef run_qwen_llm(prompt,model,tokenizer):\n    messages = [",
        "detail": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.RewritePrompt",
        "documentation": {}
    },
    {
        "label": "run_qwen_llm",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.RewritePrompt",
        "description": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.RewritePrompt",
        "peekOfCode": "def run_qwen_llm(prompt,model,tokenizer):\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )",
        "detail": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.RewritePrompt",
        "documentation": {}
    },
    {
        "label": "available_models",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "peekOfCode": "def available_models() -> List[str]:\n    \"\"\"Returns the names of available CLIP models\"\"\"\n    return list(_MODELS.keys())\ndef load(name: str, device: Union[str, torch.device] = \"cuda\" if torch.cuda.is_available() else \"cpu\", jit: bool = False, download_root: str = None):\n    \"\"\"Load a CLIP model\n    Parameters\n    ----------\n    name : str\n        A model name listed by `clip.available_models()`, or the path to a model checkpoint containing the state_dict\n    device : Union[str, torch.device]",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "documentation": {}
    },
    {
        "label": "load",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "peekOfCode": "def load(name: str, device: Union[str, torch.device] = \"cuda\" if torch.cuda.is_available() else \"cpu\", jit: bool = False, download_root: str = None):\n    \"\"\"Load a CLIP model\n    Parameters\n    ----------\n    name : str\n        A model name listed by `clip.available_models()`, or the path to a model checkpoint containing the state_dict\n    device : Union[str, torch.device]\n        The device to put the loaded model\n    jit : bool\n        Whether to load the optimized JIT model or more hackable non-JIT model (default).",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "documentation": {}
    },
    {
        "label": "tokenize",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "peekOfCode": "def tokenize(texts: Union[str, List[str]], context_length: int = 77, truncate: bool = False) -> Union[torch.IntTensor, torch.LongTensor]:\n    \"\"\"\n    Returns the tokenized representation of given input string(s)\n    Parameters\n    ----------\n    texts : Union[str, List[str]]\n        An input string or a list of input strings to tokenize\n    context_length : int\n        The context length to use; all CLIP models use 77 as the context length\n    truncate: bool",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "peekOfCode": "__all__ = [\"available_models\", \"load\", \"tokenize\"]\n_tokenizer = _Tokenizer()\n_MODELS = {\n    \"RN50\": \"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\",\n    \"RN101\": \"https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt\",\n    \"RN50x4\": \"https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt\",\n    \"RN50x16\": \"https://openaipublic.azureedge.net/clip/models/52378b407f34354e150460fe41077663dd5b39c54cd0bfd2b27167a4a06ec9aa/RN50x16.pt\",\n    \"RN50x64\": \"https://openaipublic.azureedge.net/clip/models/be1cfb55d75a9666199fb2206c106743da0f6468c9d327f3e0d0a543a9919d9c/RN50x64.pt\",\n    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n    \"ViT-B/16\": \"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\",",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "documentation": {}
    },
    {
        "label": "_tokenizer",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "peekOfCode": "_tokenizer = _Tokenizer()\n_MODELS = {\n    \"RN50\": \"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\",\n    \"RN101\": \"https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt\",\n    \"RN50x4\": \"https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt\",\n    \"RN50x16\": \"https://openaipublic.azureedge.net/clip/models/52378b407f34354e150460fe41077663dd5b39c54cd0bfd2b27167a4a06ec9aa/RN50x16.pt\",\n    \"RN50x64\": \"https://openaipublic.azureedge.net/clip/models/be1cfb55d75a9666199fb2206c106743da0f6468c9d327f3e0d0a543a9919d9c/RN50x64.pt\",\n    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n    \"ViT-B/16\": \"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\",\n    \"ViT-L/14\": \"https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt\",",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "documentation": {}
    },
    {
        "label": "_MODELS",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "peekOfCode": "_MODELS = {\n    \"RN50\": \"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\",\n    \"RN101\": \"https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt\",\n    \"RN50x4\": \"https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt\",\n    \"RN50x16\": \"https://openaipublic.azureedge.net/clip/models/52378b407f34354e150460fe41077663dd5b39c54cd0bfd2b27167a4a06ec9aa/RN50x16.pt\",\n    \"RN50x64\": \"https://openaipublic.azureedge.net/clip/models/be1cfb55d75a9666199fb2206c106743da0f6468c9d327f3e0d0a543a9919d9c/RN50x64.pt\",\n    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n    \"ViT-B/16\": \"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\",\n    \"ViT-L/14\": \"https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt\",\n    \"ViT-L/14@336px\": \"https://openaipublic.azureedge.net/clip/models/3035c92b350959924f9f00213499208652fc7ea050643e8b385c2dac08641f02/ViT-L-14-336px.pt\",",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "documentation": {}
    },
    {
        "label": "Bottleneck",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "peekOfCode": "class Bottleneck(nn.Module):\n    expansion = 4\n    def __init__(self, inplanes, planes, stride=1):\n        super().__init__()\n        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "documentation": {}
    },
    {
        "label": "AttentionPool2d",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "peekOfCode": "class AttentionPool2d(nn.Module):\n    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n        super().__init__()\n        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n        self.num_heads = num_heads\n    def forward(self, x):",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "documentation": {}
    },
    {
        "label": "ModifiedResNet",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "peekOfCode": "class ModifiedResNet(nn.Module):\n    \"\"\"\n    A ResNet class that is similar to torchvision's but contains the following changes:\n    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n    - The final pooling layer is a QKV attention instead of an average pool\n    \"\"\"\n    def __init__(self, layers, output_dim, heads, input_resolution=224, width=64):\n        super().__init__()\n        self.output_dim = output_dim",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "documentation": {}
    },
    {
        "label": "LayerNorm",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "peekOfCode": "class LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        ret = super().forward(x.type(torch.float32))\n        return ret.type(orig_type)\nclass QuickGELU(nn.Module):\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\nclass ResidualAttentionBlock(nn.Module):",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "documentation": {}
    },
    {
        "label": "QuickGELU",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "peekOfCode": "class QuickGELU(nn.Module):\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\nclass ResidualAttentionBlock(nn.Module):\n    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(d_model, n_head)\n        self.ln_1 = LayerNorm(d_model)\n        self.mlp = nn.Sequential(OrderedDict([\n            (\"c_fc\", nn.Linear(d_model, d_model * 4)),",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "documentation": {}
    },
    {
        "label": "ResidualAttentionBlock",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "peekOfCode": "class ResidualAttentionBlock(nn.Module):\n    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(d_model, n_head)\n        self.ln_1 = LayerNorm(d_model)\n        self.mlp = nn.Sequential(OrderedDict([\n            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n            (\"gelu\", QuickGELU()),\n            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n        ]))",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "peekOfCode": "class Transformer(nn.Module):\n    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n        self.width = width\n        self.layers = layers\n        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n    def forward(self, x: torch.Tensor):\n        return self.resblocks(x)\nclass VisionTransformer(nn.Module):\n    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int):",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "documentation": {}
    },
    {
        "label": "VisionTransformer",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "peekOfCode": "class VisionTransformer(nn.Module):\n    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int):\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.output_dim = output_dim\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)\n        scale = width ** -0.5\n        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n        self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width))\n        self.ln_pre = LayerNorm(width)",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "documentation": {}
    },
    {
        "label": "CLIP",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "peekOfCode": "class CLIP(nn.Module):\n    def __init__(self,\n                 embed_dim: int,\n                 # vision\n                 image_resolution: int,\n                 vision_layers: Union[Tuple[int, int, int, int], int],\n                 vision_width: int,\n                 vision_patch_size: int,\n                 # text\n                 context_length: int,",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "documentation": {}
    },
    {
        "label": "convert_weights",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "peekOfCode": "def convert_weights(model: nn.Module):\n    \"\"\"Convert applicable model parameters to fp16\"\"\"\n    def _convert_weights_to_fp16(l):\n        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n            l.weight.data = l.weight.data.half()\n            if l.bias is not None:\n                l.bias.data = l.bias.data.half()\n        if isinstance(l, nn.MultiheadAttention):\n            for attr in [*[f\"{s}_proj_weight\" for s in [\"in\", \"q\", \"k\", \"v\"]], \"in_proj_bias\", \"bias_k\", \"bias_v\"]:\n                tensor = getattr(l, attr)",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "documentation": {}
    },
    {
        "label": "build_model",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "peekOfCode": "def build_model(state_dict: dict):\n    vit = \"visual.proj\" in state_dict\n    if vit:\n        vision_width = state_dict[\"visual.conv1.weight\"].shape[0]\n        vision_layers = len([k for k in state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n        vision_patch_size = state_dict[\"visual.conv1.weight\"].shape[-1]\n        grid_size = round((state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n        image_resolution = vision_patch_size * grid_size\n    else:\n        counts: list = [len(set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"visual.layer{b}\"))) for b in [1, 2, 3, 4]]",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "documentation": {}
    },
    {
        "label": "SimpleTokenizer",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "peekOfCode": "class SimpleTokenizer(object):\n    def __init__(self, bpe_path: str = default_bpe()):\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')\n        merges = merges[1:49152-256-2+1]\n        merges = [tuple(merge.split()) for merge in merges]\n        vocab = list(bytes_to_unicode().values())\n        vocab = vocab + [v+'</w>' for v in vocab]\n        for merge in merges:",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "documentation": {}
    },
    {
        "label": "default_bpe",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "peekOfCode": "def default_bpe():\n    return os.path.join(os.path.dirname(os.path.abspath(__file__)), \"bpe_simple_vocab_16e6.txt.gz\")\n@lru_cache()\ndef bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "documentation": {}
    },
    {
        "label": "bytes_to_unicode",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "peekOfCode": "def bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "documentation": {}
    },
    {
        "label": "get_pairs",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "peekOfCode": "def get_pairs(word):\n    \"\"\"Return set of symbol pairs in a word.\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    \"\"\"\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "documentation": {}
    },
    {
        "label": "basic_clean",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "peekOfCode": "def basic_clean(text):\n    text = ftfy.fix_text(text)\n    text = html.unescape(html.unescape(text))\n    return text.strip()\ndef whitespace_clean(text):\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.strip()\n    return text\nclass SimpleTokenizer(object):\n    def __init__(self, bpe_path: str = default_bpe()):",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "documentation": {}
    },
    {
        "label": "whitespace_clean",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "peekOfCode": "def whitespace_clean(text):\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.strip()\n    return text\nclass SimpleTokenizer(object):\n    def __init__(self, bpe_path: str = default_bpe()):\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')\n        merges = merges[1:49152-256-2+1]",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "documentation": {}
    },
    {
        "label": "test_consistency",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.tests.test_consistency",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.tests.test_consistency",
        "peekOfCode": "def test_consistency(model_name):\n    device = \"cpu\"\n    jit_model, transform = clip.load(model_name, device=device, jit=True)\n    py_model, _ = clip.load(model_name, device=device, jit=False)\n    image = transform(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)\n    text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n    with torch.no_grad():\n        logits_per_image, _ = jit_model(image, text)\n        jit_probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n        logits_per_image, _ = py_model(image, text)",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.tests.test_consistency",
        "documentation": {}
    },
    {
        "label": "tokenize",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.hubconf",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.hubconf",
        "peekOfCode": "def tokenize():\n    return _tokenize\n_entrypoints = {model_functions[model]: _create_hub_entrypoint(model) for model in _available_models()}\nglobals().update(_entrypoints)",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.hubconf",
        "documentation": {}
    },
    {
        "label": "dependencies",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.hubconf",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.hubconf",
        "peekOfCode": "dependencies = [\"torch\", \"torchvision\", \"ftfy\", \"regex\", \"tqdm\"]\n# For compatibility (cannot include special characters in function name)\nmodel_functions = { model: re.sub(f'[{string.punctuation}]', '_', model) for model in _available_models()}\ndef _create_hub_entrypoint(model):\n    def entrypoint(**kwargs):      \n        return _load(model, **kwargs)\n    entrypoint.__doc__ = f\"\"\"Loads the {model} CLIP model\n        Parameters\n        ----------\n        device : Union[str, torch.device]",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.hubconf",
        "documentation": {}
    },
    {
        "label": "model_functions",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.hubconf",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.hubconf",
        "peekOfCode": "model_functions = { model: re.sub(f'[{string.punctuation}]', '_', model) for model in _available_models()}\ndef _create_hub_entrypoint(model):\n    def entrypoint(**kwargs):      \n        return _load(model, **kwargs)\n    entrypoint.__doc__ = f\"\"\"Loads the {model} CLIP model\n        Parameters\n        ----------\n        device : Union[str, torch.device]\n            The device to put the loaded model\n        jit : bool",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.hubconf",
        "documentation": {}
    },
    {
        "label": "_entrypoints",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.hubconf",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.hubconf",
        "peekOfCode": "_entrypoints = {model_functions[model]: _create_hub_entrypoint(model) for model in _available_models()}\nglobals().update(_entrypoints)",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.hubconf",
        "documentation": {}
    },
    {
        "label": "model_dir",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.download_model",
        "description": "tiny-universe-main.content.TinyIMGRAG.download_model",
        "peekOfCode": "model_dir = modelscope.snapshot_download('Qwen/Qwen2.5-VL-3B-Instruct', cache_dir='./model/', revision='master')\nmodel_dir = modelscope.snapshot_download('Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4', cache_dir='./model/', revision='master')\nmodel_dir = modelscope.snapshot_download('stabilityai/stable-diffusion-xl-base-1.0', cache_dir='./model/', revision='master')\nmodel_dir = huggingface_hub.snapshot_download(repo_id=\"h94/IP-Adapter\", local_dir=\"./model/\", max_workers=1)",
        "detail": "tiny-universe-main.content.TinyIMGRAG.download_model",
        "documentation": {}
    },
    {
        "label": "model_dir",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.download_model",
        "description": "tiny-universe-main.content.TinyIMGRAG.download_model",
        "peekOfCode": "model_dir = modelscope.snapshot_download('Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4', cache_dir='./model/', revision='master')\nmodel_dir = modelscope.snapshot_download('stabilityai/stable-diffusion-xl-base-1.0', cache_dir='./model/', revision='master')\nmodel_dir = huggingface_hub.snapshot_download(repo_id=\"h94/IP-Adapter\", local_dir=\"./model/\", max_workers=1)",
        "detail": "tiny-universe-main.content.TinyIMGRAG.download_model",
        "documentation": {}
    },
    {
        "label": "model_dir",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.download_model",
        "description": "tiny-universe-main.content.TinyIMGRAG.download_model",
        "peekOfCode": "model_dir = modelscope.snapshot_download('stabilityai/stable-diffusion-xl-base-1.0', cache_dir='./model/', revision='master')\nmodel_dir = huggingface_hub.snapshot_download(repo_id=\"h94/IP-Adapter\", local_dir=\"./model/\", max_workers=1)",
        "detail": "tiny-universe-main.content.TinyIMGRAG.download_model",
        "documentation": {}
    },
    {
        "label": "model_dir",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.download_model",
        "description": "tiny-universe-main.content.TinyIMGRAG.download_model",
        "peekOfCode": "model_dir = huggingface_hub.snapshot_download(repo_id=\"h94/IP-Adapter\", local_dir=\"./model/\", max_workers=1)",
        "detail": "tiny-universe-main.content.TinyIMGRAG.download_model",
        "documentation": {}
    },
    {
        "label": "ImageRAGPipeline",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.main",
        "description": "tiny-universe-main.content.TinyIMGRAG.main",
        "peekOfCode": "class ImageRAGPipeline:\n    def __init__(self, base_output_dir=\"./datasets/results\"):\n        self.base_output_dir = base_output_dir\n        os.makedirs(self.base_output_dir, exist_ok=True)\n        # Initialize all models as None (lazy loading)\n        self.vl_model = None\n        self.vl_processor = None\n        self.llm_model = None\n        self.llm_tokenizer = None\n        self.clip_model = None",
        "detail": "tiny-universe-main.content.TinyIMGRAG.main",
        "documentation": {}
    },
    {
        "label": "ModelArgs",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyLLM.code.model",
        "description": "tiny-universe-main.content.TinyLLM.code.model",
        "peekOfCode": "class ModelArgs:\n    # \n    dim: int = 288  # \n    n_layers: int = 6  # Transformer\n    n_heads: int = 6  # \n    n_kv_heads: Optional[int] = 6  # /n_heads\n    vocab_size: int = 32000  # \n    hidden_dim: Optional[int] = None  # \n    multiple_of: int = 32  # MLP\n    norm_eps: float = 1e-5  # epsilon",
        "detail": "tiny-universe-main.content.TinyLLM.code.model",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyLLM.code.model",
        "description": "tiny-universe-main.content.TinyLLM.code.model",
        "peekOfCode": "class RMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float):\n        super().__init__()\n        # eps0\n        self.eps = eps\n        # weight1\n        self.weight = nn.Parameter(torch.ones(dim))\n    def _norm(self, x):\n        # RMSNorm\n        # x.pow(2).mean(-1, keepdim=True)x",
        "detail": "tiny-universe-main.content.TinyLLM.code.model",
        "documentation": {}
    },
    {
        "label": "Attention",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyLLM.code.model",
        "description": "tiny-universe-main.content.TinyLLM.code.model",
        "peekOfCode": "class Attention(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        # n_kv_headskeyvalue\n        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n        # \n        assert args.n_heads % self.n_kv_heads == 0\n        # 1\n        model_parallel_size = 1\n        # ",
        "detail": "tiny-universe-main.content.TinyLLM.code.model",
        "documentation": {}
    },
    {
        "label": "MLP",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyLLM.code.model",
        "description": "tiny-universe-main.content.TinyLLM.code.model",
        "peekOfCode": "class MLP(nn.Module):\n    def __init__(self, dim: int, hidden_dim: int, multiple_of: int, dropout: float):\n        super().__init__()\n        # 4\n        # 2/3multiple_of\n        if hidden_dim is None:\n            hidden_dim = 4 * dim\n            hidden_dim = int(2 * hidden_dim / 3)\n            hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n        # ",
        "detail": "tiny-universe-main.content.TinyLLM.code.model",
        "documentation": {}
    },
    {
        "label": "DecoderLayer",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyLLM.code.model",
        "description": "tiny-universe-main.content.TinyLLM.code.model",
        "peekOfCode": "class DecoderLayer(nn.Module):\n    def __init__(self, layer_id: int, args: ModelArgs):\n        super().__init__()\n        # \n        self.n_heads = args.n_heads\n        # \n        self.dim = args.dim\n        # \n        self.head_dim = args.dim // args.n_heads\n        # LLaMA2Attention",
        "detail": "tiny-universe-main.content.TinyLLM.code.model",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyLLM.code.model",
        "description": "tiny-universe-main.content.TinyLLM.code.model",
        "peekOfCode": "class Transformer(nn.Module):\n    last_loss: Optional[torch.Tensor]\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        # \n        self.args = args\n        # \n        self.vocab_size = args.vocab_size\n        # \n        self.n_layers = args.n_layers",
        "detail": "tiny-universe-main.content.TinyLLM.code.model",
        "documentation": {}
    },
    {
        "label": "precompute_freqs_cis",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyLLM.code.model",
        "description": "tiny-universe-main.content.TinyLLM.code.model",
        "peekOfCode": "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n    # torch.arange(0, dim, 2)[: (dim // 2)].float()02dim\n    # dimtheta\n    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n    # 0endend\n    t = torch.arange(end, device=freqs.device)\n    # tfreqs\n    freqs = torch.outer(t, freqs).float()\n    # \n    freqs_cos = torch.cos(freqs)",
        "detail": "tiny-universe-main.content.TinyLLM.code.model",
        "documentation": {}
    },
    {
        "label": "reshape_for_broadcast",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyLLM.code.model",
        "description": "tiny-universe-main.content.TinyLLM.code.model",
        "peekOfCode": "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n    # x\n    ndim = x.ndim\n    # 1x\n    assert 0 <= 1 < ndim\n    # freqs_cisx\n    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n    # 1freqs_cisx\n    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n    # freqs_cis",
        "detail": "tiny-universe-main.content.TinyLLM.code.model",
        "documentation": {}
    },
    {
        "label": "apply_rotary_emb",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyLLM.code.model",
        "description": "tiny-universe-main.content.TinyLLM.code.model",
        "peekOfCode": "def apply_rotary_emb(\n    xq: torch.Tensor,\n    xk: torch.Tensor,\n    freqs_cos: torch.Tensor,\n    freqs_sin: torch.Tensor\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    # \n    xq_r, xq_i = xq.float().reshape(xq.shape[:-1] + (-1, 2)).unbind(-1)\n    xk_r, xk_i = xk.float().reshape(xk.shape[:-1] + (-1, 2)).unbind(-1)\n    # ",
        "detail": "tiny-universe-main.content.TinyLLM.code.model",
        "documentation": {}
    },
    {
        "label": "repeat_kv",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyLLM.code.model",
        "description": "tiny-universe-main.content.TinyLLM.code.model",
        "peekOfCode": "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n    # /\n    bs, slen, n_kv_heads, head_dim = x.shape\n    # 1\n    if n_rep == 1:\n        return x\n    # \n    return (\n        x[:, :, :, None, :]  # \n        .expand(bs, slen, n_kv_heads, n_rep, head_dim)  # n_rep",
        "detail": "tiny-universe-main.content.TinyLLM.code.model",
        "documentation": {}
    },
    {
        "label": "PretokDataset",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "description": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "peekOfCode": "class PretokDataset(torch.utils.data.IterableDataset):\n    \"\"\" PyTorch \"\"\"\n    def __init__(self, split, max_seq_len, vocab_size, vocab_source):\n        \"\"\"\n        \n        :\n        split: str, 'train'  'test'\n        max_seq_len: int, \n        vocab_size: int, \n        vocab_source: str, 'llama2'  'custom'",
        "detail": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "documentation": {}
    },
    {
        "label": "Task",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "description": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "peekOfCode": "class Task:\n    @staticmethod\n    def iter_batches(batch_size, device, num_workers=0, **dataset_kwargs):\n        ds = PretokDataset(**dataset_kwargs)\n        dl = torch.utils.data.DataLoader(\n            ds, batch_size=batch_size, pin_memory=True, num_workers=num_workers\n        )\n        for x, y in dl:\n            x = x.to(device, non_blocking=True)\n            y = y.to(device, non_blocking=True)",
        "detail": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "documentation": {}
    },
    {
        "label": "process_shard",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "description": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "peekOfCode": "def process_shard(args, vocab_size, tokenizer_model_path):\n    \"\"\"\n    \n    :\n    args: tuple, ID\n    vocab_size: int, \n    \"\"\"\n    # ID\n    shard_id, shard = args\n    # ",
        "detail": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "documentation": {}
    },
    {
        "label": "pretokenize",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "description": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "peekOfCode": "def pretokenize(vocab_size):\n    \"\"\"\n    \n    :\n    vocab_size: int, \n    \"\"\"\n    # \n    data_dir = os.path.join(DATA_CACHE_DIR, \"TinyStories_all_data\")\n    # JSON\n    shard_filenames = sorted(glob.glob(os.path.join(data_dir, \"*.json\")))",
        "detail": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "documentation": {}
    },
    {
        "label": "DATA_CACHE_DIR",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "description": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "peekOfCode": "DATA_CACHE_DIR = 'data'\nTOKENIZER_MODEL = \"./data/tok4096.model\"\n# \ndef process_shard(args, vocab_size, tokenizer_model_path):\n    \"\"\"\n    \n    :\n    args: tuple, ID\n    vocab_size: int, \n    \"\"\"",
        "detail": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "documentation": {}
    },
    {
        "label": "TOKENIZER_MODEL",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "description": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "peekOfCode": "TOKENIZER_MODEL = \"./data/tok4096.model\"\n# \ndef process_shard(args, vocab_size, tokenizer_model_path):\n    \"\"\"\n    \n    :\n    args: tuple, ID\n    vocab_size: int, \n    \"\"\"\n    # ID",
        "detail": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "documentation": {}
    },
    {
        "label": "TextGenerator",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyLLM.code.sample",
        "description": "tiny-universe-main.content.TinyLLM.code.sample",
        "peekOfCode": "class TextGenerator:\n    def __init__(self, \n                 checkpoint='output/ckpt.pt',  # \n                 tokenizer_model_path='tok4096.model',  # \n                 seed=1337,  # \n                 device=None,  #  CUDA CUDA CPU\n                 dtype=\"float32\"):  #  float32 float16  bfloat16\n        \"\"\"\n         TextGenerator \n        \"\"\"",
        "detail": "tiny-universe-main.content.TinyLLM.code.sample",
        "documentation": {}
    },
    {
        "label": "Tokenizer",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyLLM.code.tokenizer",
        "description": "tiny-universe-main.content.TinyLLM.code.tokenizer",
        "peekOfCode": "class Tokenizer:\n    def __init__(self, tokenizer_model=None):\n        \"\"\"\n        SentencePiecetoken ID\n        :\n        tokenizer_model: str,  TOKENIZER_MODEL\n        \"\"\"\n        # \n        model_path = tokenizer_model if tokenizer_model else TOKENIZER_MODEL\n        # ",
        "detail": "tiny-universe-main.content.TinyLLM.code.tokenizer",
        "documentation": {}
    },
    {
        "label": "TOKENIZER_MODEL",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.tokenizer",
        "description": "tiny-universe-main.content.TinyLLM.code.tokenizer",
        "peekOfCode": "TOKENIZER_MODEL = \"./data/tok4096.model\"\nclass Tokenizer:\n    def __init__(self, tokenizer_model=None):\n        \"\"\"\n        SentencePiecetoken ID\n        :\n        tokenizer_model: str,  TOKENIZER_MODEL\n        \"\"\"\n        # \n        model_path = tokenizer_model if tokenizer_model else TOKENIZER_MODEL",
        "detail": "tiny-universe-main.content.TinyLLM.code.tokenizer",
        "documentation": {}
    },
    {
        "label": "estimate_loss",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "def estimate_loss():\n    out = {}  # \n    model.eval()  #  dropout  batchnorm \n    for split in [\"train\", \"val\"]:  # \n        batch_iter = iter_batches(split=split)  # \n        losses = torch.zeros(eval_iters)  #  CPU \n        for k in range(eval_iters):  # \n            X, Y = next(batch_iter)  #  X  Y\n            with ctx:  #  torch.autocast()\n                logits = model(X, Y)  # ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "get_lr",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "def get_lr(it):\n    \"\"\"\n     it \n    \n    \"\"\"\n    # 1)  warmup_iters \n    if it < warmup_iters:\n        return learning_rate * it / warmup_iters  # \n    # 2)  lr_decay_iters min_lr\n    if it > lr_decay_iters:",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "out_dir",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "out_dir = \"output\"  # \neval_interval = 2000  # \nlog_interval = 1  # \neval_iters = 100  # \neval_only = False  # True\nalways_save_checkpoint = False  # True\ninit_from = \"scratch\"  # 'scratch''resume'\n# \nbatch_size = 8  # \nmax_seq_len = 256  # ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "eval_interval",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "eval_interval = 2000  # \nlog_interval = 1  # \neval_iters = 100  # \neval_only = False  # True\nalways_save_checkpoint = False  # True\ninit_from = \"scratch\"  # 'scratch''resume'\n# \nbatch_size = 8  # \nmax_seq_len = 256  # \nvocab_size = 4096  # ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "log_interval",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "log_interval = 1  # \neval_iters = 100  # \neval_only = False  # True\nalways_save_checkpoint = False  # True\ninit_from = \"scratch\"  # 'scratch''resume'\n# \nbatch_size = 8  # \nmax_seq_len = 256  # \nvocab_size = 4096  # \n# ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "eval_iters",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "eval_iters = 100  # \neval_only = False  # True\nalways_save_checkpoint = False  # True\ninit_from = \"scratch\"  # 'scratch''resume'\n# \nbatch_size = 8  # \nmax_seq_len = 256  # \nvocab_size = 4096  # \n# \ndim = 288  # ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "eval_only",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "eval_only = False  # True\nalways_save_checkpoint = False  # True\ninit_from = \"scratch\"  # 'scratch''resume'\n# \nbatch_size = 8  # \nmax_seq_len = 256  # \nvocab_size = 4096  # \n# \ndim = 288  # \nn_layers = 8  # Transformer",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "always_save_checkpoint",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "always_save_checkpoint = False  # True\ninit_from = \"scratch\"  # 'scratch''resume'\n# \nbatch_size = 8  # \nmax_seq_len = 256  # \nvocab_size = 4096  # \n# \ndim = 288  # \nn_layers = 8  # Transformer\nn_heads = 8  # ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "init_from",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "init_from = \"scratch\"  # 'scratch''resume'\n# \nbatch_size = 8  # \nmax_seq_len = 256  # \nvocab_size = 4096  # \n# \ndim = 288  # \nn_layers = 8  # Transformer\nn_heads = 8  # \nn_kv_heads = 4  # ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "batch_size = 8  # \nmax_seq_len = 256  # \nvocab_size = 4096  # \n# \ndim = 288  # \nn_layers = 8  # Transformer\nn_heads = 8  # \nn_kv_heads = 4  # \nmultiple_of = 32  # \ndropout = 0.0  # Dropout",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "max_seq_len",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "max_seq_len = 256  # \nvocab_size = 4096  # \n# \ndim = 288  # \nn_layers = 8  # Transformer\nn_heads = 8  # \nn_kv_heads = 4  # \nmultiple_of = 32  # \ndropout = 0.0  # Dropout\n# AdamW",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "vocab_size",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "vocab_size = 4096  # \n# \ndim = 288  # \nn_layers = 8  # Transformer\nn_heads = 8  # \nn_kv_heads = 4  # \nmultiple_of = 32  # \ndropout = 0.0  # Dropout\n# AdamW\ngradient_accumulation_steps = 4  # ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "dim",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "dim = 288  # \nn_layers = 8  # Transformer\nn_heads = 8  # \nn_kv_heads = 4  # \nmultiple_of = 32  # \ndropout = 0.0  # Dropout\n# AdamW\ngradient_accumulation_steps = 4  # \nlearning_rate = 5e-4  # \nmax_iters = 100000  # ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "n_layers",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "n_layers = 8  # Transformer\nn_heads = 8  # \nn_kv_heads = 4  # \nmultiple_of = 32  # \ndropout = 0.0  # Dropout\n# AdamW\ngradient_accumulation_steps = 4  # \nlearning_rate = 5e-4  # \nmax_iters = 100000  # \nweight_decay = 1e-1  # ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "n_heads",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "n_heads = 8  # \nn_kv_heads = 4  # \nmultiple_of = 32  # \ndropout = 0.0  # Dropout\n# AdamW\ngradient_accumulation_steps = 4  # \nlearning_rate = 5e-4  # \nmax_iters = 100000  # \nweight_decay = 1e-1  # \nbeta1 = 0.9  # AdamW1",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "n_kv_heads",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "n_kv_heads = 4  # \nmultiple_of = 32  # \ndropout = 0.0  # Dropout\n# AdamW\ngradient_accumulation_steps = 4  # \nlearning_rate = 5e-4  # \nmax_iters = 100000  # \nweight_decay = 1e-1  # \nbeta1 = 0.9  # AdamW1\nbeta2 = 0.95  # AdamW2",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "multiple_of",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "multiple_of = 32  # \ndropout = 0.0  # Dropout\n# AdamW\ngradient_accumulation_steps = 4  # \nlearning_rate = 5e-4  # \nmax_iters = 100000  # \nweight_decay = 1e-1  # \nbeta1 = 0.9  # AdamW1\nbeta2 = 0.95  # AdamW2\ngrad_clip = 1.0  # 0",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "dropout",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "dropout = 0.0  # Dropout\n# AdamW\ngradient_accumulation_steps = 4  # \nlearning_rate = 5e-4  # \nmax_iters = 100000  # \nweight_decay = 1e-1  # \nbeta1 = 0.9  # AdamW1\nbeta2 = 0.95  # AdamW2\ngrad_clip = 1.0  # 0\n# ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "gradient_accumulation_steps",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "gradient_accumulation_steps = 4  # \nlearning_rate = 5e-4  # \nmax_iters = 100000  # \nweight_decay = 1e-1  # \nbeta1 = 0.9  # AdamW1\nbeta2 = 0.95  # AdamW2\ngrad_clip = 1.0  # 0\n# \ndecay_lr = True  # \nwarmup_iters = 1000  # ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "learning_rate",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "learning_rate = 5e-4  # \nmax_iters = 100000  # \nweight_decay = 1e-1  # \nbeta1 = 0.9  # AdamW1\nbeta2 = 0.95  # AdamW2\ngrad_clip = 1.0  # 0\n# \ndecay_lr = True  # \nwarmup_iters = 1000  # \n# ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "max_iters",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "max_iters = 100000  # \nweight_decay = 1e-1  # \nbeta1 = 0.9  # AdamW1\nbeta2 = 0.95  # AdamW2\ngrad_clip = 1.0  # 0\n# \ndecay_lr = True  # \nwarmup_iters = 1000  # \n# \ndevice = \"cuda:0\"  # 'cpu''cuda''cuda:0'",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "weight_decay",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "weight_decay = 1e-1  # \nbeta1 = 0.9  # AdamW1\nbeta2 = 0.95  # AdamW2\ngrad_clip = 1.0  # 0\n# \ndecay_lr = True  # \nwarmup_iters = 1000  # \n# \ndevice = \"cuda:0\"  # 'cpu''cuda''cuda:0'\ndtype = \"bfloat16\"  # 'float32''bfloat16''float16'",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "beta1",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "beta1 = 0.9  # AdamW1\nbeta2 = 0.95  # AdamW2\ngrad_clip = 1.0  # 0\n# \ndecay_lr = True  # \nwarmup_iters = 1000  # \n# \ndevice = \"cuda:0\"  # 'cpu''cuda''cuda:0'\ndtype = \"bfloat16\"  # 'float32''bfloat16''float16'\n# -----------------------------------------------------------------------------",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "beta2",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "beta2 = 0.95  # AdamW2\ngrad_clip = 1.0  # 0\n# \ndecay_lr = True  # \nwarmup_iters = 1000  # \n# \ndevice = \"cuda:0\"  # 'cpu''cuda''cuda:0'\ndtype = \"bfloat16\"  # 'float32''bfloat16''float16'\n# -----------------------------------------------------------------------------\n# ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "grad_clip",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "grad_clip = 1.0  # 0\n# \ndecay_lr = True  # \nwarmup_iters = 1000  # \n# \ndevice = \"cuda:0\"  # 'cpu''cuda''cuda:0'\ndtype = \"bfloat16\"  # 'float32''bfloat16''float16'\n# -----------------------------------------------------------------------------\n# \nconfig_keys = [",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "decay_lr",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "decay_lr = True  # \nwarmup_iters = 1000  # \n# \ndevice = \"cuda:0\"  # 'cpu''cuda''cuda:0'\ndtype = \"bfloat16\"  # 'float32''bfloat16''float16'\n# -----------------------------------------------------------------------------\n# \nconfig_keys = [\n    k\n    for k, v in globals().items()",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "warmup_iters",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "warmup_iters = 1000  # \n# \ndevice = \"cuda:0\"  # 'cpu''cuda''cuda:0'\ndtype = \"bfloat16\"  # 'float32''bfloat16''float16'\n# -----------------------------------------------------------------------------\n# \nconfig_keys = [\n    k\n    for k, v in globals().items()\n    if not k.startswith(\"_\") and isinstance(v, (int, float, bool, str))",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "device = \"cuda:0\"  # 'cpu''cuda''cuda:0'\ndtype = \"bfloat16\"  # 'float32''bfloat16''float16'\n# -----------------------------------------------------------------------------\n# \nconfig_keys = [\n    k\n    for k, v in globals().items()\n    if not k.startswith(\"_\") and isinstance(v, (int, float, bool, str))\n]\nconfig = {k: globals()[k] for k in config_keys}  # ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "dtype",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "dtype = \"bfloat16\"  # 'float32''bfloat16''float16'\n# -----------------------------------------------------------------------------\n# \nconfig_keys = [\n    k\n    for k, v in globals().items()\n    if not k.startswith(\"_\") and isinstance(v, (int, float, bool, str))\n]\nconfig = {k: globals()[k] for k in config_keys}  # \n# -----------------------------------------------------------------------------",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "config_keys",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "config_keys = [\n    k\n    for k, v in globals().items()\n    if not k.startswith(\"_\") and isinstance(v, (int, float, bool, str))\n]\nconfig = {k: globals()[k] for k in config_keys}  # \n# -----------------------------------------------------------------------------\n# \nlr_decay_iters = max_iters  # \nmin_lr = 0.0  # ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "config = {k: globals()[k] for k in config_keys}  # \n# -----------------------------------------------------------------------------\n# \nlr_decay_iters = max_iters  # \nmin_lr = 0.0  # \nvocab_source = 'custom'  # \nmaster_process = True  # \nseed_offset = 0  # \nddp_world_size = 1  # \ntokens_per_iter = batch_size * max_seq_len  # token",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "lr_decay_iters",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "lr_decay_iters = max_iters  # \nmin_lr = 0.0  # \nvocab_source = 'custom'  # \nmaster_process = True  # \nseed_offset = 0  # \nddp_world_size = 1  # \ntokens_per_iter = batch_size * max_seq_len  # token\n# \ntorch.manual_seed(1337 + seed_offset)\ntorch.backends.cuda.matmul.allow_tf32 = True  # matmultf32",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "min_lr",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "min_lr = 0.0  # \nvocab_source = 'custom'  # \nmaster_process = True  # \nseed_offset = 0  # \nddp_world_size = 1  # \ntokens_per_iter = batch_size * max_seq_len  # token\n# \ntorch.manual_seed(1337 + seed_offset)\ntorch.backends.cuda.matmul.allow_tf32 = True  # matmultf32\ntorch.backends.cudnn.allow_tf32 = True  # cudnntf32",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "vocab_source",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "vocab_source = 'custom'  # \nmaster_process = True  # \nseed_offset = 0  # \nddp_world_size = 1  # \ntokens_per_iter = batch_size * max_seq_len  # token\n# \ntorch.manual_seed(1337 + seed_offset)\ntorch.backends.cuda.matmul.allow_tf32 = True  # matmultf32\ntorch.backends.cudnn.allow_tf32 = True  # cudnntf32\ndevice_type = \"cuda\" if \"cuda\" in device else \"cpu\"  # ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "master_process",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "master_process = True  # \nseed_offset = 0  # \nddp_world_size = 1  # \ntokens_per_iter = batch_size * max_seq_len  # token\n# \ntorch.manual_seed(1337 + seed_offset)\ntorch.backends.cuda.matmul.allow_tf32 = True  # matmultf32\ntorch.backends.cudnn.allow_tf32 = True  # cudnntf32\ndevice_type = \"cuda\" if \"cuda\" in device else \"cpu\"  # \nptdtype = torch.float16  # ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "seed_offset",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "seed_offset = 0  # \nddp_world_size = 1  # \ntokens_per_iter = batch_size * max_seq_len  # token\n# \ntorch.manual_seed(1337 + seed_offset)\ntorch.backends.cuda.matmul.allow_tf32 = True  # matmultf32\ntorch.backends.cudnn.allow_tf32 = True  # cudnntf32\ndevice_type = \"cuda\" if \"cuda\" in device else \"cpu\"  # \nptdtype = torch.float16  # \n# ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "ddp_world_size",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "ddp_world_size = 1  # \ntokens_per_iter = batch_size * max_seq_len  # token\n# \ntorch.manual_seed(1337 + seed_offset)\ntorch.backends.cuda.matmul.allow_tf32 = True  # matmultf32\ntorch.backends.cudnn.allow_tf32 = True  # cudnntf32\ndevice_type = \"cuda\" if \"cuda\" in device else \"cpu\"  # \nptdtype = torch.float16  # \n# \nctx = (",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "tokens_per_iter",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "tokens_per_iter = batch_size * max_seq_len  # token\n# \ntorch.manual_seed(1337 + seed_offset)\ntorch.backends.cuda.matmul.allow_tf32 = True  # matmultf32\ntorch.backends.cudnn.allow_tf32 = True  # cudnntf32\ndevice_type = \"cuda\" if \"cuda\" in device else \"cpu\"  # \nptdtype = torch.float16  # \n# \nctx = (\n    nullcontext()",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "torch.backends.cuda.matmul.allow_tf32",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "torch.backends.cuda.matmul.allow_tf32 = True  # matmultf32\ntorch.backends.cudnn.allow_tf32 = True  # cudnntf32\ndevice_type = \"cuda\" if \"cuda\" in device else \"cpu\"  # \nptdtype = torch.float16  # \n# \nctx = (\n    nullcontext()\n    if device_type == \"cpu\"\n    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n)",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "torch.backends.cudnn.allow_tf32",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "torch.backends.cudnn.allow_tf32 = True  # cudnntf32\ndevice_type = \"cuda\" if \"cuda\" in device else \"cpu\"  # \nptdtype = torch.float16  # \n# \nctx = (\n    nullcontext()\n    if device_type == \"cpu\"\n    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n)\n#  iter_batches",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "device_type",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "device_type = \"cuda\" if \"cuda\" in device else \"cpu\"  # \nptdtype = torch.float16  # \n# \nctx = (\n    nullcontext()\n    if device_type == \"cpu\"\n    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n)\n#  iter_batches\niter_batches = partial(",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "ptdtype",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "ptdtype = torch.float16  # \n# \nctx = (\n    nullcontext()\n    if device_type == \"cpu\"\n    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n)\n#  iter_batches\niter_batches = partial(\n    Task.iter_batches,  #  Task  iter_batches ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "ctx",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "ctx = (\n    nullcontext()\n    if device_type == \"cpu\"\n    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n)\n#  iter_batches\niter_batches = partial(\n    Task.iter_batches,  #  Task  iter_batches \n    batch_size=batch_size,  # \n    max_seq_len=max_seq_len,  # ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "iter_batches",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "iter_batches = partial(\n    Task.iter_batches,  #  Task  iter_batches \n    batch_size=batch_size,  # \n    max_seq_len=max_seq_len,  # \n    vocab_size=vocab_size,  # \n    vocab_source=vocab_source,  #  llama2  custom\n    device=device,  #  GPU  CPU\n    num_workers=0,  #  worker 0 \n)\n# ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "iter_num",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "iter_num = 0  # \n# \nbest_val_loss = 1e9  # \n# \nmodel_args = dict(\n    dim=dim,  # \n    n_layers=n_layers,  # Transformer \n    n_heads=n_heads,  # \n    n_kv_heads=n_kv_heads,  # \n    vocab_size=vocab_size,  # ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "best_val_loss",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "best_val_loss = 1e9  # \n# \nmodel_args = dict(\n    dim=dim,  # \n    n_layers=n_layers,  # Transformer \n    n_heads=n_heads,  # \n    n_kv_heads=n_kv_heads,  # \n    vocab_size=vocab_size,  # \n    multiple_of=multiple_of,  # \n    max_seq_len=max_seq_len,  # ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "model_args",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "model_args = dict(\n    dim=dim,  # \n    n_layers=n_layers,  # Transformer \n    n_heads=n_heads,  # \n    n_kv_heads=n_kv_heads,  # \n    vocab_size=vocab_size,  # \n    multiple_of=multiple_of,  # \n    max_seq_len=max_seq_len,  # \n    dropout=dropout,  # dropout \n)",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "gptconf",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "gptconf = ModelArgs(**model_args)\nmodel = Transformer(gptconf)\nmodel.to(device)\n#  GradScalerAMP\n#  enabled=Falsescaler \nscaler = torch.cuda.amp.GradScaler(enabled=(dtype == \"float16\"))\n#  configure_optimizers \noptimizer = model.configure_optimizers(\n    weight_decay,  # L2 \n    learning_rate,  # ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "model = Transformer(gptconf)\nmodel.to(device)\n#  GradScalerAMP\n#  enabled=Falsescaler \nscaler = torch.cuda.amp.GradScaler(enabled=(dtype == \"float16\"))\n#  configure_optimizers \noptimizer = model.configure_optimizers(\n    weight_decay,  # L2 \n    learning_rate,  # \n    (beta1, beta2),  # Adam  beta1  beta2 ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "scaler",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == \"float16\"))\n#  configure_optimizers \noptimizer = model.configure_optimizers(\n    weight_decay,  # L2 \n    learning_rate,  # \n    (beta1, beta2),  # Adam  beta1  beta2 \n    device_type  #  GPU  CPU\n)\n# \n@torch.no_grad()  #  no_grad ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "optimizer",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "optimizer = model.configure_optimizers(\n    weight_decay,  # L2 \n    learning_rate,  # \n    (beta1, beta2),  # Adam  beta1  beta2 \n    device_type  #  GPU  CPU\n)\n# \n@torch.no_grad()  #  no_grad \ndef estimate_loss():\n    out = {}  # ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "train_batch_iter",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "train_batch_iter = iter_batches(split=\"train\")\nX, Y = next(train_batch_iter)  # \nt0 = time.time()  # \nlocal_iter_num = 0  # \nraw_model = model  #  (DDP)\nrunning_mfu = -1.0  # \nos.makedirs(out_dir, exist_ok=True)\nwhile True:\n    # step\n    lr = get_lr(iter_num) if decay_lr else learning_rate",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "t0",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "t0 = time.time()  # \nlocal_iter_num = 0  # \nraw_model = model  #  (DDP)\nrunning_mfu = -1.0  # \nos.makedirs(out_dir, exist_ok=True)\nwhile True:\n    # step\n    lr = get_lr(iter_num) if decay_lr else learning_rate\n    # \n    for param_group in optimizer.param_groups:",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "local_iter_num",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "local_iter_num = 0  # \nraw_model = model  #  (DDP)\nrunning_mfu = -1.0  # \nos.makedirs(out_dir, exist_ok=True)\nwhile True:\n    # step\n    lr = get_lr(iter_num) if decay_lr else learning_rate\n    # \n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = lr",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "raw_model",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "raw_model = model  #  (DDP)\nrunning_mfu = -1.0  # \nos.makedirs(out_dir, exist_ok=True)\nwhile True:\n    # step\n    lr = get_lr(iter_num) if decay_lr else learning_rate\n    # \n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = lr\n    # ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "running_mfu",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "running_mfu = -1.0  # \nos.makedirs(out_dir, exist_ok=True)\nwhile True:\n    # step\n    lr = get_lr(iter_num) if decay_lr else learning_rate\n    # \n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = lr\n    # \n    if iter_num % eval_interval == 0 and master_process:",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "download_file",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "description": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "peekOfCode": "def download_file(url: str, fname: str, chunk_size=1024):\n    \"\"\"HTTP GET\"\"\"\n    resp = requests.get(url, stream=True)\n    # 0'content-length'\n    total = int(resp.headers.get(\"content-length\", 0))\n    # \n    with open(fname, \"wb\") as file, tqdm(\n        desc=fname,           # \n        total=total,          # \n        unit=\"iB\",            # 'iB'",
        "detail": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "documentation": {}
    },
    {
        "label": "download",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "description": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "peekOfCode": "def download():\n    \"\"\"DATA_CACHE_DIR\"\"\"\n    os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n    # TinyStoriesURL\n    data_url = \"https://www.modelscope.cn/datasets/AI-ModelScope/TinyStories/resolve/master/TinyStories_all_data.tar.gz\"\n    data_filename = os.path.join(DATA_CACHE_DIR, \"TinyStories_all_data.tar.gz\")\n    # \n    if not os.path.exists(data_filename):\n        print(f\"Downloading {data_url} to {data_filename}...\")\n        download_file(data_url, data_filename)  # download_file",
        "detail": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "documentation": {}
    },
    {
        "label": "load_text_from_files",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "description": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "peekOfCode": "def load_text_from_files(path):\n    path_list = glob.glob(path)\n    text_data = []\n    for file_path in path_list:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            text_data.extend(file.readlines())\n    return text_data\ndef batch_iterator(text_data, batch_size=648):\n    for i in range(0, len(text_data), batch_size):\n        yield text_data[i:i + batch_size]",
        "detail": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "documentation": {}
    },
    {
        "label": "batch_iterator",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "description": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "peekOfCode": "def batch_iterator(text_data, batch_size=648):\n    for i in range(0, len(text_data), batch_size):\n        yield text_data[i:i + batch_size]\ndef train_vocab(vocab_size: int=32000, num_shards: int=20):\n    \"\"\"\n    vocab_size: int, \n    num_shards: int, \n    \"\"\"\n    # \n    assert vocab_size > 0, \"Vocab size must be positive\"",
        "detail": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "documentation": {}
    },
    {
        "label": "train_vocab",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "description": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "peekOfCode": "def train_vocab(vocab_size: int=32000, num_shards: int=20):\n    \"\"\"\n    vocab_size: int, \n    num_shards: int, \n    \"\"\"\n    # \n    assert vocab_size > 0, \"Vocab size must be positive\"\n    # SentencePiece \n    prefix = os.path.join(DATA_CACHE_DIR, f\"tok{vocab_size}\")\n    # 1)  tiny.txt",
        "detail": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "documentation": {}
    },
    {
        "label": "DATA_CACHE_DIR",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "description": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "peekOfCode": "DATA_CACHE_DIR = 'data'\ndef download_file(url: str, fname: str, chunk_size=1024):\n    \"\"\"HTTP GET\"\"\"\n    resp = requests.get(url, stream=True)\n    # 0'content-length'\n    total = int(resp.headers.get(\"content-length\", 0))\n    # \n    with open(fname, \"wb\") as file, tqdm(\n        desc=fname,           # \n        total=total,          # ",
        "detail": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "documentation": {}
    },
    {
        "label": "BaseEmbeddings",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "description": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "peekOfCode": "class BaseEmbeddings:\n    \"\"\"\n    Base class for embeddings\n    \"\"\"\n    def __init__(self, path: str, is_api: bool) -> None:\n        self.path = path\n        self.is_api = is_api\n    def get_embedding(self, text: str, model: str) -> List[float]:\n        raise NotImplementedError\n    @classmethod",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "documentation": {}
    },
    {
        "label": "OpenAIEmbedding",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "description": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "peekOfCode": "class OpenAIEmbedding(BaseEmbeddings):\n    \"\"\"\n    class for OpenAI embeddings\n    \"\"\"\n    def __init__(self, path: str = '', is_api: bool = True) -> None:\n        super().__init__(path, is_api)\n        if self.is_api:\n            from openai import OpenAI\n            self.client = OpenAI()\n            self.client.api_key = os.getenv(\"OPENAI_API_KEY\")",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "documentation": {}
    },
    {
        "label": "JinaEmbedding",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "description": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "peekOfCode": "class JinaEmbedding(BaseEmbeddings):\n    \"\"\"\n    class for Jina embeddings\n    \"\"\"\n    def __init__(self, path: str = 'jinaai/jina-embeddings-v2-base-zh', is_api: bool = False) -> None:\n        super().__init__(path, is_api)\n        self._model = self.load_model()\n    def get_embedding(self, text: str) -> List[float]:\n        return self._model.encode([text])[0].tolist()\n    def load_model(self):",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "documentation": {}
    },
    {
        "label": "ZhipuEmbedding",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "description": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "peekOfCode": "class ZhipuEmbedding(BaseEmbeddings):\n    \"\"\"\n    class for Zhipu embeddings\n    \"\"\"\n    def __init__(self, path: str = '', is_api: bool = True) -> None:\n        super().__init__(path, is_api)\n        if self.is_api:\n            from zhipuai import ZhipuAI\n            self.client = ZhipuAI(api_key=os.getenv(\"ZHIPUAI_API_KEY\")) \n    def get_embedding(self, text: str) -> List[float]:",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "documentation": {}
    },
    {
        "label": "DashscopeEmbedding",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "description": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "peekOfCode": "class DashscopeEmbedding(BaseEmbeddings):\n    \"\"\"\n    class for Dashscope embeddings\n    \"\"\"\n    def __init__(self, path: str = '', is_api: bool = True) -> None:\n        super().__init__(path, is_api)\n        if self.is_api:\n            import dashscope\n            dashscope.api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n            self.client = dashscope.TextEmbedding",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "documentation": {}
    },
    {
        "label": "os.environ['CURL_CA_BUNDLE']",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "description": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "peekOfCode": "os.environ['CURL_CA_BUNDLE'] = ''\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv())\nclass BaseEmbeddings:\n    \"\"\"\n    Base class for embeddings\n    \"\"\"\n    def __init__(self, path: str, is_api: bool) -> None:\n        self.path = path\n        self.is_api = is_api",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "documentation": {}
    },
    {
        "label": "_",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "description": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "peekOfCode": "_ = load_dotenv(find_dotenv())\nclass BaseEmbeddings:\n    \"\"\"\n    Base class for embeddings\n    \"\"\"\n    def __init__(self, path: str, is_api: bool) -> None:\n        self.path = path\n        self.is_api = is_api\n    def get_embedding(self, text: str, model: str) -> List[float]:\n        raise NotImplementedError",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "description": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "peekOfCode": "class BaseModel:\n    def __init__(self, path: str = '') -> None:\n        self.path = path\n    def chat(self, prompt: str, history: List[dict], content: str) -> str:\n        pass\n    def load_model(self):\n        pass\nclass OpenAIChat(BaseModel):\n    def __init__(self, path: str = '', model: str = \"gpt-3.5-turbo-1106\") -> None:\n        super().__init__(path)",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "documentation": {}
    },
    {
        "label": "OpenAIChat",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "description": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "peekOfCode": "class OpenAIChat(BaseModel):\n    def __init__(self, path: str = '', model: str = \"gpt-3.5-turbo-1106\") -> None:\n        super().__init__(path)\n        self.model = model\n    def chat(self, prompt: str, history: List[dict], content: str) -> str:\n        from openai import OpenAI\n        client = OpenAI()\n        client.api_key = os.getenv(\"OPENAI_API_KEY\")   \n        client.base_url = os.getenv(\"OPENAI_BASE_URL\")\n        history.append({'role': 'user', 'content': PROMPT_TEMPLATE['RAG_PROMPT_TEMPALTE'].format(question=prompt, context=content)})",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "documentation": {}
    },
    {
        "label": "InternLMChat",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "description": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "peekOfCode": "class InternLMChat(BaseModel):\n    def __init__(self, path: str = '') -> None:\n        super().__init__(path)\n        self.load_model()\n    def chat(self, prompt: str, history: List = [], content: str='') -> str:\n        prompt = PROMPT_TEMPLATE['InternLM_PROMPT_TEMPALTE'].format(question=prompt, context=content)\n        response, history = self.model.chat(self.tokenizer, prompt, history)\n        return response\n    def load_model(self):\n        import torch",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "documentation": {}
    },
    {
        "label": "DashscopeChat",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "description": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "peekOfCode": "class DashscopeChat(BaseModel):\n    def __init__(self, path: str = '', model: str = \"qwen-turbo\") -> None:\n        super().__init__(path)\n        self.model = model\n    def chat(self, prompt: str, history: List[Dict], content: str) -> str:\n        import dashscope\n        dashscope.api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n        history.append({'role': 'user', 'content': PROMPT_TEMPLATE['RAG_PROMPT_TEMPALTE'].format(question=prompt, context=content)})\n        response = dashscope.Generation.call(\n            model=self.model,",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "documentation": {}
    },
    {
        "label": "ZhipuChat",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "description": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "peekOfCode": "class ZhipuChat(BaseModel):\n    def __init__(self, path: str = '', model: str = \"glm-4\") -> None:\n        super().__init__(path)\n        from zhipuai import ZhipuAI\n        self.client = ZhipuAI(api_key=os.getenv(\"ZHIPUAI_API_KEY\"))\n        self.model = model\n    def chat(self, prompt: str, history: List[Dict], content: str) -> str:\n        history.append({'role': 'user', 'content': PROMPT_TEMPLATE['RAG_PROMPT_TEMPALTE'].format(question=prompt, context=content)})\n        response = self.client.chat.completions.create(\n            model=self.model,",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "documentation": {}
    },
    {
        "label": "PROMPT_TEMPLATE",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "description": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "peekOfCode": "PROMPT_TEMPLATE = dict(\n    RAG_PROMPT_TEMPALTE=\"\"\"\n        : {question}\n        \n        \n        {context}\n        \n        \n        :\"\"\",\n    InternLM_PROMPT_TEMPALTE=\"\"\",",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "documentation": {}
    },
    {
        "label": "ReadFiles",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.utils",
        "description": "tiny-universe-main.content.TinyRAG.RAG.utils",
        "peekOfCode": "class ReadFiles:\n    \"\"\"\n    class to read files\n    \"\"\"\n    def __init__(self, path: str) -> None:\n        self._path = path\n        self.file_list = self.get_files()\n    def get_files(self):\n        # argsdir_path\n        file_list = []",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.utils",
        "documentation": {}
    },
    {
        "label": "Documents",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.utils",
        "description": "tiny-universe-main.content.TinyRAG.RAG.utils",
        "peekOfCode": "class Documents:\n    \"\"\"\n        json\n    \"\"\"\n    def __init__(self, path: str = '') -> None:\n        self.path = path\n    def get_content(self):\n        with open(self.path, mode='r', encoding='utf-8') as f:\n            content = json.load(f)\n        return content",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.utils",
        "documentation": {}
    },
    {
        "label": "enc",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.utils",
        "description": "tiny-universe-main.content.TinyRAG.RAG.utils",
        "peekOfCode": "enc = tiktoken.get_encoding(\"cl100k_base\")\nclass ReadFiles:\n    \"\"\"\n    class to read files\n    \"\"\"\n    def __init__(self, path: str) -> None:\n        self._path = path\n        self.file_list = self.get_files()\n    def get_files(self):\n        # argsdir_path",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.utils",
        "documentation": {}
    },
    {
        "label": "VectorStore",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.VectorBase",
        "description": "tiny-universe-main.content.TinyRAG.RAG.VectorBase",
        "peekOfCode": "class VectorStore:\n    def __init__(self, document: List[str] = ['']) -> None:\n        self.document = document\n    def get_vector(self, EmbeddingModel: BaseEmbeddings) -> List[List[float]]:\n        self.vectors = []\n        for doc in tqdm(self.document, desc=\"Calculating embeddings\"):\n            self.vectors.append(EmbeddingModel.get_embedding(doc))\n        return self.vectors\n    def persist(self, path: str = 'storage'):\n        if not os.path.exists(path):",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.VectorBase",
        "documentation": {}
    },
    {
        "label": "model_dir",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyRAG.down_model",
        "description": "tiny-universe-main.content.TinyRAG.down_model",
        "peekOfCode": "model_dir = snapshot_download('Shanghai_AI_Laboratory/internlm2-chat-7b', cache_dir='/root/autodl-tmp/', revision='master')\nmodel_dir = snapshot_download('jinaai/jina-embeddings-v2-base-zh', cache_dir='/root/autodl-tmp/', revision='master')",
        "detail": "tiny-universe-main.content.TinyRAG.down_model",
        "documentation": {}
    },
    {
        "label": "model_dir",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyRAG.down_model",
        "description": "tiny-universe-main.content.TinyRAG.down_model",
        "peekOfCode": "model_dir = snapshot_download('jinaai/jina-embeddings-v2-base-zh', cache_dir='/root/autodl-tmp/', revision='master')",
        "detail": "tiny-universe-main.content.TinyRAG.down_model",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttention",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "description": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "peekOfCode": "class MultiHeadAttention(nn.Module):\n    def __init__(self, config, is_causal=False):\n        # \n        # config: \n        super().__init__()\n        # \n        assert config.n_embd % config.n_head == 0\n        # Wq, Wk, Wv  n_embd x n_embd\n        self.c_attns = nn.ModuleList([nn.Linear(config.n_embd, config.n_embd, bias=config.bias) for _ in range(3)])\n        #  n_embd x n_embd",
        "detail": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "documentation": {}
    },
    {
        "label": "MLP",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "description": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "peekOfCode": "class MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        # Transformer  RELU \n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n        self.relu    = nn.ReLU()\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n        self.dropout = nn.Dropout(config.dropout)\n    def forward(self, x):\n        x = self.c_fc(x)",
        "detail": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "documentation": {}
    },
    {
        "label": "LayerNorm",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "description": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "peekOfCode": "class LayerNorm(nn.Module):\n    #  Pytorch  LayerNorm  Pytorch  LayerNorm  None\n    def __init__(self, ndim, bias):\n        super().__init__()\n        # \n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n    def forward(self, input):\n        #  Pytorch  LayerNorm\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)",
        "detail": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "documentation": {}
    },
    {
        "label": "EncoderLayer",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "description": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "peekOfCode": "class EncoderLayer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        #  Layer  LayerNorm Attention  MLP \n        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n        # Encoder  is_causal=False\n        self.attn = MultiHeadAttention(config, is_causal=False)\n        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n        self.mlp = MLP(config)\n    def forward(self, x):",
        "detail": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "description": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "peekOfCode": "class Encoder(nn.Module):\n    def __init__(self, config):\n        super(Encoder, self).__init__() \n        #  Encoder  N  Encoder Layer \n        self.layers = nn.ModuleList([EncoderLayer(config) for _ in range(config.n_layer)])\n        self.norm = LayerNorm(config.n_embd, bias=config.bias)\n    def forward(self, x):\n        \" N  Encoder Layer\"\n        for layer in self.layers:\n            x = layer(x)",
        "detail": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "documentation": {}
    },
    {
        "label": "DecoderLayer",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "description": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "peekOfCode": "class DecoderLayer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        #  Layer  LayerNorm Mask Attention Self Attention  MLP \n        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n        # Decoder  Mask Attention is_causal=True\n        self.m_attn = MultiHeadAttention(config, is_causal=True)\n        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n        # Decoder   Encoder  Attention is_causal=False\n        self.attn = MultiHeadAttention(config, is_causal=False)",
        "detail": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "description": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "peekOfCode": "class Decoder(nn.Module):\n    def __init__(self, config):\n        super(Decoder, self).__init__() \n        #  Decoder  N  Decoder Layer \n        self.layers = nn.ModuleList([DecoderLayer(config) for _ in range(config.n_layer)])\n        self.norm = LayerNorm(config.n_embd, bias=config.bias)\n    def forward(self, x, enc_out):\n        \"Pass the input (and mask) through each layer in turn.\"\n        for layer in self.layers:\n            x = layer(x, enc_out)",
        "detail": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "documentation": {}
    },
    {
        "label": "PositionalEncoding",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "description": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "peekOfCode": "class PositionalEncoding(nn.Module):\n    # \n    def __init__(self, config):\n        super(PositionalEncoding, self).__init__()\n        # Dropout \n        self.dropout = nn.Dropout(p=config.dropout)\n        # block size \n        pe = torch.zeros(config.block_size, config.n_embd)\n        position = torch.arange(0, config.block_size).unsqueeze(1)\n        div_term = torch.exp(",
        "detail": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "description": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "peekOfCode": "class Transformer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        #  block size\n        assert config.vocab_size is not None\n        assert config.block_size is not None\n        self.config = config\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = PositionalEncoding(config),",
        "detail": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "documentation": {}
    },
    {
        "label": "attention",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "description": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "peekOfCode": "def attention(q, k, v, dropout_module = None, is_causal=False, dropout=None, mask=None):\n    #  QK^T / sqrt(d_k) (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n    #  Casual LM mask \n    if is_causal:\n        #  block_size \n        att = att.masked_fill(mask[:,:,:k.size(-2),:k.size(-2)] == 0, float('-inf'))\n    #  softmax (B, nh, T, T)\n    att = F.softmax(att, dim=-1)\n    # Attention Dropout",
        "detail": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "documentation": {}
    }
]