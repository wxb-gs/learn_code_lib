[
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "OllamaLLM",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "ChatOllama",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "AIMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "AIMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "BaseMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "SystemMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "trim_messages",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain_core.messages",
        "description": "langchain_core.messages",
        "isExtraImport": true,
        "detail": "langchain_core.messages",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "streamlit",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "streamlit",
        "description": "streamlit",
        "detail": "streamlit",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "sleep",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "sleep",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain.schema.messages",
        "description": "langchain.schema.messages",
        "isExtraImport": true,
        "detail": "langchain.schema.messages",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain.schema.messages",
        "description": "langchain.schema.messages",
        "isExtraImport": true,
        "detail": "langchain.schema.messages",
        "documentation": {}
    },
    {
        "label": "HumanMessage",
        "importPath": "langchain.schema.messages",
        "description": "langchain.schema.messages",
        "isExtraImport": true,
        "detail": "langchain.schema.messages",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "QApplication",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QDialog",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QVBoxLayout",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QHBoxLayout",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFormLayout",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLabel",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLineEdit",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSpinBox",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QDoubleSpinBox",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QCheckBox",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QComboBox",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QPushButton",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QGroupBox",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFrame",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QTextEdit",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSlider",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QProgressBar",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QApplication",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QDialog",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QVBoxLayout",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QHBoxLayout",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFormLayout",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLabel",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLineEdit",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QCheckBox",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QPushButton",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QGroupBox",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QListWidget",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QListWidgetItem",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMessageBox",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFrame",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSizePolicy",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QApplication",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMainWindow",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QDialog",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QVBoxLayout",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QHBoxLayout",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QPushButton",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLineEdit",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QScrollArea",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QWidget",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLabel",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFrame",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSpacerItem",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSizePolicy",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QApplication",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QWidget",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QVBoxLayout",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QTextEdit",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QPushButton",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QApplication",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QWidget",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QVBoxLayout",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QTextEdit",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QPushButton",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QApplication",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QWidget",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QVBoxLayout",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QHBoxLayout",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLabel",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QProgressBar",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMainWindow",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QPushButton",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QTextEdit",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMessageBox",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QAction",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QApplication",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QFrame",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "# 你现有的导入\r\n                             QHBoxLayout",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QLabel",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QListWidget",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QListWidgetItem",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMainWindow",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QMenu",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QPushButton",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QScrollArea",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSizePolicy",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSplitter",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QTextEdit",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QVBoxLayout",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QWidget",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QDialog",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSpacerItem",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSizePolicy",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QWidget",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QApplication",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSizePolicy",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QDesktopWidget",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QGraphicsDropShadowEffect",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "QSpacerItem",
        "importPath": "PyQt5.QtWidgets",
        "description": "PyQt5.QtWidgets",
        "isExtraImport": true,
        "detail": "PyQt5.QtWidgets",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "QTimer",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "pyqtSignal",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "QTimer",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "pyqtSignal",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "QThread",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "QObject",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "pyqtSignal",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "QObject",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "pyqtSignal",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "QThread",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "QTimer",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "QThread",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "pyqtSignal",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "QPropertyAnimation",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "QEasingCurve",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "QSize",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "QTimer",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "pyqtSignal",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "Qt",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "pyqtSignal",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "QThread",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "QBasicTimer",
        "importPath": "PyQt5.QtCore",
        "description": "PyQt5.QtCore",
        "isExtraImport": true,
        "detail": "PyQt5.QtCore",
        "documentation": {}
    },
    {
        "label": "QFont",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QPalette",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QColor",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QIcon",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QFont",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QIcon",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QFont",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QPalette",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QColor",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QIcon",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QTextCursor",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QFont",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QPixmap",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QPainter",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QBrush",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QColor",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QPalette",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QBrush",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QColor",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QFont",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QIcon",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QPainter",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QPixmap",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QTextCursor",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QPixmap",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "QColor",
        "importPath": "PyQt5.QtGui",
        "description": "PyQt5.QtGui",
        "isExtraImport": true,
        "detail": "PyQt5.QtGui",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "sqlite3",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sqlite3",
        "description": "sqlite3",
        "detail": "sqlite3",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "uuid",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uuid",
        "description": "uuid",
        "detail": "uuid",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "ConversationDatabase",
        "importPath": "database",
        "description": "database",
        "isExtraImport": true,
        "detail": "database",
        "documentation": {}
    },
    {
        "label": "mistune",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "mistune",
        "description": "mistune",
        "detail": "mistune",
        "documentation": {}
    },
    {
        "label": "ChatInterface",
        "importPath": "main_ui",
        "description": "main_ui",
        "isExtraImport": true,
        "detail": "main_ui",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "ChatBot",
        "importPath": "chatbot",
        "description": "chatbot",
        "isExtraImport": true,
        "detail": "chatbot",
        "documentation": {}
    },
    {
        "label": "ChatBotThread",
        "importPath": "chatbot",
        "description": "chatbot",
        "isExtraImport": true,
        "detail": "chatbot",
        "documentation": {}
    },
    {
        "label": "scrollbar_style",
        "importPath": "styles.qeditor",
        "description": "styles.qeditor",
        "isExtraImport": true,
        "detail": "styles.qeditor",
        "documentation": {}
    },
    {
        "label": "qeditor_qss",
        "importPath": "styles.qeditor",
        "description": "styles.qeditor",
        "isExtraImport": true,
        "detail": "styles.qeditor",
        "documentation": {}
    },
    {
        "label": "blue_btn_qss",
        "importPath": "styles.btn_qss",
        "description": "styles.btn_qss",
        "isExtraImport": true,
        "detail": "styles.btn_qss",
        "documentation": {}
    },
    {
        "label": "simple_btn_qss",
        "importPath": "styles.btn_qss",
        "description": "styles.btn_qss",
        "isExtraImport": true,
        "detail": "styles.btn_qss",
        "documentation": {}
    },
    {
        "label": "delete_btn_qss",
        "importPath": "styles.btn_qss",
        "description": "styles.btn_qss",
        "isExtraImport": true,
        "detail": "styles.btn_qss",
        "documentation": {}
    },
    {
        "label": "SearchDialog",
        "importPath": "components.search_dialog",
        "description": "components.search_dialog",
        "isExtraImport": true,
        "detail": "components.search_dialog",
        "documentation": {}
    },
    {
        "label": "ConversationEditDialog",
        "importPath": "components.params_dialog",
        "description": "components.params_dialog",
        "isExtraImport": true,
        "detail": "components.params_dialog",
        "documentation": {}
    },
    {
        "label": "ConversationDatabase",
        "importPath": "db.database",
        "description": "db.database",
        "isExtraImport": true,
        "detail": "db.database",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "pyaudio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pyaudio",
        "description": "pyaudio",
        "detail": "pyaudio",
        "documentation": {}
    },
    {
        "label": "wave",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "wave",
        "description": "wave",
        "detail": "wave",
        "documentation": {}
    },
    {
        "label": "WhisperModel",
        "importPath": "faster_whisper",
        "description": "faster_whisper",
        "isExtraImport": true,
        "detail": "faster_whisper",
        "documentation": {}
    },
    {
        "label": "StateGraph",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "END",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "START",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "MessagesState",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "StateGraph",
        "importPath": "langgraph.graph",
        "description": "langgraph.graph",
        "isExtraImport": true,
        "detail": "langgraph.graph",
        "documentation": {}
    },
    {
        "label": "asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio",
        "description": "asyncio",
        "detail": "asyncio",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "MessagesPlaceholder",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "MessagesPlaceholder",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "StrOutputParser",
        "importPath": "langchain_core.output_parsers",
        "description": "langchain_core.output_parsers",
        "isExtraImport": true,
        "detail": "langchain_core.output_parsers",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "ChatOpenAI",
        "importPath": "langchain_openai",
        "description": "langchain_openai",
        "isExtraImport": true,
        "detail": "langchain_openai",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "importPath": "deepeval",
        "description": "deepeval",
        "isExtraImport": true,
        "detail": "deepeval",
        "documentation": {}
    },
    {
        "label": "assert_test",
        "importPath": "deepeval",
        "description": "deepeval",
        "isExtraImport": true,
        "detail": "deepeval",
        "documentation": {}
    },
    {
        "label": "LLMTestCase",
        "importPath": "deepeval.test_case",
        "description": "deepeval.test_case",
        "isExtraImport": true,
        "detail": "deepeval.test_case",
        "documentation": {}
    },
    {
        "label": "LLMTestCase",
        "importPath": "deepeval.test_case",
        "description": "deepeval.test_case",
        "isExtraImport": true,
        "detail": "deepeval.test_case",
        "documentation": {}
    },
    {
        "label": "AnswerRelevancyMetric",
        "importPath": "deepeval.metrics",
        "description": "deepeval.metrics",
        "isExtraImport": true,
        "detail": "deepeval.metrics",
        "documentation": {}
    },
    {
        "label": "HallucinationMetric",
        "importPath": "deepeval.metrics",
        "description": "deepeval.metrics",
        "isExtraImport": true,
        "detail": "deepeval.metrics",
        "documentation": {}
    },
    {
        "label": "AnswerRelevancyMetric",
        "importPath": "deepeval.metrics",
        "description": "deepeval.metrics",
        "isExtraImport": true,
        "detail": "deepeval.metrics",
        "documentation": {}
    },
    {
        "label": "DeepEvalCallbackHandler",
        "importPath": "langchain_community.callbacks.confident_callback",
        "description": "langchain_community.callbacks.confident_callback",
        "isExtraImport": true,
        "detail": "langchain_community.callbacks.confident_callback",
        "documentation": {}
    },
    {
        "label": "OllamaModel",
        "importPath": "deepeval.models",
        "description": "deepeval.models",
        "isExtraImport": true,
        "detail": "deepeval.models",
        "documentation": {}
    },
    {
        "label": "app",
        "importPath": "langgraph_use",
        "description": "langgraph_use",
        "isExtraImport": true,
        "detail": "langgraph_use",
        "documentation": {}
    },
    {
        "label": "tool",
        "importPath": "langchain_core.tools",
        "description": "langchain_core.tools",
        "isExtraImport": true,
        "detail": "langchain_core.tools",
        "documentation": {}
    },
    {
        "label": "tool",
        "importPath": "langchain_core.tools",
        "description": "langchain_core.tools",
        "isExtraImport": true,
        "detail": "langchain_core.tools",
        "documentation": {}
    },
    {
        "label": "MemorySaver",
        "importPath": "langgraph.checkpoint.memory",
        "description": "langgraph.checkpoint.memory",
        "isExtraImport": true,
        "detail": "langgraph.checkpoint.memory",
        "documentation": {}
    },
    {
        "label": "MemorySaver",
        "importPath": "langgraph.checkpoint.memory",
        "description": "langgraph.checkpoint.memory",
        "isExtraImport": true,
        "detail": "langgraph.checkpoint.memory",
        "documentation": {}
    },
    {
        "label": "create_react_agent",
        "importPath": "langgraph.prebuilt",
        "description": "langgraph.prebuilt",
        "isExtraImport": true,
        "detail": "langgraph.prebuilt",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "IPython.display",
        "description": "IPython.display",
        "isExtraImport": true,
        "detail": "IPython.display",
        "documentation": {}
    },
    {
        "label": "display",
        "importPath": "IPython.display",
        "description": "IPython.display",
        "isExtraImport": true,
        "detail": "IPython.display",
        "documentation": {}
    },
    {
        "label": "Memory",
        "importPath": "mem0",
        "description": "mem0",
        "isExtraImport": true,
        "detail": "mem0",
        "documentation": {}
    },
    {
        "label": "PromptTemplate",
        "importPath": "langchain.prompts",
        "description": "langchain.prompts",
        "isExtraImport": true,
        "detail": "langchain.prompts",
        "documentation": {}
    },
    {
        "label": "TextLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "TextLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "TextLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "TextLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "PyPDFLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "load_summarize_chain",
        "importPath": "langchain.chains.summarize",
        "description": "langchain.chains.summarize",
        "isExtraImport": true,
        "detail": "langchain.chains.summarize",
        "documentation": {}
    },
    {
        "label": "load_summarize_chain",
        "importPath": "langchain.chains.summarize",
        "description": "langchain.chains.summarize",
        "isExtraImport": true,
        "detail": "langchain.chains.summarize",
        "documentation": {}
    },
    {
        "label": "load_summarize_chain",
        "importPath": "langchain.chains.summarize",
        "description": "langchain.chains.summarize",
        "isExtraImport": true,
        "detail": "langchain.chains.summarize",
        "documentation": {}
    },
    {
        "label": "RecursiveCharacterTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "RecursiveCharacterTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "RecursiveCharacterTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "TokenTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "RecursiveCharacterTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "PythonREPL",
        "importPath": "langchain_experimental.utilities",
        "description": "langchain_experimental.utilities",
        "isExtraImport": true,
        "detail": "langchain_experimental.utilities",
        "documentation": {}
    },
    {
        "label": "RunnableLambda",
        "importPath": "langchain_core.runnables",
        "description": "langchain_core.runnables",
        "isExtraImport": true,
        "detail": "langchain_core.runnables",
        "documentation": {}
    },
    {
        "label": "RunnableConfig",
        "importPath": "langchain_core.runnables",
        "description": "langchain_core.runnables",
        "isExtraImport": true,
        "detail": "langchain_core.runnables",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "huggingface_hub",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "huggingface_hub",
        "description": "huggingface_hub",
        "detail": "huggingface_hub",
        "documentation": {}
    },
    {
        "label": "login",
        "importPath": "huggingface_hub",
        "description": "huggingface_hub",
        "isExtraImport": true,
        "detail": "huggingface_hub",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "getModel",
        "importPath": "chatbot.core.load_model.get_model",
        "description": "chatbot.core.load_model.get_model",
        "isExtraImport": true,
        "detail": "chatbot.core.load_model.get_model",
        "documentation": {}
    },
    {
        "label": "getModel",
        "importPath": "chatbot.core.load_model.get_model",
        "description": "chatbot.core.load_model.get_model",
        "isExtraImport": true,
        "detail": "chatbot.core.load_model.get_model",
        "documentation": {}
    },
    {
        "label": "militory",
        "importPath": "chatbot.utils.prompts",
        "description": "chatbot.utils.prompts",
        "isExtraImport": true,
        "detail": "chatbot.utils.prompts",
        "documentation": {}
    },
    {
        "label": "getBg",
        "importPath": "chatbot.bg.bg",
        "description": "chatbot.bg.bg",
        "isExtraImport": true,
        "detail": "chatbot.bg.bg",
        "documentation": {}
    },
    {
        "label": "fg",
        "importPath": "chatbot.fg.fg",
        "description": "chatbot.fg.fg",
        "isExtraImport": true,
        "detail": "chatbot.fg.fg",
        "documentation": {}
    },
    {
        "label": "get_model",
        "importPath": "chatbot.core.load_model",
        "description": "chatbot.core.load_model",
        "isExtraImport": true,
        "detail": "chatbot.core.load_model",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "QtCore",
        "importPath": "PyQt5",
        "description": "PyQt5",
        "isExtraImport": true,
        "detail": "PyQt5",
        "documentation": {}
    },
    {
        "label": "QtGui",
        "importPath": "PyQt5",
        "description": "PyQt5",
        "isExtraImport": true,
        "detail": "PyQt5",
        "documentation": {}
    },
    {
        "label": "QtWidgets",
        "importPath": "PyQt5",
        "description": "PyQt5",
        "isExtraImport": true,
        "detail": "PyQt5",
        "documentation": {}
    },
    {
        "label": "QtCore",
        "importPath": "PyQt5",
        "description": "PyQt5",
        "isExtraImport": true,
        "detail": "PyQt5",
        "documentation": {}
    },
    {
        "label": "QtGui",
        "importPath": "PyQt5",
        "description": "PyQt5",
        "isExtraImport": true,
        "detail": "PyQt5",
        "documentation": {}
    },
    {
        "label": "QtWidgets",
        "importPath": "PyQt5",
        "description": "PyQt5",
        "isExtraImport": true,
        "detail": "PyQt5",
        "documentation": {}
    },
    {
        "label": "Ui_loadwin",
        "importPath": "loadwin_ui",
        "description": "loadwin_ui",
        "isExtraImport": true,
        "detail": "loadwin_ui",
        "documentation": {}
    },
    {
        "label": "Ui_mainwin",
        "importPath": "Ui_mainwin",
        "description": "Ui_mainwin",
        "isExtraImport": true,
        "detail": "Ui_mainwin",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "find_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "pytest",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pytest",
        "description": "pytest",
        "detail": "pytest",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "EvaluationDataset",
        "importPath": "deepeval.dataset",
        "description": "deepeval.dataset",
        "isExtraImport": true,
        "detail": "deepeval.dataset",
        "documentation": {}
    },
    {
        "label": "EvaluationDataset",
        "importPath": "deepeval.dataset",
        "description": "deepeval.dataset",
        "isExtraImport": true,
        "detail": "deepeval.dataset",
        "documentation": {}
    },
    {
        "label": "Golden",
        "importPath": "deepeval.dataset",
        "description": "deepeval.dataset",
        "isExtraImport": true,
        "detail": "deepeval.dataset",
        "documentation": {}
    },
    {
        "label": "StorageContext",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "load_index_from_storage",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "VectorStoreIndex",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "SimpleDirectoryReader",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "Settings",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "SimpleDirectoryReader",
        "importPath": "llama_index.core",
        "description": "llama_index.core",
        "isExtraImport": true,
        "detail": "llama_index.core",
        "documentation": {}
    },
    {
        "label": "OllamaModel",
        "importPath": "deepeval.models.llms.ollama_model",
        "description": "deepeval.models.llms.ollama_model",
        "isExtraImport": true,
        "detail": "deepeval.models.llms.ollama_model",
        "documentation": {}
    },
    {
        "label": "Ollama",
        "importPath": "llama_index.llms.ollama",
        "description": "llama_index.llms.ollama",
        "isExtraImport": true,
        "detail": "llama_index.llms.ollama",
        "documentation": {}
    },
    {
        "label": "Ollama",
        "importPath": "llama_index.llms.ollama",
        "description": "llama_index.llms.ollama",
        "isExtraImport": true,
        "detail": "llama_index.llms.ollama",
        "documentation": {}
    },
    {
        "label": "OllamaEmbedding",
        "importPath": "llama_index.embeddings.ollama",
        "description": "llama_index.embeddings.ollama",
        "isExtraImport": true,
        "detail": "llama_index.embeddings.ollama",
        "documentation": {}
    },
    {
        "label": "TitleExtractor",
        "importPath": "llama_index.core.extractors",
        "description": "llama_index.core.extractors",
        "isExtraImport": true,
        "detail": "llama_index.core.extractors",
        "documentation": {}
    },
    {
        "label": "SummaryExtractor",
        "importPath": "llama_index.core.extractors",
        "description": "llama_index.core.extractors",
        "isExtraImport": true,
        "detail": "llama_index.core.extractors",
        "documentation": {}
    },
    {
        "label": "TokenTextSplitter",
        "importPath": "llama_index.core.node_parser",
        "description": "llama_index.core.node_parser",
        "isExtraImport": true,
        "detail": "llama_index.core.node_parser",
        "documentation": {}
    },
    {
        "label": "IngestionPipeline",
        "importPath": "llama_index.core.ingestion",
        "description": "llama_index.core.ingestion",
        "isExtraImport": true,
        "detail": "llama_index.core.ingestion",
        "documentation": {}
    },
    {
        "label": "ChatMessage",
        "importPath": "llama_index.core.llms",
        "description": "llama_index.core.llms",
        "isExtraImport": true,
        "detail": "llama_index.core.llms",
        "documentation": {}
    },
    {
        "label": "pprint",
        "importPath": "pprint",
        "description": "pprint",
        "isExtraImport": true,
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "ImageReader",
        "importPath": "llama_index.readers.file",
        "description": "llama_index.readers.file",
        "isExtraImport": true,
        "detail": "llama_index.readers.file",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain_core.documents",
        "description": "langchain_core.documents",
        "isExtraImport": true,
        "detail": "langchain_core.documents",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "load_pdf_documents",
        "importPath": "load_file",
        "description": "load_file",
        "isExtraImport": true,
        "detail": "load_file",
        "documentation": {}
    },
    {
        "label": "textwrap",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "textwrap",
        "description": "textwrap",
        "detail": "textwrap",
        "documentation": {}
    },
    {
        "label": "gradio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gradio",
        "description": "gradio",
        "detail": "gradio",
        "documentation": {}
    },
    {
        "label": "ChatMessageHistory",
        "importPath": "langchain_community.chat_message_histories",
        "description": "langchain_community.chat_message_histories",
        "isExtraImport": true,
        "detail": "langchain_community.chat_message_histories",
        "documentation": {}
    },
    {
        "label": "ChatMessageHistory",
        "importPath": "langchain_community.chat_message_histories",
        "description": "langchain_community.chat_message_histories",
        "isExtraImport": true,
        "detail": "langchain_community.chat_message_histories",
        "documentation": {}
    },
    {
        "label": "RunnableWithMessageHistory",
        "importPath": "langchain_core.runnables.history",
        "description": "langchain_core.runnables.history",
        "isExtraImport": true,
        "detail": "langchain_core.runnables.history",
        "documentation": {}
    },
    {
        "label": "RunnableWithMessageHistory",
        "importPath": "langchain_core.runnables.history",
        "description": "langchain_core.runnables.history",
        "isExtraImport": true,
        "detail": "langchain_core.runnables.history",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "ast",
        "description": "ast",
        "isExtraImport": true,
        "detail": "ast",
        "documentation": {}
    },
    {
        "label": "json5",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json5",
        "description": "json5",
        "detail": "json5",
        "documentation": {}
    },
    {
        "label": "InternLM2Chat",
        "importPath": "tinyAgent.LLM",
        "description": "tinyAgent.LLM",
        "isExtraImport": true,
        "detail": "tinyAgent.LLM",
        "documentation": {}
    },
    {
        "label": "Tools",
        "importPath": "tinyAgent.tool",
        "description": "tinyAgent.tool",
        "isExtraImport": true,
        "detail": "tinyAgent.tool",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "LlamaTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "LlamaForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "LlamaTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "LlamaForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "CLIPVisionModelWithProjection",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "os,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.",
        "description": "os.",
        "detail": "os.",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "torchvision",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision",
        "description": "torchvision",
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "TensorDataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "torchvision.models",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.models",
        "description": "torchvision.models",
        "detail": "torchvision.models",
        "documentation": {}
    },
    {
        "label": "linalg",
        "importPath": "scipy",
        "description": "scipy",
        "isExtraImport": true,
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "SimpleUnet",
        "importPath": "unet",
        "description": "unet",
        "isExtraImport": true,
        "detail": "unet",
        "documentation": {}
    },
    {
        "label": "SimpleUnet",
        "importPath": "unet",
        "description": "unet",
        "isExtraImport": true,
        "detail": "unet",
        "documentation": {}
    },
    {
        "label": "NoiseScheduler",
        "importPath": "diffusion",
        "description": "diffusion",
        "isExtraImport": true,
        "detail": "diffusion",
        "documentation": {}
    },
    {
        "label": "NoiseScheduler",
        "importPath": "diffusion",
        "description": "diffusion",
        "isExtraImport": true,
        "detail": "diffusion",
        "documentation": {}
    },
    {
        "label": "torch.optim",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.optim",
        "description": "torch.optim",
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "load_transformed_dataset",
        "importPath": "dataloader",
        "description": "dataloader",
        "isExtraImport": true,
        "detail": "dataloader",
        "documentation": {}
    },
    {
        "label": "sample",
        "importPath": "sample",
        "description": "sample",
        "isExtraImport": true,
        "detail": "sample",
        "documentation": {}
    },
    {
        "label": "plot",
        "importPath": "sample",
        "description": "sample",
        "isExtraImport": true,
        "detail": "sample",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "PeftModel",
        "importPath": "peft",
        "description": "peft",
        "isExtraImport": true,
        "detail": "peft",
        "documentation": {}
    },
    {
        "label": "string",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "string",
        "description": "string",
        "detail": "string",
        "documentation": {}
    },
    {
        "label": "jieba",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "jieba",
        "description": "jieba",
        "detail": "jieba",
        "documentation": {}
    },
    {
        "label": "Rouge",
        "importPath": "rouge",
        "description": "rouge",
        "isExtraImport": true,
        "detail": "rouge",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "qa_f1_score",
        "importPath": "Eval.metrics",
        "description": "Eval.metrics",
        "isExtraImport": true,
        "detail": "Eval.metrics",
        "documentation": {}
    },
    {
        "label": "qa_f1_zh_score",
        "importPath": "Eval.metrics",
        "description": "Eval.metrics",
        "isExtraImport": true,
        "detail": "Eval.metrics",
        "documentation": {}
    },
    {
        "label": "rouge_score",
        "importPath": "Eval.metrics",
        "description": "Eval.metrics",
        "isExtraImport": true,
        "detail": "Eval.metrics",
        "documentation": {}
    },
    {
        "label": "classification_score",
        "importPath": "Eval.metrics",
        "description": "Eval.metrics",
        "isExtraImport": true,
        "detail": "Eval.metrics",
        "documentation": {}
    },
    {
        "label": "rouge_zh_score",
        "importPath": "Eval.metrics",
        "description": "Eval.metrics",
        "isExtraImport": true,
        "detail": "Eval.metrics",
        "documentation": {}
    },
    {
        "label": "GAOKAO_math",
        "importPath": "Eval.metrics",
        "description": "Eval.metrics",
        "isExtraImport": true,
        "detail": "Eval.metrics",
        "documentation": {}
    },
    {
        "label": "internlm2Chat",
        "importPath": "Eval.model.LLM",
        "description": "Eval.model.LLM",
        "isExtraImport": true,
        "detail": "Eval.model.LLM",
        "documentation": {}
    },
    {
        "label": "Qwen2Chat",
        "importPath": "Eval.model.LLM",
        "description": "Eval.model.LLM",
        "isExtraImport": true,
        "detail": "Eval.model.LLM",
        "documentation": {}
    },
    {
        "label": "base64",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "base64",
        "description": "base64",
        "detail": "base64",
        "documentation": {}
    },
    {
        "label": "modelscope",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "modelscope",
        "description": "modelscope",
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "Qwen2_5_VLForConditionalGeneration",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoProcessor",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "snapshot_download",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoModel",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "modelscope",
        "description": "modelscope",
        "isExtraImport": true,
        "detail": "modelscope",
        "documentation": {}
    },
    {
        "label": "process_vision_info",
        "importPath": "qwen_vl_utils",
        "description": "qwen_vl_utils",
        "isExtraImport": true,
        "detail": "qwen_vl_utils",
        "documentation": {}
    },
    {
        "label": "AutoPipelineForText2Image",
        "importPath": "diffusers",
        "description": "diffusers",
        "isExtraImport": true,
        "detail": "diffusers",
        "documentation": {}
    },
    {
        "label": "clip",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "clip",
        "description": "clip",
        "detail": "clip",
        "documentation": {}
    },
    {
        "label": "hashlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hashlib",
        "description": "hashlib",
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "urllib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "urllib",
        "description": "urllib",
        "detail": "urllib",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "version",
        "importPath": "packaging",
        "description": "packaging",
        "isExtraImport": true,
        "detail": "packaging",
        "documentation": {}
    },
    {
        "label": "Compose",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Resize",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "CenterCrop",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "ToTensor",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Normalize",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "gzip",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gzip",
        "description": "gzip",
        "detail": "gzip",
        "documentation": {}
    },
    {
        "label": "html",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "html",
        "description": "html",
        "detail": "html",
        "documentation": {}
    },
    {
        "label": "lru_cache",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "ftfy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ftfy",
        "description": "ftfy",
        "detail": "ftfy",
        "documentation": {}
    },
    {
        "label": "regex",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "regex",
        "description": "regex",
        "detail": "regex",
        "documentation": {}
    },
    {
        "label": "tokenize",
        "importPath": "clip.clip",
        "description": "clip.clip",
        "isExtraImport": true,
        "detail": "clip.clip",
        "documentation": {}
    },
    {
        "label": "load",
        "importPath": "clip.clip",
        "description": "clip.clip",
        "isExtraImport": true,
        "detail": "clip.clip",
        "documentation": {}
    },
    {
        "label": "available_models",
        "importPath": "clip.clip",
        "description": "clip.clip",
        "isExtraImport": true,
        "detail": "clip.clip",
        "documentation": {}
    },
    {
        "label": "pkg_resources",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pkg_resources",
        "description": "pkg_resources",
        "detail": "pkg_resources",
        "documentation": {}
    },
    {
        "label": "SDXLGenerator",
        "importPath": "IMGRAG.ImgGenerator",
        "description": "IMGRAG.ImgGenerator",
        "isExtraImport": true,
        "detail": "IMGRAG.ImgGenerator",
        "documentation": {}
    },
    {
        "label": "load_qwen_vlm",
        "importPath": "IMGRAG.ImgEvaluator",
        "description": "IMGRAG.ImgEvaluator",
        "isExtraImport": true,
        "detail": "IMGRAG.ImgEvaluator",
        "documentation": {}
    },
    {
        "label": "run_qwen_vl",
        "importPath": "IMGRAG.ImgEvaluator",
        "description": "IMGRAG.ImgEvaluator",
        "isExtraImport": true,
        "detail": "IMGRAG.ImgEvaluator",
        "documentation": {}
    },
    {
        "label": "load_qwen_llm",
        "importPath": "IMGRAG.RewritePrompt",
        "description": "IMGRAG.RewritePrompt",
        "isExtraImport": true,
        "detail": "IMGRAG.RewritePrompt",
        "documentation": {}
    },
    {
        "label": "run_qwen_llm",
        "importPath": "IMGRAG.RewritePrompt",
        "description": "IMGRAG.RewritePrompt",
        "isExtraImport": true,
        "detail": "IMGRAG.RewritePrompt",
        "documentation": {}
    },
    {
        "label": "get_clip_similarities",
        "importPath": "IMGRAG.ImgRetrieval",
        "description": "IMGRAG.ImgRetrieval",
        "isExtraImport": true,
        "detail": "IMGRAG.ImgRetrieval",
        "documentation": {}
    },
    {
        "label": "struct",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "struct",
        "description": "struct",
        "detail": "struct",
        "documentation": {}
    },
    {
        "label": "inspect",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "inspect",
        "description": "inspect",
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "ProcessPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "sentencepiece",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sentencepiece",
        "description": "sentencepiece",
        "detail": "sentencepiece",
        "documentation": {}
    },
    {
        "label": "SentencePieceProcessor",
        "importPath": "sentencepiece",
        "description": "sentencepiece",
        "isExtraImport": true,
        "detail": "sentencepiece",
        "documentation": {}
    },
    {
        "label": "torch.distributed",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.distributed",
        "description": "torch.distributed",
        "detail": "torch.distributed",
        "documentation": {}
    },
    {
        "label": "Tokenizer",
        "importPath": "tokenizer",
        "description": "tokenizer",
        "isExtraImport": true,
        "detail": "tokenizer",
        "documentation": {}
    },
    {
        "label": "Tokenizer",
        "importPath": "tokenizer",
        "description": "tokenizer",
        "isExtraImport": true,
        "detail": "tokenizer",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "nullcontext",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "nullcontext",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "ModelArgs",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "ModelArgs",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "Task",
        "importPath": "preprocess",
        "description": "preprocess",
        "isExtraImport": true,
        "detail": "preprocess",
        "documentation": {}
    },
    {
        "label": "copy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "PyPDF2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "PyPDF2",
        "description": "PyPDF2",
        "detail": "PyPDF2",
        "documentation": {}
    },
    {
        "label": "markdown",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "markdown",
        "description": "markdown",
        "detail": "markdown",
        "documentation": {}
    },
    {
        "label": "html2text",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "html2text",
        "description": "html2text",
        "detail": "html2text",
        "documentation": {}
    },
    {
        "label": "tiktoken",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tiktoken",
        "description": "tiktoken",
        "detail": "tiktoken",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "BaseEmbeddings",
        "importPath": "RAG.Embeddings",
        "description": "RAG.Embeddings",
        "isExtraImport": true,
        "detail": "RAG.Embeddings",
        "documentation": {}
    },
    {
        "label": "OpenAIEmbedding",
        "importPath": "RAG.Embeddings",
        "description": "RAG.Embeddings",
        "isExtraImport": true,
        "detail": "RAG.Embeddings",
        "documentation": {}
    },
    {
        "label": "JinaEmbedding",
        "importPath": "RAG.Embeddings",
        "description": "RAG.Embeddings",
        "isExtraImport": true,
        "detail": "RAG.Embeddings",
        "documentation": {}
    },
    {
        "label": "ZhipuEmbedding",
        "importPath": "RAG.Embeddings",
        "description": "RAG.Embeddings",
        "isExtraImport": true,
        "detail": "RAG.Embeddings",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "project.stream_lit_ui.stream_output",
        "description": "project.stream_lit_ui.stream_output",
        "peekOfCode": "llm = ChatOllama(model=\"qwen2.5:7b\", streaming=True)\n# 初始化会话状态\nif \"messages\" not in st.session_state:\n    st.session_state.messages = []\n# 注入自定义CSS样式\nst.markdown(\"\"\"\n<style>\n/* 用户消息样式 - 头像靠右，气泡在左 */\n.st-emotion-cache-janbn0 {\n    flex-direction: row-reverse;  /* 反转布局使头像在右侧 */",
        "detail": "project.stream_lit_ui.stream_output",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "project.stream_lit_ui.ui",
        "description": "project.stream_lit_ui.ui",
        "peekOfCode": "llm = ChatOllama(model=\"qwen2.5:7b\", streaming=True)\nwith st.sidebar:\n    with st.echo():\n        st.write(\"This code will be printed to the sidebar.\")\n    with st.spinner(\"Loading...\"):\n        time.sleep(5)\n    st.success(\"Done!\")\nwith st.container():\n    usr_img = Image.open(\"./icon/user.png\")\n    sys_img = Image.open(\"./icon/sys.png\")",
        "detail": "project.stream_lit_ui.ui",
        "documentation": {}
    },
    {
        "label": "ModernParameterDialog",
        "kind": 6,
        "importPath": "project.ui.components.dialog",
        "description": "project.ui.components.dialog",
        "peekOfCode": "class ModernParameterDialog(QDialog):\n    \"\"\"现代化的参数设置对话框\"\"\"\n    def __init__(self, title=\"参数设置\", parent=None):\n        super().__init__(parent)\n        self.setWindowTitle(title)\n        self.setModal(True)\n        self.resize(450, 600)\n        # 存储参数值\n        self.parameters = {}\n        self.widgets = {}",
        "detail": "project.ui.components.dialog",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "project.ui.components.dialog",
        "description": "project.ui.components.dialog",
        "peekOfCode": "def main():\n    app = QApplication(sys.argv)\n    # 创建对话框\n    dialog = ModernParameterDialog(\"系统配置\")\n    # 可选：设置初始参数\n    initial_params = {\n        'project_name': '测试项目',\n        'max_count': 200,\n        'precision': 0.05,\n        'mode': '自动模式',",
        "detail": "project.ui.components.dialog",
        "documentation": {}
    },
    {
        "label": "WakeWordsList",
        "kind": 6,
        "importPath": "project.ui.components.params_dialog",
        "description": "project.ui.components.params_dialog",
        "peekOfCode": "class WakeWordsList(QFrame):\n    \"\"\"唤醒词列表组件\"\"\"\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self.setup_ui()\n    def setup_ui(self):\n        \"\"\"设置UI\"\"\"\n        self.setStyleSheet(\"\"\"\n            QFrame {\n                background-color: white;",
        "detail": "project.ui.components.params_dialog",
        "documentation": {}
    },
    {
        "label": "ConversationEditDialog",
        "kind": 6,
        "importPath": "project.ui.components.params_dialog",
        "description": "project.ui.components.params_dialog",
        "peekOfCode": "class ConversationEditDialog(QDialog):\n    \"\"\"会话编辑对话框\"\"\"\n    def __init__(self, conversation_data=None, parent=None):\n        super().__init__(parent)\n        self.conversation_data = conversation_data or {}\n        self.setWindowTitle(\"编辑会话\")\n        self.setModal(True)\n        self.resize(500, 650)\n        # 存储参数值\n        self.parameters = {}",
        "detail": "project.ui.components.params_dialog",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "project.ui.components.params_dialog",
        "description": "project.ui.components.params_dialog",
        "peekOfCode": "def main():\n    app = QApplication(sys.argv)\n    # 测试数据 - 使用不同格式的时间戳\n    test_data = {\n        'id': '111111111111114',\n        'name': '生活顾问',\n        'create_time': 1710072000,  # 整数时间戳\n        'last_used_time': '1717977000.123',  # 字符串格式的浮点时间戳\n        'wake_words': ['生活', '建议', '推荐'],\n        'smart_mode': False",
        "detail": "project.ui.components.params_dialog",
        "documentation": {}
    },
    {
        "label": "ConversationItem",
        "kind": 6,
        "importPath": "project.ui.components.search_dialog",
        "description": "project.ui.components.search_dialog",
        "peekOfCode": "class ConversationItem(QFrame):\n    \"\"\"会话项目组件\"\"\"\n    clicked = pyqtSignal(dict)\n    def __init__(self, conversation_data):\n        super().__init__()\n        self.conversation_data = conversation_data\n        self.setup_ui()\n    def setup_ui(self):\n        self.setFrameStyle(QFrame.StyledPanel)\n        self.setStyleSheet(\"\"\"",
        "detail": "project.ui.components.search_dialog",
        "documentation": {}
    },
    {
        "label": "SearchDialog",
        "kind": 6,
        "importPath": "project.ui.components.search_dialog",
        "description": "project.ui.components.search_dialog",
        "peekOfCode": "class SearchDialog(QDialog):\n    \"\"\"ChatGPT风格的搜索对话框\"\"\"\n    conversation_selected = pyqtSignal(dict)\n    def __init__(self, conversations, parent=None):\n        super().__init__(parent)\n        self.conversations = conversations\n        self.filtered_conversations = conversations.copy()\n        self.setup_ui()\n        self.setup_search()\n    def setup_ui(self):",
        "detail": "project.ui.components.search_dialog",
        "documentation": {}
    },
    {
        "label": "MainWindow",
        "kind": 6,
        "importPath": "project.ui.components.search_dialog",
        "description": "project.ui.components.search_dialog",
        "peekOfCode": "class MainWindow(QMainWindow):\n    \"\"\"主窗口\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.setup_sample_data()\n        self.setup_ui()\n    def setup_sample_data(self):\n        \"\"\"设置示例数据\"\"\"\n        self.conversations = [\n            {",
        "detail": "project.ui.components.search_dialog",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "project.ui.components.search_dialog",
        "description": "project.ui.components.search_dialog",
        "peekOfCode": "def main():\n    app = QApplication(sys.argv)\n    # 设置应用样式\n    app.setStyle('Fusion')\n    window = MainWindow()\n    window.show()\n    sys.exit(app.exec_())\nif __name__ == '__main__':\n    main()",
        "detail": "project.ui.components.search_dialog",
        "documentation": {}
    },
    {
        "label": "ConversationDatabase",
        "kind": 6,
        "importPath": "project.ui.db.database",
        "description": "project.ui.db.database",
        "peekOfCode": "class ConversationDatabase:\n    \"\"\"对话数据库管理类\"\"\"\n    def __init__(self, db_path=\"conversations.db\"):\n        \"\"\"\n        初始化数据库连接\n        Args:\n            db_path (str): 数据库文件路径\n        \"\"\"\n        self.db_path = db_path\n        self.init_database()",
        "detail": "project.ui.db.database",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "project.ui.db.test_db",
        "description": "project.ui.db.test_db",
        "peekOfCode": "def main():\n    \"\"\"主函数：演示数据库操作\"\"\"\n    # 初始化数据库\n    print(\"=== 初始化数据库 ===\")\n    db = ConversationDatabase(\"test_conversations.db\")\n    # 创建示例对话\n    print(\"\\n=== 创建示例对话 ===\")\n    # 创建学习伙伴对话\n    learning_partner = db.create_new_conversation(\n        name=\"学习伙伴\",",
        "detail": "project.ui.db.test_db",
        "documentation": {}
    },
    {
        "label": "export_conversation_to_file",
        "kind": 2,
        "importPath": "project.ui.db.test_db",
        "description": "project.ui.db.test_db",
        "peekOfCode": "def export_conversation_to_file(db, conversation_id, export_path):\n    \"\"\"导出对话到文件\"\"\"\n    try:\n        conversation = db.load_conversation(conversation_id)\n        if conversation:\n            import json\n            with open(export_path, 'w', encoding='utf-8') as f:\n                json.dump(conversation, f, ensure_ascii=False, indent=2)\n            return True\n        return False",
        "detail": "project.ui.db.test_db",
        "documentation": {}
    },
    {
        "label": "rename_conversation",
        "kind": 2,
        "importPath": "project.ui.db.test_db",
        "description": "project.ui.db.test_db",
        "peekOfCode": "def rename_conversation(db, conversation_id, new_name):\n    \"\"\"重命名对话\"\"\"\n    try:\n        conversation = db.load_conversation(conversation_id)\n        if conversation:\n            conversation['name'] = new_name\n            conversation['last_updated'] = datetime.now().isoformat()\n            return db.save_conversation(conversation)\n        return False\n    except Exception as e:",
        "detail": "project.ui.db.test_db",
        "documentation": {}
    },
    {
        "label": "duplicate_conversation",
        "kind": 2,
        "importPath": "project.ui.db.test_db",
        "description": "project.ui.db.test_db",
        "peekOfCode": "def duplicate_conversation(db, conversation_id):\n    \"\"\"复制对话\"\"\"\n    try:\n        original = db.load_conversation(conversation_id)\n        if original:\n            duplicate = original.copy()\n            duplicate['id'] = db.generate_uuid()\n            duplicate['name'] = f\"{original['name']} - 副本\"\n            duplicate['create_time'] = datetime.now().timestamp()\n            duplicate['last_used_time'] = datetime.now().timestamp()",
        "detail": "project.ui.db.test_db",
        "documentation": {}
    },
    {
        "label": "ChatBot",
        "kind": 6,
        "importPath": "project.ui.demo.stream_demo",
        "description": "project.ui.demo.stream_demo",
        "peekOfCode": "class ChatBot(QObject):\n    update_text = pyqtSignal(str)\n    def __init__(self):\n        super().__init__()\n    def connect(self, func_update):\n        self.update_text.connect(func_update)\n    def disconnect(self):\n        self.update_text.disconnect()\n    def answer(self, question: str):\n        human_msg = HumanMessage(content=question)",
        "detail": "project.ui.demo.stream_demo",
        "documentation": {}
    },
    {
        "label": "ChatBotThread",
        "kind": 6,
        "importPath": "project.ui.demo.stream_demo",
        "description": "project.ui.demo.stream_demo",
        "peekOfCode": "class ChatBotThread(QThread):\n    finished = pyqtSignal()\n    def __init__(self, question: str, bot: ChatBot):\n        super().__init__()\n        self.question = question\n        self.bot = bot\n    def run(self):\n        self.bot.answer(self.question)\n        self.finished.emit()\nclass ChatApp(QWidget):",
        "detail": "project.ui.demo.stream_demo",
        "documentation": {}
    },
    {
        "label": "ChatApp",
        "kind": 6,
        "importPath": "project.ui.demo.stream_demo",
        "description": "project.ui.demo.stream_demo",
        "peekOfCode": "class ChatApp(QWidget):\n    def __init__(self):\n        super().__init__()\n        self.setWindowTitle(\"LangChain ChatOllama 流式输出\")\n        self.resize(700, 500)\n        self.layout = QVBoxLayout()\n        self.output_area = QTextEdit()\n        self.output_area.setReadOnly(True)\n        self.output_area.setStyleSheet(\"\"\"\n            QTextEdit {",
        "detail": "project.ui.demo.stream_demo",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "project.ui.demo.stream_demo",
        "description": "project.ui.demo.stream_demo",
        "peekOfCode": "llm = ChatOllama(model=\"qwen2.5:7b\", streaming=True)\nclass ChatBot(QObject):\n    update_text = pyqtSignal(str)\n    def __init__(self):\n        super().__init__()\n    def connect(self, func_update):\n        self.update_text.connect(func_update)\n    def disconnect(self):\n        self.update_text.disconnect()\n    def answer(self, question: str):",
        "detail": "project.ui.demo.stream_demo",
        "documentation": {}
    },
    {
        "label": "blue_btn_qss",
        "kind": 5,
        "importPath": "project.ui.styles.btn_qss",
        "description": "project.ui.styles.btn_qss",
        "peekOfCode": "blue_btn_qss = \"\"\"\n            QPushButton {\n                background-color: #1a73e8;\n                border: none;\n                border-radius: 18px;\n                color: white;\n                padding: 0px 16px;\n                font-size: 14px;\n                font-weight: 500;\n                font-family: \"Microsoft YaHei UI\", \"PingFang SC\", sans-serif;",
        "detail": "project.ui.styles.btn_qss",
        "documentation": {}
    },
    {
        "label": "delete_btn_qss",
        "kind": 5,
        "importPath": "project.ui.styles.btn_qss",
        "description": "project.ui.styles.btn_qss",
        "peekOfCode": "delete_btn_qss = \"\"\"\n            QPushButton {\n                background-color: #f8f9fa;\n                border: 1px solid #e8eaed;\n                color: #5f6368;\n                border-radius: 18px;\n                font-size: 16px;\n                font-weight: bold;\n                font-family: \"Microsoft YaHei UI\", \"PingFang SC\", sans-serif;\n            }",
        "detail": "project.ui.styles.btn_qss",
        "documentation": {}
    },
    {
        "label": "scrollbar_style",
        "kind": 5,
        "importPath": "project.ui.styles.qeditor",
        "description": "project.ui.styles.qeditor",
        "peekOfCode": "scrollbar_style = \"\"\"\n    /* 垂直滚动条整体样式 */\n    QScrollBar:vertical {\n        border: none;\n        background: #F9FAFB;\n        width: 2px;\n    }\n    /* 水平滚动条整体样式 */\n    QScrollBar:horizontal {\n        border: none;",
        "detail": "project.ui.styles.qeditor",
        "documentation": {}
    },
    {
        "label": "qeditor_qss",
        "kind": 5,
        "importPath": "project.ui.styles.qeditor",
        "description": "project.ui.styles.qeditor",
        "peekOfCode": "qeditor_qss = \"\"\"\n    QTextEdit {\n        background: transparent;\n        color: #374151;\n        font-size: 14px;\n        font-family: \"SF Pro Text\", \"PingFang SC\", \"Microsoft YaHei\", -apple-system, BlinkMacSystemFont, sans-serif;\n        line-height: 1.6;\n        font-weight: 400;\n        border: none;\n        margin: 0px;",
        "detail": "project.ui.styles.qeditor",
        "documentation": {}
    },
    {
        "label": "ChatBot",
        "kind": 6,
        "importPath": "project.ui.chatbot",
        "description": "project.ui.chatbot",
        "peekOfCode": "class ChatBot(QObject):\n    # 通过信号通知界面新加token\n    update_text = pyqtSignal(str)\n    # gen_end = pyqtSignal(bool)\n    def __init__(self) -> None:\n        super().__init__() #必须要调用的\n        return \n    def connect(self, func_update):\n        self.update_text.connect(func_update)\n    def disconnect(self):",
        "detail": "project.ui.chatbot",
        "documentation": {}
    },
    {
        "label": "ChatBotThread",
        "kind": 6,
        "importPath": "project.ui.chatbot",
        "description": "project.ui.chatbot",
        "peekOfCode": "class ChatBotThread(QThread):\n    # 增加线程结束信号\n    finished = pyqtSignal(str)\n    def __init__(self, question: str, bot: ChatBot):\n        super().__init__()\n        self.question = question\n        self.bot = bot\n    def run(self):\n        full_content = self.bot.answer(self.question)\n        # 完成后发出 finished 信号",
        "detail": "project.ui.chatbot",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "project.ui.chatbot",
        "description": "project.ui.chatbot",
        "peekOfCode": "llm = ChatOllama(model=\"qwen2.5:7b\", streaming=True)\nclass ChatBot(QObject):\n    # 通过信号通知界面新加token\n    update_text = pyqtSignal(str)\n    # gen_end = pyqtSignal(bool)\n    def __init__(self) -> None:\n        super().__init__() #必须要调用的\n        return \n    def connect(self, func_update):\n        self.update_text.connect(func_update)",
        "detail": "project.ui.chatbot",
        "documentation": {}
    },
    {
        "label": "LoadingWorker",
        "kind": 6,
        "importPath": "project.ui.load",
        "description": "project.ui.load",
        "peekOfCode": "class LoadingWorker(QThread):\n    \"\"\"加载工作线程\"\"\"\n    progress_updated = pyqtSignal(int)\n    loading_finished = pyqtSignal()\n    def run(self):\n        \"\"\"模拟加载过程\"\"\"\n        for i in range(101):\n            time.sleep(0.05)  # 模拟加载时间\n            self.progress_updated.emit(i)\n        self.loading_finished.emit()",
        "detail": "project.ui.load",
        "documentation": {}
    },
    {
        "label": "LoadingScreen",
        "kind": 6,
        "importPath": "project.ui.load",
        "description": "project.ui.load",
        "peekOfCode": "class LoadingScreen(QWidget):\n    \"\"\"加载界面类\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.main_window = None\n        self.background_pixmap = None\n        self.loadBackground()\n        self.initUI()\n        self.startLoading()\n    def loadBackground(self):",
        "detail": "project.ui.load",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "project.ui.load",
        "description": "project.ui.load",
        "peekOfCode": "def main():\n    \"\"\"主函数\"\"\"\n    app = QApplication(sys.argv)\n    # 设置应用程序属性\n    app.setApplicationName('智能问答系统')\n    app.setApplicationVersion('1.0')\n    # 创建并显示加载界面\n    loading_screen = LoadingScreen()\n    loading_screen.show()\n    sys.exit(app.exec_())",
        "detail": "project.ui.load",
        "documentation": {}
    },
    {
        "label": "StreamingMessageWidget",
        "kind": 6,
        "importPath": "project.ui.main_ui",
        "description": "project.ui.main_ui",
        "peekOfCode": "class StreamingMessageWidget(QWidget):\n    \"\"\"支持流式输出的消息气泡组件\"\"\"\n    def __init__(self, message=\"\", is_user=True, parent=None):\n        super().__init__(parent)\n        self.is_user = is_user\n        self.full_message = message\n        self.current_message = \"\"\n        self.setup_ui()\n    def setup_ui(self):\n        # 设置组件的尺寸策略",
        "detail": "project.ui.main_ui",
        "documentation": {}
    },
    {
        "label": "ChatInterface",
        "kind": 6,
        "importPath": "project.ui.main_ui",
        "description": "project.ui.main_ui",
        "peekOfCode": "class ChatInterface(QMainWindow):\n    \"\"\"主聊天界面\"\"\"\n    def __init__(self):\n        super().__init__()\n        # self.conversations_dir = \"chat_conversations\"  # 对话文件夹\n        self.current_conversation = []\n        self.conversations = []  # 包含已保存和未保存的会话\n        self.current_conversation_index = -1  # 当前对话在历史列表中的索引，-1表示新对话\n        self.current_conversation_file = None  # 当前对话的文件名\n        self.is_ai_responding = False  # AI是否正在回复",
        "detail": "project.ui.main_ui",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "project.ui.main_ui",
        "description": "project.ui.main_ui",
        "peekOfCode": "def main():\n    app = QApplication(sys.argv)\n    # 设置应用程序样式\n    app.setStyle('Fusion')\n    window = ChatInterface()\n    window.show()\n    sys.exit(app.exec_())\nif __name__ == '__main__':\n    main()",
        "detail": "project.ui.main_ui",
        "documentation": {}
    },
    {
        "label": "chatbot",
        "kind": 5,
        "importPath": "project.ui.main_ui",
        "description": "project.ui.main_ui",
        "peekOfCode": "chatbot = ChatBot()\ndb = ConversationDatabase()\nMAX_HEIGHT = 650\nMIN_HEIGHT = 28\nUSER_MAX_WIDTH = 300\nAI_MAX_WIDTH = 680\nclass StreamingMessageWidget(QWidget):\n    \"\"\"支持流式输出的消息气泡组件\"\"\"\n    def __init__(self, message=\"\", is_user=True, parent=None):\n        super().__init__(parent)",
        "detail": "project.ui.main_ui",
        "documentation": {}
    },
    {
        "label": "db",
        "kind": 5,
        "importPath": "project.ui.main_ui",
        "description": "project.ui.main_ui",
        "peekOfCode": "db = ConversationDatabase()\nMAX_HEIGHT = 650\nMIN_HEIGHT = 28\nUSER_MAX_WIDTH = 300\nAI_MAX_WIDTH = 680\nclass StreamingMessageWidget(QWidget):\n    \"\"\"支持流式输出的消息气泡组件\"\"\"\n    def __init__(self, message=\"\", is_user=True, parent=None):\n        super().__init__(parent)\n        self.is_user = is_user",
        "detail": "project.ui.main_ui",
        "documentation": {}
    },
    {
        "label": "MAX_HEIGHT",
        "kind": 5,
        "importPath": "project.ui.main_ui",
        "description": "project.ui.main_ui",
        "peekOfCode": "MAX_HEIGHT = 650\nMIN_HEIGHT = 28\nUSER_MAX_WIDTH = 300\nAI_MAX_WIDTH = 680\nclass StreamingMessageWidget(QWidget):\n    \"\"\"支持流式输出的消息气泡组件\"\"\"\n    def __init__(self, message=\"\", is_user=True, parent=None):\n        super().__init__(parent)\n        self.is_user = is_user\n        self.full_message = message",
        "detail": "project.ui.main_ui",
        "documentation": {}
    },
    {
        "label": "MIN_HEIGHT",
        "kind": 5,
        "importPath": "project.ui.main_ui",
        "description": "project.ui.main_ui",
        "peekOfCode": "MIN_HEIGHT = 28\nUSER_MAX_WIDTH = 300\nAI_MAX_WIDTH = 680\nclass StreamingMessageWidget(QWidget):\n    \"\"\"支持流式输出的消息气泡组件\"\"\"\n    def __init__(self, message=\"\", is_user=True, parent=None):\n        super().__init__(parent)\n        self.is_user = is_user\n        self.full_message = message\n        self.current_message = \"\"",
        "detail": "project.ui.main_ui",
        "documentation": {}
    },
    {
        "label": "USER_MAX_WIDTH",
        "kind": 5,
        "importPath": "project.ui.main_ui",
        "description": "project.ui.main_ui",
        "peekOfCode": "USER_MAX_WIDTH = 300\nAI_MAX_WIDTH = 680\nclass StreamingMessageWidget(QWidget):\n    \"\"\"支持流式输出的消息气泡组件\"\"\"\n    def __init__(self, message=\"\", is_user=True, parent=None):\n        super().__init__(parent)\n        self.is_user = is_user\n        self.full_message = message\n        self.current_message = \"\"\n        self.setup_ui()",
        "detail": "project.ui.main_ui",
        "documentation": {}
    },
    {
        "label": "AI_MAX_WIDTH",
        "kind": 5,
        "importPath": "project.ui.main_ui",
        "description": "project.ui.main_ui",
        "peekOfCode": "AI_MAX_WIDTH = 680\nclass StreamingMessageWidget(QWidget):\n    \"\"\"支持流式输出的消息气泡组件\"\"\"\n    def __init__(self, message=\"\", is_user=True, parent=None):\n        super().__init__(parent)\n        self.is_user = is_user\n        self.full_message = message\n        self.current_message = \"\"\n        self.setup_ui()\n    def setup_ui(self):",
        "detail": "project.ui.main_ui",
        "documentation": {}
    },
    {
        "label": "VAD_Whisper",
        "kind": 6,
        "importPath": "project.ui.vad_fast_whisper",
        "description": "project.ui.vad_fast_whisper",
        "peekOfCode": "class VAD_Whisper:\n    def __init__(self, model_size=\"large-v3-turbo-ct2\", beam_size=5, language='zh', \n                 initial_prompt=\"这是一条控制指令，A机A机，B机B机，平飞，偏置飞行，直线加速，小角度盘旋，拉起发射，俯仰角10度，速度100马赫。\"):\n        self.model_size = model_size\n        self.beam_size = beam_size\n        self.language = language\n        self.initial_prompt = initial_prompt\n        self.condition_on_previous_text = True\n        # 加载模型和工具函数\n        self.model, self.utils = torch.hub.load(",
        "detail": "project.ui.vad_fast_whisper",
        "documentation": {}
    },
    {
        "label": "GraphState",
        "kind": 6,
        "importPath": "python_use.agent.langchain_use.demo.test",
        "description": "python_use.agent.langchain_use.demo.test",
        "peekOfCode": "class GraphState(dict):\n    pass\n# 节点1：调用 LLM\ndef answer_question(state: GraphState) -> GraphState:\n    messages = state[\"messages\"]\n    response = llm(messages)\n    state[\"messages\"].append({\"role\": \"assistant\", \"content\": response.content})\n    return state\n# 1. 判断是否需要搜索\ndef should_search(state: GraphState) -> str:",
        "detail": "python_use.agent.langchain_use.demo.test",
        "documentation": {}
    },
    {
        "label": "answer_question",
        "kind": 2,
        "importPath": "python_use.agent.langchain_use.demo.test",
        "description": "python_use.agent.langchain_use.demo.test",
        "peekOfCode": "def answer_question(state: GraphState) -> GraphState:\n    messages = state[\"messages\"]\n    response = llm(messages)\n    state[\"messages\"].append({\"role\": \"assistant\", \"content\": response.content})\n    return state\n# 1. 判断是否需要搜索\ndef should_search(state: GraphState) -> str:\n    question = state[\"messages\"][-1][\"content\"]\n    if \"latest\" in question or \"current\" in question:\n        return \"search\"",
        "detail": "python_use.agent.langchain_use.demo.test",
        "documentation": {}
    },
    {
        "label": "should_search",
        "kind": 2,
        "importPath": "python_use.agent.langchain_use.demo.test",
        "description": "python_use.agent.langchain_use.demo.test",
        "peekOfCode": "def should_search(state: GraphState) -> str:\n    question = state[\"messages\"][-1][\"content\"]\n    if \"latest\" in question or \"current\" in question:\n        return \"search\"\n    return \"answer\"\n# 2. 定义搜索节点\ndef search_web(state: GraphState) -> GraphState:\n    # 假设是搜索后的结果\n    result = \"According to recent sources, ...\"\n    state[\"messages\"].append({\"role\": \"function\", \"name\": \"search_web\", \"content\": result})",
        "detail": "python_use.agent.langchain_use.demo.test",
        "documentation": {}
    },
    {
        "label": "search_web",
        "kind": 2,
        "importPath": "python_use.agent.langchain_use.demo.test",
        "description": "python_use.agent.langchain_use.demo.test",
        "peekOfCode": "def search_web(state: GraphState) -> GraphState:\n    # 假设是搜索后的结果\n    result = \"According to recent sources, ...\"\n    state[\"messages\"].append({\"role\": \"function\", \"name\": \"search_web\", \"content\": result})\n    return state\n# 构建图\ngraph = StateGraph(GraphState)\n# 添加节点\ngraph.add_node(\"answer\", answer_question)\ngraph.add_node(\"search\", search_web)",
        "detail": "python_use.agent.langchain_use.demo.test",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "python_use.agent.langchain_use.demo.test",
        "description": "python_use.agent.langchain_use.demo.test",
        "peekOfCode": "llm = ChatOllama(model=\"qwen2.5:7b\")\n# 定义图状态：只包含消息列表\nclass GraphState(dict):\n    pass\n# 节点1：调用 LLM\ndef answer_question(state: GraphState) -> GraphState:\n    messages = state[\"messages\"]\n    response = llm(messages)\n    state[\"messages\"].append({\"role\": \"assistant\", \"content\": response.content})\n    return state",
        "detail": "python_use.agent.langchain_use.demo.test",
        "documentation": {}
    },
    {
        "label": "graph",
        "kind": 5,
        "importPath": "python_use.agent.langchain_use.demo.test",
        "description": "python_use.agent.langchain_use.demo.test",
        "peekOfCode": "graph = StateGraph(GraphState)\n# 添加节点\ngraph.add_node(\"answer\", answer_question)\ngraph.add_node(\"search\", search_web)\n# 添加分支判断\ngraph.add_conditional_edges(\"router\", should_search, {\n    \"answer\": \"answer\",\n    \"search\": \"search\",\n})\n# 路由后进入终点",
        "detail": "python_use.agent.langchain_use.demo.test",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "python_use.agent.langchain_use.demo.test",
        "description": "python_use.agent.langchain_use.demo.test",
        "peekOfCode": "app = graph.compile()\n# 测试\nstate = GraphState(messages=[HumanMessage(content=\"What is the latest news about AI?\")])\nresult = app.invoke(state)\nprint(result[\"messages\"][-1][\"content\"])",
        "detail": "python_use.agent.langchain_use.demo.test",
        "documentation": {}
    },
    {
        "label": "state",
        "kind": 5,
        "importPath": "python_use.agent.langchain_use.demo.test",
        "description": "python_use.agent.langchain_use.demo.test",
        "peekOfCode": "state = GraphState(messages=[HumanMessage(content=\"What is the latest news about AI?\")])\nresult = app.invoke(state)\nprint(result[\"messages\"][-1][\"content\"])",
        "detail": "python_use.agent.langchain_use.demo.test",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "python_use.agent.langchain_use.demo.test",
        "description": "python_use.agent.langchain_use.demo.test",
        "peekOfCode": "result = app.invoke(state)\nprint(result[\"messages\"][-1][\"content\"])",
        "detail": "python_use.agent.langchain_use.demo.test",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "python_use.agent.langchain_use.chat",
        "description": "python_use.agent.langchain_use.chat",
        "peekOfCode": "model = ChatOpenAI(model=\"qwen2:7b\", base_url=\"127.0.0.1:10087\")  # 使用新包中的类\nasync def run():\n    # 方法1: 异步流式调用\n    print(\"\\n--- 方法1 ---\")\n    async for chunk in model.astream(\"天空的颜色是什么？\"):\n        print(chunk.content, end=\"\", flush=True)\n    print(\"next???????\")\n    # 方法2: 使用LCEL链式处理\n    print(\"\\n\\n--- 方法2 ---\")\n    prompt = ChatPromptTemplate.from_template(\"讲一个关于{topic}的笑话\")",
        "detail": "python_use.agent.langchain_use.chat",
        "documentation": {}
    },
    {
        "label": "evaluate_agent",
        "kind": 2,
        "importPath": "python_use.agent.langchain_use.eval_langgraph_agent",
        "description": "python_use.agent.langchain_use.eval_langgraph_agent",
        "peekOfCode": "def evaluate_agent():\n    test_results = []\n    thread_id = uuid.uuid4()\n    config = {\"configurable\": {\"thread_id\": thread_id}}\n    for case in test_cases:\n        # 运行代理\n        messages = []\n        input_msg = HumanMessage(content=case[\"input\"])\n        for event in app.stream(\n            {\"messages\": [input_msg]}, ",
        "detail": "python_use.agent.langchain_use.eval_langgraph_agent",
        "documentation": {}
    },
    {
        "label": "print_results",
        "kind": 2,
        "importPath": "python_use.agent.langchain_use.eval_langgraph_agent",
        "description": "python_use.agent.langchain_use.eval_langgraph_agent",
        "peekOfCode": "def print_results(results):\n    print(\"=\"*50)\n    print(\"LangGraph代理评估报告\")\n    print(\"=\"*50)\n    for i, res in enumerate(results, 1):\n        print(f\"\\n测试 #{i}: {res['description']}\")\n        print(f\"输入: {res['input']}\")\n        print(f\"预期: {res['expected']}\")\n        print(f\"实际: {res['actual']}\")\n        print(\"\\n评估指标:\")",
        "detail": "python_use.agent.langchain_use.eval_langgraph_agent",
        "documentation": {}
    },
    {
        "label": "eval_model",
        "kind": 5,
        "importPath": "python_use.agent.langchain_use.eval_langgraph_agent",
        "description": "python_use.agent.langchain_use.eval_langgraph_agent",
        "peekOfCode": "eval_model = OllamaModel(model='qwen2.5:7b')\n# 配置评估指标\nmetrics = [\n    AnswerRelevancyMetric(threshold=0.7, model=eval_model),\n    HallucinationMetric(threshold=0.8, model=eval_model),\n]\n# # 主要作用是用于自动评估？？\n# # 正确初始化回调处理器\n# callback_handler = DeepEvalCallbackHandler(metrics=metrics)\n# 定义测试场景",
        "detail": "python_use.agent.langchain_use.eval_langgraph_agent",
        "documentation": {}
    },
    {
        "label": "metrics",
        "kind": 5,
        "importPath": "python_use.agent.langchain_use.eval_langgraph_agent",
        "description": "python_use.agent.langchain_use.eval_langgraph_agent",
        "peekOfCode": "metrics = [\n    AnswerRelevancyMetric(threshold=0.7, model=eval_model),\n    HallucinationMetric(threshold=0.8, model=eval_model),\n]\n# # 主要作用是用于自动评估？？\n# # 正确初始化回调处理器\n# callback_handler = DeepEvalCallbackHandler(metrics=metrics)\n# 定义测试场景\ntest_cases = [\n    {",
        "detail": "python_use.agent.langchain_use.eval_langgraph_agent",
        "documentation": {}
    },
    {
        "label": "test_cases",
        "kind": 5,
        "importPath": "python_use.agent.langchain_use.eval_langgraph_agent",
        "description": "python_use.agent.langchain_use.eval_langgraph_agent",
        "peekOfCode": "test_cases = [\n    {\n        \"input\": \"hi! I'm bob. What is my age?\",\n        \"expected_output\": \"42 years old\",\n        \"context\": [\"User name is Bob\"],\n        \"description\": \"应正确调用工具获取年龄\"\n    },\n    {\n        \"input\": \"do you remember my name?\",\n        \"expected_output\": \"Bob\",",
        "detail": "python_use.agent.langchain_use.eval_langgraph_agent",
        "documentation": {}
    },
    {
        "label": "get_user_age",
        "kind": 2,
        "importPath": "python_use.agent.langchain_use.langgraph_use",
        "description": "python_use.agent.langchain_use.langgraph_use",
        "peekOfCode": "def get_user_age(name: str) -> str:\n    \"\"\"Use this tool to find the user's age.\"\"\"\n    # This is a placeholder for the actual implementation\n    if \"bob\" in name.lower():\n        return \"42 years old\"\n    return \"41 years old\"\nmemory = MemorySaver()\nmodel = ChatOllama(model=\"Qwen2.5-7B-Instruct\")\n# 上下文处理得到\ndef prompt(state) -> list[BaseMessage]:",
        "detail": "python_use.agent.langchain_use.langgraph_use",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 2,
        "importPath": "python_use.agent.langchain_use.langgraph_use",
        "description": "python_use.agent.langchain_use.langgraph_use",
        "peekOfCode": "def prompt(state) -> list[BaseMessage]:\n    \"\"\"Given the agent state, return a list of messages for the chat model.\"\"\"\n    # We're using the message processor defined above.\n    return trim_messages(\n        state[\"messages\"],\n        token_counter=len,  # <-- len will simply count the number of messages rather than tokens\n        max_tokens=5,  # <-- allow up to 5 messages.\n        strategy=\"last\",\n        # Most chat models expect that chat history starts with either:\n        # (1) a HumanMessage or",
        "detail": "python_use.agent.langchain_use.langgraph_use",
        "documentation": {}
    },
    {
        "label": "memory",
        "kind": 5,
        "importPath": "python_use.agent.langchain_use.langgraph_use",
        "description": "python_use.agent.langchain_use.langgraph_use",
        "peekOfCode": "memory = MemorySaver()\nmodel = ChatOllama(model=\"Qwen2.5-7B-Instruct\")\n# 上下文处理得到\ndef prompt(state) -> list[BaseMessage]:\n    \"\"\"Given the agent state, return a list of messages for the chat model.\"\"\"\n    # We're using the message processor defined above.\n    return trim_messages(\n        state[\"messages\"],\n        token_counter=len,  # <-- len will simply count the number of messages rather than tokens\n        max_tokens=5,  # <-- allow up to 5 messages.",
        "detail": "python_use.agent.langchain_use.langgraph_use",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "python_use.agent.langchain_use.langgraph_use",
        "description": "python_use.agent.langchain_use.langgraph_use",
        "peekOfCode": "model = ChatOllama(model=\"Qwen2.5-7B-Instruct\")\n# 上下文处理得到\ndef prompt(state) -> list[BaseMessage]:\n    \"\"\"Given the agent state, return a list of messages for the chat model.\"\"\"\n    # We're using the message processor defined above.\n    return trim_messages(\n        state[\"messages\"],\n        token_counter=len,  # <-- len will simply count the number of messages rather than tokens\n        max_tokens=5,  # <-- allow up to 5 messages.\n        strategy=\"last\",",
        "detail": "python_use.agent.langchain_use.langgraph_use",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "python_use.agent.langchain_use.langgraph_use",
        "description": "python_use.agent.langchain_use.langgraph_use",
        "peekOfCode": "app = create_react_agent(\n    model,\n    tools=[get_user_age],\n    checkpointer=memory,\n    prompt=prompt,\n)\nif __name__ == \"__main__\":\n    # The thread id is a unique key that identifies\n    # this particular conversation.\n    # We'll just generate a random uuid here.",
        "detail": "python_use.agent.langchain_use.langgraph_use",
        "documentation": {}
    },
    {
        "label": "call_model",
        "kind": 2,
        "importPath": "python_use.agent.memory.langchain_memory",
        "description": "python_use.agent.memory.langchain_memory",
        "peekOfCode": "def call_model(state: MessagesState):\n    response = model.invoke(state[\"messages\"])\n    # We return a list, because this will get added to the existing list\n    return {\"messages\": response}\n# Define the two nodes we will cycle between\nworkflow.add_edge(START, \"model\")\nworkflow.add_node(\"model\", call_model)\n# Adding memory is straight forward in langgraph!\nmemory = MemorySaver()\napp = workflow.compile(",
        "detail": "python_use.agent.memory.langchain_memory",
        "documentation": {}
    },
    {
        "label": "workflow",
        "kind": 5,
        "importPath": "python_use.agent.memory.langchain_memory",
        "description": "python_use.agent.memory.langchain_memory",
        "peekOfCode": "workflow = StateGraph(state_schema=MessagesState)\n# Define a chat model\n### 设置环境\nos.environ[\"OPENAI_API_KEY\"] =  'sk-d58877221cec4f208dc353259ca9c8bc' # 阿里云Qwen密钥\nos.environ[\"OPENAI_BASE_URL\"] = 'https://dashscope.aliyuncs.com/compatible-mode/v1'   # 例如：https://dashscope.aliyuncs.com/compatible-mode/v1\n# 初始化大模型\nmodel = ChatOpenAI(\n    # model=os.getenv('DEEPSEEK_MODEL'),\n    # api_key=os.getenv('DEEPSEEK_API_KEY'),\n    # base_url=os.getenv('DEEPSEEK_URL')",
        "detail": "python_use.agent.memory.langchain_memory",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_API_KEY\"]",
        "kind": 5,
        "importPath": "python_use.agent.memory.langchain_memory",
        "description": "python_use.agent.memory.langchain_memory",
        "peekOfCode": "os.environ[\"OPENAI_API_KEY\"] =  'sk-d58877221cec4f208dc353259ca9c8bc' # 阿里云Qwen密钥\nos.environ[\"OPENAI_BASE_URL\"] = 'https://dashscope.aliyuncs.com/compatible-mode/v1'   # 例如：https://dashscope.aliyuncs.com/compatible-mode/v1\n# 初始化大模型\nmodel = ChatOpenAI(\n    # model=os.getenv('DEEPSEEK_MODEL'),\n    # api_key=os.getenv('DEEPSEEK_API_KEY'),\n    # base_url=os.getenv('DEEPSEEK_URL')\n    model = \"qwen-plus\",\n    api_key =  os.getenv('OPENAI_API_KEY'),\n    base_url = os.getenv('OPENAI_BASE_URL')",
        "detail": "python_use.agent.memory.langchain_memory",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENAI_BASE_URL\"]",
        "kind": 5,
        "importPath": "python_use.agent.memory.langchain_memory",
        "description": "python_use.agent.memory.langchain_memory",
        "peekOfCode": "os.environ[\"OPENAI_BASE_URL\"] = 'https://dashscope.aliyuncs.com/compatible-mode/v1'   # 例如：https://dashscope.aliyuncs.com/compatible-mode/v1\n# 初始化大模型\nmodel = ChatOpenAI(\n    # model=os.getenv('DEEPSEEK_MODEL'),\n    # api_key=os.getenv('DEEPSEEK_API_KEY'),\n    # base_url=os.getenv('DEEPSEEK_URL')\n    model = \"qwen-plus\",\n    api_key =  os.getenv('OPENAI_API_KEY'),\n    base_url = os.getenv('OPENAI_BASE_URL')\n)",
        "detail": "python_use.agent.memory.langchain_memory",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "python_use.agent.memory.langchain_memory",
        "description": "python_use.agent.memory.langchain_memory",
        "peekOfCode": "model = ChatOpenAI(\n    # model=os.getenv('DEEPSEEK_MODEL'),\n    # api_key=os.getenv('DEEPSEEK_API_KEY'),\n    # base_url=os.getenv('DEEPSEEK_URL')\n    model = \"qwen-plus\",\n    api_key =  os.getenv('OPENAI_API_KEY'),\n    base_url = os.getenv('OPENAI_BASE_URL')\n)\n# Define the function that calls the model\ndef call_model(state: MessagesState):",
        "detail": "python_use.agent.memory.langchain_memory",
        "documentation": {}
    },
    {
        "label": "memory",
        "kind": 5,
        "importPath": "python_use.agent.memory.langchain_memory",
        "description": "python_use.agent.memory.langchain_memory",
        "peekOfCode": "memory = MemorySaver()\napp = workflow.compile(\n    checkpointer=memory\n)\n# The thread id is a unique key that identifies\n# this particular conversation.\n# We'll just generate a random uuid here.\n# This enables a single application to manage conversations among multiple users.\nthread_id = uuid.uuid4()\nconfig = {\"configurable\": {\"thread_id\": thread_id}}",
        "detail": "python_use.agent.memory.langchain_memory",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "python_use.agent.memory.langchain_memory",
        "description": "python_use.agent.memory.langchain_memory",
        "peekOfCode": "app = workflow.compile(\n    checkpointer=memory\n)\n# The thread id is a unique key that identifies\n# this particular conversation.\n# We'll just generate a random uuid here.\n# This enables a single application to manage conversations among multiple users.\nthread_id = uuid.uuid4()\nconfig = {\"configurable\": {\"thread_id\": thread_id}}\ninput_message = HumanMessage(content=\"hi! I'm bob\")",
        "detail": "python_use.agent.memory.langchain_memory",
        "documentation": {}
    },
    {
        "label": "thread_id",
        "kind": 5,
        "importPath": "python_use.agent.memory.langchain_memory",
        "description": "python_use.agent.memory.langchain_memory",
        "peekOfCode": "thread_id = uuid.uuid4()\nconfig = {\"configurable\": {\"thread_id\": thread_id}}\ninput_message = HumanMessage(content=\"hi! I'm bob\")\nfor event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n    event[\"messages\"][-1].pretty_print()\n# Here, let's confirm that the AI remembers our name!\ninput_message = HumanMessage(content=\"what was my name?\")\nfor event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n    event[\"messages\"][-1].pretty_print()\nsnapstate = app.get_state(config)",
        "detail": "python_use.agent.memory.langchain_memory",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "python_use.agent.memory.langchain_memory",
        "description": "python_use.agent.memory.langchain_memory",
        "peekOfCode": "config = {\"configurable\": {\"thread_id\": thread_id}}\ninput_message = HumanMessage(content=\"hi! I'm bob\")\nfor event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n    event[\"messages\"][-1].pretty_print()\n# Here, let's confirm that the AI remembers our name!\ninput_message = HumanMessage(content=\"what was my name?\")\nfor event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n    event[\"messages\"][-1].pretty_print()\nsnapstate = app.get_state(config)\nprint(snapstate.values)",
        "detail": "python_use.agent.memory.langchain_memory",
        "documentation": {}
    },
    {
        "label": "input_message",
        "kind": 5,
        "importPath": "python_use.agent.memory.langchain_memory",
        "description": "python_use.agent.memory.langchain_memory",
        "peekOfCode": "input_message = HumanMessage(content=\"hi! I'm bob\")\nfor event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n    event[\"messages\"][-1].pretty_print()\n# Here, let's confirm that the AI remembers our name!\ninput_message = HumanMessage(content=\"what was my name?\")\nfor event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n    event[\"messages\"][-1].pretty_print()\nsnapstate = app.get_state(config)\nprint(snapstate.values)",
        "detail": "python_use.agent.memory.langchain_memory",
        "documentation": {}
    },
    {
        "label": "input_message",
        "kind": 5,
        "importPath": "python_use.agent.memory.langchain_memory",
        "description": "python_use.agent.memory.langchain_memory",
        "peekOfCode": "input_message = HumanMessage(content=\"what was my name?\")\nfor event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n    event[\"messages\"][-1].pretty_print()\nsnapstate = app.get_state(config)\nprint(snapstate.values)",
        "detail": "python_use.agent.memory.langchain_memory",
        "documentation": {}
    },
    {
        "label": "snapstate",
        "kind": 5,
        "importPath": "python_use.agent.memory.langchain_memory",
        "description": "python_use.agent.memory.langchain_memory",
        "peekOfCode": "snapstate = app.get_state(config)\nprint(snapstate.values)",
        "detail": "python_use.agent.memory.langchain_memory",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "python_use.agent.memory._mem0",
        "description": "python_use.agent.memory._mem0",
        "peekOfCode": "llm = ChatOllama(model=\"qwen2.5:7b\")\nconfig = {\n    \"vector_store\": {\n        \"provider\": \"qdrant\",\n        \"config\": {\n            \"collection_name\": \"test\",\n            \"host\": \"localhost\",\n            \"port\": 6333,\n            \"embedding_model_dims\": 1024  # 修改为实际模型维度\n        },",
        "detail": "python_use.agent.memory._mem0",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "python_use.agent.memory._mem0",
        "description": "python_use.agent.memory._mem0",
        "peekOfCode": "config = {\n    \"vector_store\": {\n        \"provider\": \"qdrant\",\n        \"config\": {\n            \"collection_name\": \"test\",\n            \"host\": \"localhost\",\n            \"port\": 6333,\n            \"embedding_model_dims\": 1024  # 修改为实际模型维度\n        },\n    },",
        "detail": "python_use.agent.memory._mem0",
        "documentation": {}
    },
    {
        "label": "m",
        "kind": 5,
        "importPath": "python_use.agent.memory._mem0",
        "description": "python_use.agent.memory._mem0",
        "peekOfCode": "m = Memory.from_config(config)\nprint(\"加载完成\")\nmessages = [\n    {\"role\": \"user\", \"content\": \"Could you tell me what 1 plus 1111 equals?\"},\n    {\"role\": \"assistant\", \"content\": \"1112\"},\n    {\"role\": \"user\", \"content\": \"What is that answer plus 1?\"},\n    {\"role\": \"assistant\", \"content\": \"1113\"}\n]\nprompt = PromptTemplate(\n    input_variables=[\"history\", \"question\"],",
        "detail": "python_use.agent.memory._mem0",
        "documentation": {}
    },
    {
        "label": "messages",
        "kind": 5,
        "importPath": "python_use.agent.memory._mem0",
        "description": "python_use.agent.memory._mem0",
        "peekOfCode": "messages = [\n    {\"role\": \"user\", \"content\": \"Could you tell me what 1 plus 1111 equals?\"},\n    {\"role\": \"assistant\", \"content\": \"1112\"},\n    {\"role\": \"user\", \"content\": \"What is that answer plus 1?\"},\n    {\"role\": \"assistant\", \"content\": \"1113\"}\n]\nprompt = PromptTemplate(\n    input_variables=[\"history\", \"question\"],\n    template = \"\"\"\n结合上述参考历史对话记录:{history}",
        "detail": "python_use.agent.memory._mem0",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "python_use.agent.memory._mem0",
        "description": "python_use.agent.memory._mem0",
        "peekOfCode": "prompt = PromptTemplate(\n    input_variables=[\"history\", \"question\"],\n    template = \"\"\"\n结合上述参考历史对话记录:{history}\n回答:{question}\"\"\"\n)\nquestion = \"最近的一个数学问题是什么\"\nmyPrompt = prompt.format_prompt(\n    history=messages,\n    question=question",
        "detail": "python_use.agent.memory._mem0",
        "documentation": {}
    },
    {
        "label": "question",
        "kind": 5,
        "importPath": "python_use.agent.memory._mem0",
        "description": "python_use.agent.memory._mem0",
        "peekOfCode": "question = \"最近的一个数学问题是什么\"\nmyPrompt = prompt.format_prompt(\n    history=messages,\n    question=question\n)\nres = llm.invoke(myPrompt)\nprint(res.content)\nmessages.append({\"role\": \"user\", \"content\": question })\nmessages.append({\"role\": \"assistant\", \"content\": res.content})\n# 确保 user_id 一致",
        "detail": "python_use.agent.memory._mem0",
        "documentation": {}
    },
    {
        "label": "myPrompt",
        "kind": 5,
        "importPath": "python_use.agent.memory._mem0",
        "description": "python_use.agent.memory._mem0",
        "peekOfCode": "myPrompt = prompt.format_prompt(\n    history=messages,\n    question=question\n)\nres = llm.invoke(myPrompt)\nprint(res.content)\nmessages.append({\"role\": \"user\", \"content\": question })\nmessages.append({\"role\": \"assistant\", \"content\": res.content})\n# 确保 user_id 一致\nm.add(messages, user_id=\"damn\", metadata={\"type\": \"math_qa\"})",
        "detail": "python_use.agent.memory._mem0",
        "documentation": {}
    },
    {
        "label": "res",
        "kind": 5,
        "importPath": "python_use.agent.memory._mem0",
        "description": "python_use.agent.memory._mem0",
        "peekOfCode": "res = llm.invoke(myPrompt)\nprint(res.content)\nmessages.append({\"role\": \"user\", \"content\": question })\nmessages.append({\"role\": \"assistant\", \"content\": res.content})\n# 确保 user_id 一致\nm.add(messages, user_id=\"damn\", metadata={\"type\": \"math_qa\"})\nrelated_memories = m.search(query=\"waht's the last questions？\", user_id=\"damn\", filters={\"type\": \"math_qa\"})\nprint(related_memories )",
        "detail": "python_use.agent.memory._mem0",
        "documentation": {}
    },
    {
        "label": "related_memories",
        "kind": 5,
        "importPath": "python_use.agent.memory._mem0",
        "description": "python_use.agent.memory._mem0",
        "peekOfCode": "related_memories = m.search(query=\"waht's the last questions？\", user_id=\"damn\", filters={\"type\": \"math_qa\"})\nprint(related_memories )",
        "detail": "python_use.agent.memory._mem0",
        "documentation": {}
    },
    {
        "label": "loader",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_map_reduce",
        "description": "python_use.agent.split.summary_map_reduce",
        "peekOfCode": "loader = TextLoader(\"./chineseJH.txt\", encoding=\"utf-8\")\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\ndocs = loader.load_and_split(text_splitter)\n# 初始化模型\nllm = ChatOpenAI(model=\"qwen:7b\", base_url=\"http://127.0.0.1:10087/v1\", api_key=\"vllm\")\n# 创建链式处理器\nchain = load_summarize_chain(\n    llm,\n    chain_type=\"refine\",\n    verbose=True,",
        "detail": "python_use.agent.split.summary_map_reduce",
        "documentation": {}
    },
    {
        "label": "text_splitter",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_map_reduce",
        "description": "python_use.agent.split.summary_map_reduce",
        "peekOfCode": "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\ndocs = loader.load_and_split(text_splitter)\n# 初始化模型\nllm = ChatOpenAI(model=\"qwen:7b\", base_url=\"http://127.0.0.1:10087/v1\", api_key=\"vllm\")\n# 创建链式处理器\nchain = load_summarize_chain(\n    llm,\n    chain_type=\"refine\",\n    verbose=True,\n    map_prompt=PromptTemplate.from_template(\"总结本段内容：\\n{text}\"),  # 自定义Map提示词",
        "detail": "python_use.agent.split.summary_map_reduce",
        "documentation": {}
    },
    {
        "label": "docs",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_map_reduce",
        "description": "python_use.agent.split.summary_map_reduce",
        "peekOfCode": "docs = loader.load_and_split(text_splitter)\n# 初始化模型\nllm = ChatOpenAI(model=\"qwen:7b\", base_url=\"http://127.0.0.1:10087/v1\", api_key=\"vllm\")\n# 创建链式处理器\nchain = load_summarize_chain(\n    llm,\n    chain_type=\"refine\",\n    verbose=True,\n    map_prompt=PromptTemplate.from_template(\"总结本段内容：\\n{text}\"),  # 自定义Map提示词\n    combine_prompt=PromptTemplate.from_template(\"合并以下摘要：\\n{text}\") # 自定义Reduce提示词",
        "detail": "python_use.agent.split.summary_map_reduce",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_map_reduce",
        "description": "python_use.agent.split.summary_map_reduce",
        "peekOfCode": "llm = ChatOpenAI(model=\"qwen:7b\", base_url=\"http://127.0.0.1:10087/v1\", api_key=\"vllm\")\n# 创建链式处理器\nchain = load_summarize_chain(\n    llm,\n    chain_type=\"refine\",\n    verbose=True,\n    map_prompt=PromptTemplate.from_template(\"总结本段内容：\\n{text}\"),  # 自定义Map提示词\n    combine_prompt=PromptTemplate.from_template(\"合并以下摘要：\\n{text}\") # 自定义Reduce提示词\n)\n# 执行总结（处理前5块）",
        "detail": "python_use.agent.split.summary_map_reduce",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_map_reduce",
        "description": "python_use.agent.split.summary_map_reduce",
        "peekOfCode": "chain = load_summarize_chain(\n    llm,\n    chain_type=\"refine\",\n    verbose=True,\n    map_prompt=PromptTemplate.from_template(\"总结本段内容：\\n{text}\"),  # 自定义Map提示词\n    combine_prompt=PromptTemplate.from_template(\"合并以下摘要：\\n{text}\") # 自定义Reduce提示词\n)\n# 执行总结（处理前5块）\nresult = chain.invoke(docs[:2])\nprint(result)",
        "detail": "python_use.agent.split.summary_map_reduce",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_map_reduce",
        "description": "python_use.agent.split.summary_map_reduce",
        "peekOfCode": "result = chain.invoke(docs[:2])\nprint(result)\nprint(\"===\"*50)\nprint(docs[:2])",
        "detail": "python_use.agent.split.summary_map_reduce",
        "documentation": {}
    },
    {
        "label": "loader",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_refine",
        "description": "python_use.agent.split.summary_refine",
        "peekOfCode": "loader = TextLoader(\"./chineseJH.txt\", encoding=\"utf-8\")\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\ndocs = loader.load_and_split(text_splitter)\n# 初始化模型\nllm = ChatOpenAI(model=\"qwen:7b\", base_url=\"http://127.0.0.1:10087/v1\", api_key=\"vllm\")\n# 修正后的提示模板\nquestion_prompt = PromptTemplate(\n    template=\"根据新内容完善总结：{text}\",\n    input_variables=[\"text\"]\n)",
        "detail": "python_use.agent.split.summary_refine",
        "documentation": {}
    },
    {
        "label": "text_splitter",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_refine",
        "description": "python_use.agent.split.summary_refine",
        "peekOfCode": "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\ndocs = loader.load_and_split(text_splitter)\n# 初始化模型\nllm = ChatOpenAI(model=\"qwen:7b\", base_url=\"http://127.0.0.1:10087/v1\", api_key=\"vllm\")\n# 修正后的提示模板\nquestion_prompt = PromptTemplate(\n    template=\"根据新内容完善总结：{text}\",\n    input_variables=[\"text\"]\n)\nrefine_prompt = PromptTemplate(",
        "detail": "python_use.agent.split.summary_refine",
        "documentation": {}
    },
    {
        "label": "docs",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_refine",
        "description": "python_use.agent.split.summary_refine",
        "peekOfCode": "docs = loader.load_and_split(text_splitter)\n# 初始化模型\nllm = ChatOpenAI(model=\"qwen:7b\", base_url=\"http://127.0.0.1:10087/v1\", api_key=\"vllm\")\n# 修正后的提示模板\nquestion_prompt = PromptTemplate(\n    template=\"根据新内容完善总结：{text}\",\n    input_variables=[\"text\"]\n)\nrefine_prompt = PromptTemplate(\n    template=\"当前总结：{existing_answer}\\n新增内容：{text}\\n生成更新后的总结：\",",
        "detail": "python_use.agent.split.summary_refine",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_refine",
        "description": "python_use.agent.split.summary_refine",
        "peekOfCode": "llm = ChatOpenAI(model=\"qwen:7b\", base_url=\"http://127.0.0.1:10087/v1\", api_key=\"vllm\")\n# 修正后的提示模板\nquestion_prompt = PromptTemplate(\n    template=\"根据新内容完善总结：{text}\",\n    input_variables=[\"text\"]\n)\nrefine_prompt = PromptTemplate(\n    template=\"当前总结：{existing_answer}\\n新增内容：{text}\\n生成更新后的总结：\",\n    input_variables=[\"existing_answer\", \"text\"]\n)",
        "detail": "python_use.agent.split.summary_refine",
        "documentation": {}
    },
    {
        "label": "question_prompt",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_refine",
        "description": "python_use.agent.split.summary_refine",
        "peekOfCode": "question_prompt = PromptTemplate(\n    template=\"根据新内容完善总结：{text}\",\n    input_variables=[\"text\"]\n)\nrefine_prompt = PromptTemplate(\n    template=\"当前总结：{existing_answer}\\n新增内容：{text}\\n生成更新后的总结：\",\n    input_variables=[\"existing_answer\", \"text\"]\n)\n# 创建链式处理器\nrefine_chain = load_summarize_chain(",
        "detail": "python_use.agent.split.summary_refine",
        "documentation": {}
    },
    {
        "label": "refine_prompt",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_refine",
        "description": "python_use.agent.split.summary_refine",
        "peekOfCode": "refine_prompt = PromptTemplate(\n    template=\"当前总结：{existing_answer}\\n新增内容：{text}\\n生成更新后的总结：\",\n    input_variables=[\"existing_answer\", \"text\"]\n)\n# 创建链式处理器\nrefine_chain = load_summarize_chain(\n    llm,\n    chain_type=\"refine\",\n    verbose=True,\n    question_prompt=question_prompt,",
        "detail": "python_use.agent.split.summary_refine",
        "documentation": {}
    },
    {
        "label": "refine_chain",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_refine",
        "description": "python_use.agent.split.summary_refine",
        "peekOfCode": "refine_chain = load_summarize_chain(\n    llm,\n    chain_type=\"refine\",\n    verbose=True,\n    question_prompt=question_prompt,\n    refine_prompt=refine_prompt\n)\n# 正确调用方式\ninputs = {\n    \"input_documents\": docs[:3],",
        "detail": "python_use.agent.split.summary_refine",
        "documentation": {}
    },
    {
        "label": "inputs",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_refine",
        "description": "python_use.agent.split.summary_refine",
        "peekOfCode": "inputs = {\n    \"input_documents\": docs[:3],\n    \"question\": \"请总结以下内容\"  # 自定义问题（可选）\n}\nrefined_summary = refine_chain.invoke(inputs)\nprint(refined_summary)",
        "detail": "python_use.agent.split.summary_refine",
        "documentation": {}
    },
    {
        "label": "refined_summary",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_refine",
        "description": "python_use.agent.split.summary_refine",
        "peekOfCode": "refined_summary = refine_chain.invoke(inputs)\nprint(refined_summary)",
        "detail": "python_use.agent.split.summary_refine",
        "documentation": {}
    },
    {
        "label": "loader",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_stuff",
        "description": "python_use.agent.split.summary_stuff",
        "peekOfCode": "loader = TextLoader(\"./chineseJH.txt\", encoding=\"utf-8\")\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\ndocs = loader.load_and_split(text_splitter)\n# 初始化模型\nllm = ChatOpenAI(model=\"qwen:7b\", base_url=\"http://127.0.0.1:10087/v1\", api_key=\"vllm\")\n# 创建链式处理器\nchain = load_summarize_chain(\n    llm,\n    verbose=True,\n)",
        "detail": "python_use.agent.split.summary_stuff",
        "documentation": {}
    },
    {
        "label": "text_splitter",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_stuff",
        "description": "python_use.agent.split.summary_stuff",
        "peekOfCode": "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\ndocs = loader.load_and_split(text_splitter)\n# 初始化模型\nllm = ChatOpenAI(model=\"qwen:7b\", base_url=\"http://127.0.0.1:10087/v1\", api_key=\"vllm\")\n# 创建链式处理器\nchain = load_summarize_chain(\n    llm,\n    verbose=True,\n)\n# 执行总结（处理前5块）",
        "detail": "python_use.agent.split.summary_stuff",
        "documentation": {}
    },
    {
        "label": "docs",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_stuff",
        "description": "python_use.agent.split.summary_stuff",
        "peekOfCode": "docs = loader.load_and_split(text_splitter)\n# 初始化模型\nllm = ChatOpenAI(model=\"qwen:7b\", base_url=\"http://127.0.0.1:10087/v1\", api_key=\"vllm\")\n# 创建链式处理器\nchain = load_summarize_chain(\n    llm,\n    verbose=True,\n)\n# 执行总结（处理前5块）\nresult = chain.invoke(docs[:2])",
        "detail": "python_use.agent.split.summary_stuff",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_stuff",
        "description": "python_use.agent.split.summary_stuff",
        "peekOfCode": "llm = ChatOpenAI(model=\"qwen:7b\", base_url=\"http://127.0.0.1:10087/v1\", api_key=\"vllm\")\n# 创建链式处理器\nchain = load_summarize_chain(\n    llm,\n    verbose=True,\n)\n# 执行总结（处理前5块）\nresult = chain.invoke(docs[:2])\nprint(result)\nprint(\"===\"*50)",
        "detail": "python_use.agent.split.summary_stuff",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_stuff",
        "description": "python_use.agent.split.summary_stuff",
        "peekOfCode": "chain = load_summarize_chain(\n    llm,\n    verbose=True,\n)\n# 执行总结（处理前5块）\nresult = chain.invoke(docs[:2])\nprint(result)\nprint(\"===\"*50)\nprint(docs[:2])",
        "detail": "python_use.agent.split.summary_stuff",
        "documentation": {}
    },
    {
        "label": "result",
        "kind": 5,
        "importPath": "python_use.agent.split.summary_stuff",
        "description": "python_use.agent.split.summary_stuff",
        "peekOfCode": "result = chain.invoke(docs[:2])\nprint(result)\nprint(\"===\"*50)\nprint(docs[:2])",
        "detail": "python_use.agent.split.summary_stuff",
        "documentation": {}
    },
    {
        "label": "python_execute_tool",
        "kind": 2,
        "importPath": "python_use.agent.tools.code_run",
        "description": "python_use.agent.tools.code_run",
        "peekOfCode": "def python_execute_tool(code: str):\n    \"\"\"安全执行Python代码并捕获输出的工具\"\"\"\n    repl = PythonREPL()\n    try:\n        # 执行代码并获取输出\n        output = repl.run(code)\n        return {\n            \"status\": \"success\",\n            \"output\": output,\n            \"error\": None",
        "detail": "python_use.agent.tools.code_run",
        "documentation": {}
    },
    {
        "label": "ds",
        "kind": 5,
        "importPath": "python_use.eval.gaia",
        "description": "python_use.eval.gaia",
        "peekOfCode": "ds = load_dataset(\"gaia-benchmark/GAIA\", '2023_all', cache_dir=\"./cache\", trust_remote_code=True)\n# 查看数据集结构\nprint(ds['validation'][0])  # 验证集含答案，用于本地调试\nprint(ds['test'][0])        # 测试集无答案，仅提交结果",
        "detail": "python_use.eval.gaia",
        "documentation": {}
    },
    {
        "label": "getBg",
        "kind": 2,
        "importPath": "python_use.import_use.src.chatbot.bg.bg",
        "description": "python_use.import_use.src.chatbot.bg.bg",
        "peekOfCode": "def getBg():\n    print(\"bg\")",
        "detail": "python_use.import_use.src.chatbot.bg.bg",
        "documentation": {}
    },
    {
        "label": "getModel",
        "kind": 2,
        "importPath": "python_use.import_use.src.chatbot.core.load_model.get_model",
        "description": "python_use.import_use.src.chatbot.core.load_model.get_model",
        "peekOfCode": "def getModel():\n    print(\"getmodel\")\n    return \"Model\"",
        "detail": "python_use.import_use.src.chatbot.core.load_model.get_model",
        "documentation": {}
    },
    {
        "label": "Neo4j_rag",
        "kind": 6,
        "importPath": "python_use.import_use.src.chatbot.core.rag.neo4j_rag",
        "description": "python_use.import_use.src.chatbot.core.rag.neo4j_rag",
        "peekOfCode": "class Neo4j_rag:\n    def __init__(self):\n        getModel()\n        getBg()\n        fg()\n        pass\n    def process(self):\n        return True\nif __name__ == \"__main__\":\n    neo4j_processor = Neo4j_rag()",
        "detail": "python_use.import_use.src.chatbot.core.rag.neo4j_rag",
        "documentation": {}
    },
    {
        "label": "Rag",
        "kind": 6,
        "importPath": "python_use.import_use.src.chatbot.core.rag.rag",
        "description": "python_use.import_use.src.chatbot.core.rag.rag",
        "peekOfCode": "class Rag:\n    def __init__(self):\n        pass\n    def getItem(self):\n        return True",
        "detail": "python_use.import_use.src.chatbot.core.rag.rag",
        "documentation": {}
    },
    {
        "label": "fg",
        "kind": 2,
        "importPath": "python_use.import_use.src.chatbot.fg.fg",
        "description": "python_use.import_use.src.chatbot.fg.fg",
        "peekOfCode": "def fg():\n    print(\"fg\")",
        "detail": "python_use.import_use.src.chatbot.fg.fg",
        "documentation": {}
    },
    {
        "label": "MILITORY_PROMPT",
        "kind": 5,
        "importPath": "python_use.import_use.src.chatbot.utils.prompts.militory",
        "description": "python_use.import_use.src.chatbot.utils.prompts.militory",
        "peekOfCode": "MILITORY_PROMPT = \"militory_prompt\"\nprint(f\"load_model2 {getModel()}\")",
        "detail": "python_use.import_use.src.chatbot.utils.prompts.militory",
        "documentation": {}
    },
    {
        "label": "cal",
        "kind": 2,
        "importPath": "python_use.import_use.src.chatbot.utils.utils",
        "description": "python_use.import_use.src.chatbot.utils.utils",
        "peekOfCode": "def cal():\n    return 1 * 3",
        "detail": "python_use.import_use.src.chatbot.utils.utils",
        "documentation": {}
    },
    {
        "label": "Ui_loadwin",
        "kind": 6,
        "importPath": "python_use.pyqt_use.loading_screen.loadwin_ui",
        "description": "python_use.pyqt_use.loading_screen.loadwin_ui",
        "peekOfCode": "class Ui_loadwin(object):\n    def setupUi(self, loadwin):\n        loadwin.setObjectName(\"loadwin\")\n        # loadwin.resize(455, 349)\n        self.gridLayout = QtWidgets.QGridLayout(loadwin)\n        self.gridLayout.setObjectName(\"gridLayout\")\n        # # 在布局初始化后添加\n        # self.gridLayout.setRowStretch(0, 1)  # 图片区域占用剩余空间\n        # self.gridLayout.setRowStretch(1, 0)  # 信息标签固定高度\n        # self.gridLayout.setRowStretch(2, 0)  # 进度条固定高度",
        "detail": "python_use.pyqt_use.loading_screen.loadwin_ui",
        "documentation": {}
    },
    {
        "label": "LoadWin",
        "kind": 6,
        "importPath": "python_use.pyqt_use.loading_screen.main",
        "description": "python_use.pyqt_use.loading_screen.main",
        "peekOfCode": "class LoadWin(QWidget, Ui_loadwin):  # 启动画面类\n    def __init__(self):\n        super(LoadWin, self).__init__()\n        self.setupUi(self)\n        self.setWindowTitle(\"加载界面\")\n        screen = QDesktopWidget().screenGeometry()\n        # 设置窗口为屏幕的60%宽、40%高\n        self.resize(int(screen.width() * 0.5), int(screen.height() * 0.6))\n        # 保持宽高比缩放（根据实际需求选择）\n        self.setMinimumSize(400, 300)  # 最小尺寸限制",
        "detail": "python_use.pyqt_use.loading_screen.main",
        "documentation": {}
    },
    {
        "label": "LoadThread",
        "kind": 6,
        "importPath": "python_use.pyqt_use.loading_screen.main",
        "description": "python_use.pyqt_use.loading_screen.main",
        "peekOfCode": "class LoadThread(QThread):  # 自定义计算线程类 -----------\n    part_signal = pyqtSignal(int)  # 进度信号\n    data_signal = pyqtSignal(str)  # 数据传输信号\n    show_signal = pyqtSignal()\n    def __init__(self):\n        super().__init__()\n        self.ret = None\n    # 启动线程\n    def run(self):\n        # 该消息发送后，启用process_set_part函数",
        "detail": "python_use.pyqt_use.loading_screen.main",
        "documentation": {}
    },
    {
        "label": "MainWin",
        "kind": 6,
        "importPath": "python_use.pyqt_use.loading_screen.main",
        "description": "python_use.pyqt_use.loading_screen.main",
        "peekOfCode": "class MainWin(QWidget, Ui_mainwin):  # 主页界面类 -----------\n    def __init__(self):\n        super(MainWin, self).__init__()\n        self.setupUi(self)\n        self.setWindowTitle(\"主界面\")\n    def set_data(self, mes=\"xxxxx\"):\n        self.lineEdit.setText(mes)\n# 显示主界面\ndef show_main_win():\n    w.close()",
        "detail": "python_use.pyqt_use.loading_screen.main",
        "documentation": {}
    },
    {
        "label": "show_main_win",
        "kind": 2,
        "importPath": "python_use.pyqt_use.loading_screen.main",
        "description": "python_use.pyqt_use.loading_screen.main",
        "peekOfCode": "def show_main_win():\n    w.close()\n    zhu.set_data(\"xxxxx\")\n    zhu.show()\nif __name__ == \"__main__\":\n    app = QApplication(sys.argv)\n    w = LoadWin()\n    zhu = MainWin()\n    w.show()\n    sys.exit(app.exec())",
        "detail": "python_use.pyqt_use.loading_screen.main",
        "documentation": {}
    },
    {
        "label": "BASE_DIR",
        "kind": 5,
        "importPath": "python_use.pyqt_use.loading_screen.main",
        "description": "python_use.pyqt_use.loading_screen.main",
        "peekOfCode": "BASE_DIR = os.getenv(\"BASE_DIR\")\nclass LoadWin(QWidget, Ui_loadwin):  # 启动画面类\n    def __init__(self):\n        super(LoadWin, self).__init__()\n        self.setupUi(self)\n        self.setWindowTitle(\"加载界面\")\n        screen = QDesktopWidget().screenGeometry()\n        # 设置窗口为屏幕的60%宽、40%高\n        self.resize(int(screen.width() * 0.5), int(screen.height() * 0.6))\n        # 保持宽高比缩放（根据实际需求选择）",
        "detail": "python_use.pyqt_use.loading_screen.main",
        "documentation": {}
    },
    {
        "label": "Ui_mainwin",
        "kind": 6,
        "importPath": "python_use.pyqt_use.loading_screen.Ui_mainwin",
        "description": "python_use.pyqt_use.loading_screen.Ui_mainwin",
        "peekOfCode": "class Ui_mainwin(object):\n    def setupUi(self, mainwin):\n        mainwin.setObjectName(\"mainwin\")\n        mainwin.resize(609, 432)\n        self.calendarWidget = QtWidgets.QCalendarWidget(mainwin)\n        self.calendarWidget.setGeometry(QtCore.QRect(170, 80, 248, 197))\n        self.calendarWidget.setObjectName(\"calendarWidget\")\n        self.lineEdit = QtWidgets.QLineEdit(mainwin)\n        self.lineEdit.setGeometry(QtCore.QRect(180, 320, 231, 21))\n        self.lineEdit.setObjectName(\"lineEdit\")",
        "detail": "python_use.pyqt_use.loading_screen.Ui_mainwin",
        "documentation": {}
    },
    {
        "label": "your_llm_app",
        "kind": 2,
        "importPath": "python_use.rag.llamindex_use.eval.test_deep_eval",
        "description": "python_use.rag.llamindex_use.eval.test_deep_eval",
        "peekOfCode": "def your_llm_app(input: str):\n    response = \"\"\n    streaming_response = rag_chat.stream_chat(input)\n    for token in streaming_response.response_gen:  # 正确访问响应生成器[2](@ref)\n        print(token, end=\"\", flush=True)  # 实时输出\n        response += token\n    return response\n# 配置日志\nlogging.basicConfig(\n    level=logging.INFO,",
        "detail": "python_use.rag.llamindex_use.eval.test_deep_eval",
        "documentation": {}
    },
    {
        "label": "test_llm_app",
        "kind": 2,
        "importPath": "python_use.rag.llamindex_use.eval.test_deep_eval",
        "description": "python_use.rag.llamindex_use.eval.test_deep_eval",
        "peekOfCode": "def test_llm_app(golden: Golden):\n    logging.info(f\"🚀 开始测试用例: {golden.input}\")\n    # 执行LLM调用\n    response = your_llm_app(golden.input)  \n    logging.info(f\"✅ LLM响应: {response[:100]}...\")  # 记录前100字符\n    test_case = LLMTestCase(\n        input=golden.input, \n        actual_output=response\n    )\n    # 执行评估",
        "detail": "python_use.rag.llamindex_use.eval.test_deep_eval",
        "documentation": {}
    },
    {
        "label": "Settings.llm",
        "kind": 5,
        "importPath": "python_use.rag.llamindex_use.eval.test_deep_eval",
        "description": "python_use.rag.llamindex_use.eval.test_deep_eval",
        "peekOfCode": "Settings.llm = Ollama(model=\"qwen2.5:7b\", request_timeout=360.0)\n# 设置嵌入模型（如 nomic-embed-text）\nSettings.embed_model = OllamaEmbedding(model_name=\"bge-m3\")\nstorage_context = StorageContext.from_defaults(persist_dir=\"storage\")\nindex = load_index_from_storage(\n    storage_context,\n    # we can optionally override the embed_model here\n    # it's important to use the same embed_model as the one used to build the index\n)\neval_model = OllamaModel(model=\"qwen2.5:32b\")",
        "detail": "python_use.rag.llamindex_use.eval.test_deep_eval",
        "documentation": {}
    },
    {
        "label": "Settings.embed_model",
        "kind": 5,
        "importPath": "python_use.rag.llamindex_use.eval.test_deep_eval",
        "description": "python_use.rag.llamindex_use.eval.test_deep_eval",
        "peekOfCode": "Settings.embed_model = OllamaEmbedding(model_name=\"bge-m3\")\nstorage_context = StorageContext.from_defaults(persist_dir=\"storage\")\nindex = load_index_from_storage(\n    storage_context,\n    # we can optionally override the embed_model here\n    # it's important to use the same embed_model as the one used to build the index\n)\neval_model = OllamaModel(model=\"qwen2.5:32b\")\ndataset = EvaluationDataset()\ndataset.pull(alias=\"af\")",
        "detail": "python_use.rag.llamindex_use.eval.test_deep_eval",
        "documentation": {}
    },
    {
        "label": "storage_context",
        "kind": 5,
        "importPath": "python_use.rag.llamindex_use.eval.test_deep_eval",
        "description": "python_use.rag.llamindex_use.eval.test_deep_eval",
        "peekOfCode": "storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\nindex = load_index_from_storage(\n    storage_context,\n    # we can optionally override the embed_model here\n    # it's important to use the same embed_model as the one used to build the index\n)\neval_model = OllamaModel(model=\"qwen2.5:32b\")\ndataset = EvaluationDataset()\ndataset.pull(alias=\"af\")\nrag_chat = index.as_chat_engine()",
        "detail": "python_use.rag.llamindex_use.eval.test_deep_eval",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 5,
        "importPath": "python_use.rag.llamindex_use.eval.test_deep_eval",
        "description": "python_use.rag.llamindex_use.eval.test_deep_eval",
        "peekOfCode": "index = load_index_from_storage(\n    storage_context,\n    # we can optionally override the embed_model here\n    # it's important to use the same embed_model as the one used to build the index\n)\neval_model = OllamaModel(model=\"qwen2.5:32b\")\ndataset = EvaluationDataset()\ndataset.pull(alias=\"af\")\nrag_chat = index.as_chat_engine()\ndef your_llm_app(input: str):",
        "detail": "python_use.rag.llamindex_use.eval.test_deep_eval",
        "documentation": {}
    },
    {
        "label": "eval_model",
        "kind": 5,
        "importPath": "python_use.rag.llamindex_use.eval.test_deep_eval",
        "description": "python_use.rag.llamindex_use.eval.test_deep_eval",
        "peekOfCode": "eval_model = OllamaModel(model=\"qwen2.5:32b\")\ndataset = EvaluationDataset()\ndataset.pull(alias=\"af\")\nrag_chat = index.as_chat_engine()\ndef your_llm_app(input: str):\n    response = \"\"\n    streaming_response = rag_chat.stream_chat(input)\n    for token in streaming_response.response_gen:  # 正确访问响应生成器[2](@ref)\n        print(token, end=\"\", flush=True)  # 实时输出\n        response += token",
        "detail": "python_use.rag.llamindex_use.eval.test_deep_eval",
        "documentation": {}
    },
    {
        "label": "dataset",
        "kind": 5,
        "importPath": "python_use.rag.llamindex_use.eval.test_deep_eval",
        "description": "python_use.rag.llamindex_use.eval.test_deep_eval",
        "peekOfCode": "dataset = EvaluationDataset()\ndataset.pull(alias=\"af\")\nrag_chat = index.as_chat_engine()\ndef your_llm_app(input: str):\n    response = \"\"\n    streaming_response = rag_chat.stream_chat(input)\n    for token in streaming_response.response_gen:  # 正确访问响应生成器[2](@ref)\n        print(token, end=\"\", flush=True)  # 实时输出\n        response += token\n    return response",
        "detail": "python_use.rag.llamindex_use.eval.test_deep_eval",
        "documentation": {}
    },
    {
        "label": "rag_chat",
        "kind": 5,
        "importPath": "python_use.rag.llamindex_use.eval.test_deep_eval",
        "description": "python_use.rag.llamindex_use.eval.test_deep_eval",
        "peekOfCode": "rag_chat = index.as_chat_engine()\ndef your_llm_app(input: str):\n    response = \"\"\n    streaming_response = rag_chat.stream_chat(input)\n    for token in streaming_response.response_gen:  # 正确访问响应生成器[2](@ref)\n        print(token, end=\"\", flush=True)  # 实时输出\n        response += token\n    return response\n# 配置日志\nlogging.basicConfig(",
        "detail": "python_use.rag.llamindex_use.eval.test_deep_eval",
        "documentation": {}
    },
    {
        "label": "Response",
        "kind": 6,
        "importPath": "python_use.rag.llamindex_use.load_data.extractor",
        "description": "python_use.rag.llamindex_use.load_data.extractor",
        "peekOfCode": "class Response(BaseModel):\n    \"\"\"The Response of the question\"\"\"\n    answer: int = Field(\n        description=\"The answer of the math calculation\"\n    )\ncustom_llm = Ollama(\n    model=\"qwen2.5:7b\",\n    base_url=\"http://localhost:11434\",  # 明确指定本地地址\n    request_timeout=60.0  # 延长超时时间\n)",
        "detail": "python_use.rag.llamindex_use.load_data.extractor",
        "documentation": {}
    },
    {
        "label": "custom_llm",
        "kind": 5,
        "importPath": "python_use.rag.llamindex_use.load_data.extractor",
        "description": "python_use.rag.llamindex_use.load_data.extractor",
        "peekOfCode": "custom_llm = Ollama(\n    model=\"qwen2.5:7b\",\n    base_url=\"http://localhost:11434\",  # 明确指定本地地址\n    request_timeout=60.0  # 延长超时时间\n)\nstructured_llm = custom_llm.as_structured_llm(Response)\nres = structured_llm.chat(messages=[ChatMessage(content=\"计算12+33=?\")])\npprint(res.raw)",
        "detail": "python_use.rag.llamindex_use.load_data.extractor",
        "documentation": {}
    },
    {
        "label": "structured_llm",
        "kind": 5,
        "importPath": "python_use.rag.llamindex_use.load_data.extractor",
        "description": "python_use.rag.llamindex_use.load_data.extractor",
        "peekOfCode": "structured_llm = custom_llm.as_structured_llm(Response)\nres = structured_llm.chat(messages=[ChatMessage(content=\"计算12+33=?\")])\npprint(res.raw)",
        "detail": "python_use.rag.llamindex_use.load_data.extractor",
        "documentation": {}
    },
    {
        "label": "res",
        "kind": 5,
        "importPath": "python_use.rag.llamindex_use.load_data.extractor",
        "description": "python_use.rag.llamindex_use.load_data.extractor",
        "peekOfCode": "res = structured_llm.chat(messages=[ChatMessage(content=\"计算12+33=?\")])\npprint(res.raw)",
        "detail": "python_use.rag.llamindex_use.load_data.extractor",
        "documentation": {}
    },
    {
        "label": "image_reader",
        "kind": 5,
        "importPath": "python_use.rag.llamindex_use.load_data.extractor_img",
        "description": "python_use.rag.llamindex_use.load_data.extractor_img",
        "peekOfCode": "image_reader = ImageReader(parse_text=True) \nreader = SimpleDirectoryReader(\n    input_dir=\"imgs/\",\n    file_extractor={\".png\": image_reader}\n)\ninput_files = reader.load_data()\nprint(input_files[0].image)",
        "detail": "python_use.rag.llamindex_use.load_data.extractor_img",
        "documentation": {}
    },
    {
        "label": "reader",
        "kind": 5,
        "importPath": "python_use.rag.llamindex_use.load_data.extractor_img",
        "description": "python_use.rag.llamindex_use.load_data.extractor_img",
        "peekOfCode": "reader = SimpleDirectoryReader(\n    input_dir=\"imgs/\",\n    file_extractor={\".png\": image_reader}\n)\ninput_files = reader.load_data()\nprint(input_files[0].image)",
        "detail": "python_use.rag.llamindex_use.load_data.extractor_img",
        "documentation": {}
    },
    {
        "label": "input_files",
        "kind": 5,
        "importPath": "python_use.rag.llamindex_use.load_data.extractor_img",
        "description": "python_use.rag.llamindex_use.load_data.extractor_img",
        "peekOfCode": "input_files = reader.load_data()\nprint(input_files[0].image)",
        "detail": "python_use.rag.llamindex_use.load_data.extractor_img",
        "documentation": {}
    },
    {
        "label": "split_documents",
        "kind": 2,
        "importPath": "python_use.rag.text_split.base",
        "description": "python_use.rag.text_split.base",
        "peekOfCode": "def split_documents(raw_documents, chunk_size=256, chunk_overlap=24):\n    \"\"\"使用文本分割器处理文档\"\"\"\n    if not raw_documents:\n        raise ValueError(\"没有可分割的文档内容\")\n    print(\"正在进行文档分割...\")\n    text_splitter = TokenTextSplitter(\n        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n    )\n    chunks = text_splitter.split_documents(raw_documents)\n    print(f\"已将文档页面分割成 {len(chunks)} 个片段。\")",
        "detail": "python_use.rag.text_split.base",
        "documentation": {}
    },
    {
        "label": "build",
        "kind": 2,
        "importPath": "python_use.rag.text_split.build_chromadb",
        "description": "python_use.rag.text_split.build_chromadb",
        "peekOfCode": "def build(self, folder):\n    try:\n        new_docs = load_pdf_documents(folder)\n        vector_store = self.client.get_or_create_collection(\n            name=CHROMA_KEY, embedding_function=self.embedding\n        )\n        vector_store.add(\n            documents=[doc.page_content for doc in new_docs],\n            metadatas=[doc.metadata for doc in new_docs],\n            ids=[f\"doc_{i}\" for i in range(len(new_docs))],",
        "detail": "python_use.rag.text_split.build_chromadb",
        "documentation": {}
    },
    {
        "label": "CHROMA_KEY",
        "kind": 5,
        "importPath": "python_use.rag.text_split.build_chromadb",
        "description": "python_use.rag.text_split.build_chromadb",
        "peekOfCode": "CHROMA_KEY = \"langchain\"\ndef build(self, folder):\n    try:\n        new_docs = load_pdf_documents(folder)\n        vector_store = self.client.get_or_create_collection(\n            name=CHROMA_KEY, embedding_function=self.embedding\n        )\n        vector_store.add(\n            documents=[doc.page_content for doc in new_docs],\n            metadatas=[doc.metadata for doc in new_docs],",
        "detail": "python_use.rag.text_split.build_chromadb",
        "documentation": {}
    },
    {
        "label": "load_pdf_documents",
        "kind": 2,
        "importPath": "python_use.rag.text_split.load_file",
        "description": "python_use.rag.text_split.load_file",
        "peekOfCode": "def load_pdf_documents(pdf_folder):\n    \"\"\"从指定文件夹加载PDF文档\"\"\"\n    raw_documents = []\n    if not os.path.isdir(pdf_folder):\n        raise FileNotFoundError(f\"指定的 PDF 文件夹不存在: {pdf_folder}\")\n    pdf_files = glob.glob(os.path.join(pdf_folder, \"*.pdf\"), recursive=True)\n    if not pdf_files:\n        print(f\"警告：在文件夹 {pdf_folder} 中未找到 PDF 文件。\")\n        return []\n    print(f\"找到 {len(pdf_files)} 个 PDF 文件。正在加载...\")",
        "detail": "python_use.rag.text_split.load_file",
        "documentation": {}
    },
    {
        "label": "text_splitter",
        "kind": 5,
        "importPath": "python_use.rag.text_split.split",
        "description": "python_use.rag.text_split.split",
        "peekOfCode": "text_splitter = RecursiveCharacterTextSplitter(\n    separators=[\n        \"\\n\\n\",        # 空行（最高优先级）\n        \"。\", \"!\", \"?\",  # 中文句末标点\n        \"\\n\",          # 换行符\n        \"；\", \";\",      # 分号\n        \" \", \"\",        # 空格和空字符（最低优先级）\n    ],\n    chunk_size=800,     # 增大块大小适应中文（约400-500汉字）\n    chunk_overlap=150,  # 设置15%的重叠比例",
        "detail": "python_use.rag.text_split.split",
        "documentation": {}
    },
    {
        "label": "raw_docs",
        "kind": 5,
        "importPath": "python_use.rag.text_split.split",
        "description": "python_use.rag.text_split.split",
        "peekOfCode": "raw_docs = load_pdf_documents(\"./pdfs\")\nprint(\"\\n\" + \"*\" * 50 + \" 原始文档信息 \" + \"*\" * 50)\nprint(f\"载入PDF文档数：{len(raw_docs)}\")\nprint(f\"首个文档元数据：{raw_docs[0].metadata}\\n\")\n# 执行分块处理\nchunks = text_splitter.split_documents(raw_docs)\nprint(\"\\n\" + \"=\" * 50 + \" 分块统计信息 \" + \"=\" * 50)\nprint(f\"总块数：{len(chunks)}\")\nprint(f\"平均块长度：{sum(len(c.page_content) for c in chunks)//len(chunks)}字符\")\nprint(f\"最大块长度：{max(len(c.page_content) for c in chunks)}字符\")",
        "detail": "python_use.rag.text_split.split",
        "documentation": {}
    },
    {
        "label": "chunks",
        "kind": 5,
        "importPath": "python_use.rag.text_split.split",
        "description": "python_use.rag.text_split.split",
        "peekOfCode": "chunks = text_splitter.split_documents(raw_docs)\nprint(\"\\n\" + \"=\" * 50 + \" 分块统计信息 \" + \"=\" * 50)\nprint(f\"总块数：{len(chunks)}\")\nprint(f\"平均块长度：{sum(len(c.page_content) for c in chunks)//len(chunks)}字符\")\nprint(f\"最大块长度：{max(len(c.page_content) for c in chunks)}字符\")\nprint(f\"最小块长度：{min(len(c.page_content) for c in chunks)}字符\\n\")\n# 详细分块信息输出\nprint(\"=\" * 50 + \" 分块详细信息 \" + \"=\" * 50)\nfor i, chunk in enumerate(chunks[:3]):  # 示例显示前3个分块\n    metadata = chunk.metadata",
        "detail": "python_use.rag.text_split.split",
        "documentation": {}
    },
    {
        "label": "get_session_history",
        "kind": 2,
        "importPath": "python_use.role_bot.src.history",
        "description": "python_use.role_bot.src.history",
        "peekOfCode": "def get_session_history(session_id: str) -> ChatMessageHistory:\n    if session_id not in store:\n        store[session_id] = ChatMessageHistory()\n    return store[session_id]\n# 3. 包装可运行对象（保持不变）\nrunnable = RunnableWithMessageHistory(\n    chain,\n    get_session_history,\n    input_messages_key=\"input\",\n    history_messages_key=\"history\"",
        "detail": "python_use.role_bot.src.history",
        "documentation": {}
    },
    {
        "label": "respond",
        "kind": 2,
        "importPath": "python_use.role_bot.src.history",
        "description": "python_use.role_bot.src.history",
        "peekOfCode": "def respond(message: str, history: list):\n    session_id = \"default_session\"\n    response = \"\"\n    for chunk in runnable.stream(\n        {\"input\": message},\n        config={\"configurable\": {\"session_id\": session_id}}\n    ):\n        response += chunk.content\n        yield [{\"role\": \"user\", \"content\": message},\n               {\"role\": \"assistant\", \"content\": response}]",
        "detail": "python_use.role_bot.src.history",
        "documentation": {}
    },
    {
        "label": "get_history_blocks",
        "kind": 2,
        "importPath": "python_use.role_bot.src.history",
        "description": "python_use.role_bot.src.history",
        "peekOfCode": "def get_history_blocks(expand_states=None):\n    session_id = \"default_session\"\n    history_obj = get_session_history(session_id)\n    messages = history_obj.messages\n    if not messages:\n        return [gr.Markdown(\"暂无历史记录。\")]\n    blocks = []\n    if expand_states is None or len(expand_states) != len(messages):\n        expand_states = [False] * len(messages)\n    for i, msg in enumerate(messages):",
        "detail": "python_use.role_bot.src.history",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "python_use.role_bot.src.history",
        "description": "python_use.role_bot.src.history",
        "peekOfCode": "llm = ChatOllama(model=\"Qwen2.5-7B-Instruct:latest\")\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"你是一个AI助手\"),\n    MessagesPlaceholder(variable_name=\"history\"),\n    (\"human\", \"{input}\")\n])\nchain = prompt | llm\n# 2. 初始化消息历史管理（保持不变）\nstore = {}\ndef get_session_history(session_id: str) -> ChatMessageHistory:",
        "detail": "python_use.role_bot.src.history",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "python_use.role_bot.src.history",
        "description": "python_use.role_bot.src.history",
        "peekOfCode": "prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"你是一个AI助手\"),\n    MessagesPlaceholder(variable_name=\"history\"),\n    (\"human\", \"{input}\")\n])\nchain = prompt | llm\n# 2. 初始化消息历史管理（保持不变）\nstore = {}\ndef get_session_history(session_id: str) -> ChatMessageHistory:\n    if session_id not in store:",
        "detail": "python_use.role_bot.src.history",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "python_use.role_bot.src.history",
        "description": "python_use.role_bot.src.history",
        "peekOfCode": "chain = prompt | llm\n# 2. 初始化消息历史管理（保持不变）\nstore = {}\ndef get_session_history(session_id: str) -> ChatMessageHistory:\n    if session_id not in store:\n        store[session_id] = ChatMessageHistory()\n    return store[session_id]\n# 3. 包装可运行对象（保持不变）\nrunnable = RunnableWithMessageHistory(\n    chain,",
        "detail": "python_use.role_bot.src.history",
        "documentation": {}
    },
    {
        "label": "store",
        "kind": 5,
        "importPath": "python_use.role_bot.src.history",
        "description": "python_use.role_bot.src.history",
        "peekOfCode": "store = {}\ndef get_session_history(session_id: str) -> ChatMessageHistory:\n    if session_id not in store:\n        store[session_id] = ChatMessageHistory()\n    return store[session_id]\n# 3. 包装可运行对象（保持不变）\nrunnable = RunnableWithMessageHistory(\n    chain,\n    get_session_history,\n    input_messages_key=\"input\",",
        "detail": "python_use.role_bot.src.history",
        "documentation": {}
    },
    {
        "label": "runnable",
        "kind": 5,
        "importPath": "python_use.role_bot.src.history",
        "description": "python_use.role_bot.src.history",
        "peekOfCode": "runnable = RunnableWithMessageHistory(\n    chain,\n    get_session_history,\n    input_messages_key=\"input\",\n    history_messages_key=\"history\"\n)\n# 4. 响应生成函数（保持不变）\ndef respond(message: str, history: list):\n    session_id = \"default_session\"\n    response = \"\"",
        "detail": "python_use.role_bot.src.history",
        "documentation": {}
    },
    {
        "label": "get_session_history",
        "kind": 2,
        "importPath": "python_use.role_bot.src.main",
        "description": "python_use.role_bot.src.main",
        "peekOfCode": "def get_session_history(session_id: str) -> ChatMessageHistory:\n    if session_id not in store:\n        store[session_id] = ChatMessageHistory()\n    return store[session_id]\n# 3. 包装可运行对象（保持不变）\nrunnable = RunnableWithMessageHistory(\n    chain,\n    get_session_history,\n    input_messages_key=\"input\",\n    history_messages_key=\"history\"",
        "detail": "python_use.role_bot.src.main",
        "documentation": {}
    },
    {
        "label": "respond",
        "kind": 2,
        "importPath": "python_use.role_bot.src.main",
        "description": "python_use.role_bot.src.main",
        "peekOfCode": "def respond(message: str, history: list):\n    session_id = \"default_session\"\n    response = \"\"\n    for chunk in runnable.stream(\n        {\"input\": message},\n        config={\"configurable\": {\"session_id\": session_id}}\n    ):\n        response += chunk.content\n        yield [{\"role\": \"user\", \"content\": message},\n               {\"role\": \"assistant\", \"content\": response}]",
        "detail": "python_use.role_bot.src.main",
        "documentation": {}
    },
    {
        "label": "llm",
        "kind": 5,
        "importPath": "python_use.role_bot.src.main",
        "description": "python_use.role_bot.src.main",
        "peekOfCode": "llm = ChatOllama(model=\"Qwen2.5-7B-Instruct:latest\")\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"你是一个AI助手\"),\n    MessagesPlaceholder(variable_name=\"history\"),\n    (\"human\", \"{input}\")\n])\nchain = prompt | llm\n# 2. 初始化消息历史管理（保持不变）\nstore = {}\ndef get_session_history(session_id: str) -> ChatMessageHistory:",
        "detail": "python_use.role_bot.src.main",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "python_use.role_bot.src.main",
        "description": "python_use.role_bot.src.main",
        "peekOfCode": "prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"你是一个AI助手\"),\n    MessagesPlaceholder(variable_name=\"history\"),\n    (\"human\", \"{input}\")\n])\nchain = prompt | llm\n# 2. 初始化消息历史管理（保持不变）\nstore = {}\ndef get_session_history(session_id: str) -> ChatMessageHistory:\n    if session_id not in store:",
        "detail": "python_use.role_bot.src.main",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "python_use.role_bot.src.main",
        "description": "python_use.role_bot.src.main",
        "peekOfCode": "chain = prompt | llm\n# 2. 初始化消息历史管理（保持不变）\nstore = {}\ndef get_session_history(session_id: str) -> ChatMessageHistory:\n    if session_id not in store:\n        store[session_id] = ChatMessageHistory()\n    return store[session_id]\n# 3. 包装可运行对象（保持不变）\nrunnable = RunnableWithMessageHistory(\n    chain,",
        "detail": "python_use.role_bot.src.main",
        "documentation": {}
    },
    {
        "label": "store",
        "kind": 5,
        "importPath": "python_use.role_bot.src.main",
        "description": "python_use.role_bot.src.main",
        "peekOfCode": "store = {}\ndef get_session_history(session_id: str) -> ChatMessageHistory:\n    if session_id not in store:\n        store[session_id] = ChatMessageHistory()\n    return store[session_id]\n# 3. 包装可运行对象（保持不变）\nrunnable = RunnableWithMessageHistory(\n    chain,\n    get_session_history,\n    input_messages_key=\"input\",",
        "detail": "python_use.role_bot.src.main",
        "documentation": {}
    },
    {
        "label": "runnable",
        "kind": 5,
        "importPath": "python_use.role_bot.src.main",
        "description": "python_use.role_bot.src.main",
        "peekOfCode": "runnable = RunnableWithMessageHistory(\n    chain,\n    get_session_history,\n    input_messages_key=\"input\",\n    history_messages_key=\"history\"\n)\n# 4. 响应生成函数（保持不变）\ndef respond(message: str, history: list):\n    session_id = \"default_session\"\n    response = \"\"",
        "detail": "python_use.role_bot.src.main",
        "documentation": {}
    },
    {
        "label": "Solution",
        "kind": 6,
        "importPath": "questions.test",
        "description": "questions.test",
        "peekOfCode": "class Solution:\n    def kSmallestPairs(self, nums1: List[int], nums2: List[int], k: int) -> List[List[int]]:\n        # 1 4 11\n        # 2 3 6\n        n1 = len(nums1)\n        n2 = len(nums2)\n        f = [0] * n1\n        f[0] = 1\n        ans = []\n        need = k",
        "detail": "questions.test",
        "documentation": {}
    },
    {
        "label": "Agent",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyAgent.tinyAgent.Agent",
        "description": "tiny-universe-main.content.TinyAgent.tinyAgent.Agent",
        "peekOfCode": "class Agent:\n    def __init__(self, path: str = '') -> None:\n        self.path = path\n        self.tool = Tools()\n        self.system_prompt = self.build_system_input()\n        self.model = InternLM2Chat(path)\n    def build_system_input(self):\n        tool_descs, tool_names = [], []\n        for tool in self.tool.toolConfig:\n            tool_descs.append(TOOL_DESC.format(**tool))",
        "detail": "tiny-universe-main.content.TinyAgent.tinyAgent.Agent",
        "documentation": {}
    },
    {
        "label": "TOOL_DESC",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyAgent.tinyAgent.Agent",
        "description": "tiny-universe-main.content.TinyAgent.tinyAgent.Agent",
        "peekOfCode": "TOOL_DESC = \"\"\"{name_for_model}: Call this tool to interact with the {name_for_human} API. What is the {name_for_human} API useful for? {description_for_model} Parameters: {parameters} Format the arguments as a JSON object.\"\"\"\nREACT_PROMPT = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n{tool_descs}\nUse the following format:\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tool_names}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can be repeated zero or more times)",
        "detail": "tiny-universe-main.content.TinyAgent.tinyAgent.Agent",
        "documentation": {}
    },
    {
        "label": "REACT_PROMPT",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyAgent.tinyAgent.Agent",
        "description": "tiny-universe-main.content.TinyAgent.tinyAgent.Agent",
        "peekOfCode": "REACT_PROMPT = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n{tool_descs}\nUse the following format:\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tool_names}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can be repeated zero or more times)\nThought: I now know the final answer",
        "detail": "tiny-universe-main.content.TinyAgent.tinyAgent.Agent",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyAgent.tinyAgent.LLM",
        "description": "tiny-universe-main.content.TinyAgent.tinyAgent.LLM",
        "peekOfCode": "class BaseModel:\n    def __init__(self, path: str = '') -> None:\n        self.path = path\n    def chat(self, prompt: str, history: List[dict]):\n        pass\n    def load_model(self):\n        pass\nclass InternLM2Chat(BaseModel):\n    def __init__(self, path: str = '') -> None:\n        super().__init__(path)",
        "detail": "tiny-universe-main.content.TinyAgent.tinyAgent.LLM",
        "documentation": {}
    },
    {
        "label": "InternLM2Chat",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyAgent.tinyAgent.LLM",
        "description": "tiny-universe-main.content.TinyAgent.tinyAgent.LLM",
        "peekOfCode": "class InternLM2Chat(BaseModel):\n    def __init__(self, path: str = '') -> None:\n        super().__init__(path)\n        self.load_model()\n    def load_model(self):\n        print('================ Loading model ================')\n        self.tokenizer = AutoTokenizer.from_pretrained(self.path, trust_remote_code=True)\n        self.model = AutoModelForCausalLM.from_pretrained(self.path, torch_dtype=torch.float16, trust_remote_code=True).cuda().eval()\n        print('================ Model loaded ================')\n    def chat(self, prompt: str, history: List[dict], meta_instruction:str ='') -> str:",
        "detail": "tiny-universe-main.content.TinyAgent.tinyAgent.LLM",
        "documentation": {}
    },
    {
        "label": "Tools",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyAgent.tinyAgent.tool",
        "description": "tiny-universe-main.content.TinyAgent.tinyAgent.tool",
        "peekOfCode": "class Tools:\n    def __init__(self) -> None:\n        self.toolConfig = self._tools()\n    def _tools(self):\n        tools = [\n            {\n                'name_for_human': '谷歌搜索',\n                'name_for_model': 'google_search',\n                'description_for_model': '谷歌搜索是一个通用搜索引擎，可用于访问互联网、查询百科知识、了解时事新闻等。',\n                'parameters': [",
        "detail": "tiny-universe-main.content.TinyAgent.tinyAgent.tool",
        "documentation": {}
    },
    {
        "label": "load_transformed_dataset",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.dataloader",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.dataloader",
        "peekOfCode": "def load_transformed_dataset(img_size=32, batch_size=128) -> DataLoader:\n    \"\"\"加载并转换CIFAR10数据集\"\"\"\n    train_data_transform = transforms.Compose([\n        transforms.Resize((img_size, img_size)),\n        transforms.RandomHorizontalFlip(),  # 随机水平翻转\n        transforms.ToTensor(),  # 将数据缩放到[0, 1]范围\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # 将数据缩放到[-1, 1]范围\n    ])\n    test_data_transform = transforms.Compose([\n        transforms.Resize((img_size, img_size)),",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.dataloader",
        "documentation": {}
    },
    {
        "label": "show_tensor_image",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.dataloader",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.dataloader",
        "peekOfCode": "def show_tensor_image(image):\n    reverse_transforms = transforms.Compose([\n        transforms.Lambda(lambda t: (t + 1) / 2),  # 将数据从[-1, 1]缩放到[0, 1]范围\n        transforms.Lambda(lambda t: t.permute(1, 2, 0)),  # 将通道顺序从CHW改为HWC\n        transforms.Lambda(lambda t: t * 255.),  # 将数据缩放到[0, 255]范围\n        transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),  # 将数据转换为uint8类型\n        transforms.ToPILImage(),  # 将数据转换为PIL图像格式\n    ])\n    # 如果图像是批次数据,则取第一个图像\n    if len(image.shape) == 4:",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.dataloader",
        "documentation": {}
    },
    {
        "label": "NoiseScheduler",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.diffusion",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.diffusion",
        "peekOfCode": "class NoiseScheduler(nn.Module):\n    def __init__(self, beta_start=0.0001, beta_end=0.02, num_steps=1000):\n        \"\"\"初始化噪声调度器\n        Args:\n            beta_start: β1,初始噪声水平\n            beta_end: βT,最终噪声水平  \n            num_steps: T,扩散步数\n            device: 运行设备\n        \"\"\"\n        super().__init__()",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.diffusion",
        "documentation": {}
    },
    {
        "label": "plot_diffusion_steps",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.diffusion",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.diffusion",
        "peekOfCode": "def plot_diffusion_steps(image, noise_scheduler, step_size=100):\n    \"\"\"绘制图像逐步加噪的过程\n    Args:\n        image: 原始图像\n        noise_scheduler: 噪声调度器\n        step_size: 每隔多少步绘制一次\n    Returns:\n        fig: 绘制的图像\n    \"\"\"\n    num_images = noise_scheduler.num_steps // step_size",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.diffusion",
        "documentation": {}
    },
    {
        "label": "InceptionStatistics",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.metrics",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.metrics",
        "peekOfCode": "class InceptionStatistics:\n    def __init__(self, device='cuda'):\n        self.device = device\n        # 加载预训练的Inception v3模型\n        self.model = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1, transform_input=False)\n        self.model.fc = nn.Identity()  # 移除最后的全连接层\n        self.model = self.model.to(device)\n        self.model.eval()\n        # 设置图像预处理\n        self.preprocess = transforms.Compose([",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.metrics",
        "documentation": {}
    },
    {
        "label": "calculate_inception_score",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.metrics",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.metrics",
        "peekOfCode": "def calculate_inception_score(probs, splits=10):\n    \"\"\"计算Inception Score\n    IS = exp(E[KL(p(y|x) || p(y))])\n    其中:\n    - p(y|x) 是生成图像通过Inception模型得到的条件类别分布(probs)\n    - p(y) 是边缘类别分布,通过对所有图像的p(y|x)取平均得到\n    - KL是KL散度,用于衡量两个分布的差异\n    - E是对所有图像的期望\n    具体步骤:\n    1. 将所有图像分成splits组",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.metrics",
        "documentation": {}
    },
    {
        "label": "calculate_fid",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.metrics",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.metrics",
        "peekOfCode": "def calculate_fid(real_features, fake_features):\n    \"\"\"计算Fréchet Inception Distance (FID)分数\n    FID = ||μ_r - μ_f||^2 + Tr(Σ_r + Σ_f - 2(Σ_r Σ_f)^(1/2))\n    其中:\n    - μ_r, μ_f 分别是真实图像和生成图像特征的均值向量\n    - Σ_r, Σ_f 分别是真实图像和生成图像特征的协方差矩阵\n    - Tr 表示矩阵的迹(对角线元素之和)\n    - ||·||^2 表示欧几里得距离的平方\n    FID越小表示生成图像的质量越好,分布越接近真实图像\n    \"\"\"",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.metrics",
        "documentation": {}
    },
    {
        "label": "evaluate_model",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.metrics",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.metrics",
        "peekOfCode": "def evaluate_model(model, scheduler, train_loader, num_samples, batch_size, image_size, device=\"cuda\"):\n    \"\"\"评估模型的IS和FID分数\"\"\"\n    # 生成样本\n    fake_images = []\n    num_batches = num_samples // batch_size  # 每批生成batch_size张图片\n    print(f\"生成{num_samples}张图像...\")\n    for _ in tqdm(range(num_batches)):\n        fake_batch = sample(model, scheduler, batch_size, (3, image_size, image_size), device)\n        fake_batch = ((fake_batch + 1) / 2)  # 转换到[0,1]范围\n        fake_images.append(fake_batch.cpu())",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.metrics",
        "documentation": {}
    },
    {
        "label": "sample",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.sample",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.sample",
        "peekOfCode": "def sample(model, scheduler, num_samples, size, device=\"cpu\"):\n    \"\"\"从噪声采样生成图像的函数\n    Args:\n        model: UNet模型,用于预测噪声\n        scheduler: 噪声调度器,包含采样所需的所有系数\n        num_samples: 要生成的样本数量\n        size: 生成图像的大小,如(3,32,32)\n        device: 运行设备\n    Returns:\n        生成的图像张量",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.sample",
        "documentation": {}
    },
    {
        "label": "plot",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.sample",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.sample",
        "peekOfCode": "def plot(images):\n    fig = plt.figure(figsize=(12, 8))\n    plt.axis(\"off\")\n    plt.imshow(torchvision.utils.make_grid(images, nrow=5).permute(1, 2, 0))\n    plt.tight_layout(pad=1)\n    return fig\nif __name__ == \"__main__\":\n    image_size = 32\n    model = SimpleUnet()\n    model.load_state_dict(torch.load(f\"simple-unet-ddpm-{image_size}.pth\", weights_only=True))",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.sample",
        "documentation": {}
    },
    {
        "label": "test_step",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.train",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.train",
        "peekOfCode": "def test_step(model, dataloader, noise_scheduler, criterion, epoch, num_epochs, device):\n    \"\"\"测试步骤,计算测试集上的损失\"\"\"\n    model.eval()\n    with torch.no_grad():\n        loss_sum = 0\n        num_batches = 0\n        pbar = tqdm(dataloader)\n        for batch in pbar:\n            images, _ = batch\n            images = images.to(device)",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.train",
        "documentation": {}
    },
    {
        "label": "train_step",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.train",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.train",
        "peekOfCode": "def train_step(model, dataloader, noise_scheduler, criterion, optimizer, epoch, num_epochs, device):\n    \"\"\"训练步骤,计算训练集上的损失并更新模型参数\"\"\"\n    # 设置模型为训练模式\n    model.train()\n    loss_sum = 0\n    num_batches = 0\n    pbar = tqdm(dataloader)\n    for batch in pbar:\n        # 获取一个batch的图像数据并移至指定设备\n        images, _ = batch",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.train",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.train",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.train",
        "peekOfCode": "def train(model, train_loader, test_loader, noise_scheduler, criterion, optimizer, device, num_epochs=100, img_size=32):\n    \"\"\"训练模型\"\"\"\n    for epoch in range(num_epochs):\n        train_loss = train_step(model, train_loader, noise_scheduler, criterion, optimizer, epoch, num_epochs, device)\n        test_loss = test_step(model, test_loader, noise_scheduler, criterion, epoch, num_epochs, device)\n        if epoch % 10 == 0:\n            # 采样10张图像\n            images = sample(model, noise_scheduler, 10, (3, img_size, img_size), device)\n            # 将图像从[-1, 1]范围缩放到[0, 1]范围,以便可视化\n            images = ((images + 1) / 2).detach().cpu()",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.train",
        "documentation": {}
    },
    {
        "label": "Block",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.unet",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.unet",
        "peekOfCode": "class Block(nn.Module):\n    def __init__(self, in_channels, out_channels, time_emb_dim, up=False):\n        \"\"\"UNet中的基本Block模块,包含时间嵌入和上/下采样功能\"\"\"\n        super().__init__()\n        self.time_mlp = nn.Linear(time_emb_dim, out_channels)\n        if up:\n            self.conv1 = nn.Conv2d(2 * in_channels, out_channels, kernel_size=3, padding=1)\n            self.transform = nn.ConvTranspose2d(out_channels, out_channels, kernel_size=4, stride=2, padding=1)\n        else:\n            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.unet",
        "documentation": {}
    },
    {
        "label": "SinusoidalPositionEmbeddings",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.unet",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.unet",
        "peekOfCode": "class SinusoidalPositionEmbeddings(nn.Module):\n    \"\"\"使用正弦位置编码实现时间步的嵌入,参考Transformer中的位置编码方法,使用正余弦函数将时间步映射到高维空间\"\"\"\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n    def forward(self, time):\n        device = time.device\n        # 将维度分成两半,分别用于sin和cos\n        half_dim = self.dim // 2\n        # 计算不同频率的指数衰减",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.unet",
        "documentation": {}
    },
    {
        "label": "SimpleUnet",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.unet",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.unet",
        "peekOfCode": "class SimpleUnet(nn.Module):\n    \"\"\"简单的UNet模型,用于扩散模型的噪声预测\"\"\"\n    def __init__(self):\n        super().__init__()\n        image_channels = 3\n        down_channels = (64, 128, 256, 512, 1024)\n        up_channels = (1024, 512, 256, 128, 64)\n        out_dim = 3\n        time_emb_dim = 32\n        # 时间嵌入层",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.unet",
        "documentation": {}
    },
    {
        "label": "print_shapes",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyDiffusion.ddpm.unet",
        "description": "tiny-universe-main.content.TinyDiffusion.ddpm.unet",
        "peekOfCode": "def print_shapes(model, x, time_step):\n    print(\"Input shape:\", x.shape)\n    # 时间步嵌入\n    t = model.time_embed(time_step)\n    print(\"Time embedding shape:\", t.shape)\n    # 初始卷积\n    x = model.input(x)\n    print(\"After input conv shape:\", x.shape)\n    #下采样过程\n    residual_stack = []",
        "detail": "tiny-universe-main.content.TinyDiffusion.ddpm.unet",
        "documentation": {}
    },
    {
        "label": "BaseLLM",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyEval.Eval.model.LLM",
        "description": "tiny-universe-main.content.TinyEval.Eval.model.LLM",
        "peekOfCode": "class BaseLLM:\n    def __init__(self, path: str, model_name: str, adapter_path: str) -> None:\n        self.path = path\n        self.model_name = model_name\n        self.adapter_path = adapter_path\n    def build_chat(self, tokenizer, prompt, model_name):\n        pass\n    def load_model_and_tokenizer(self, path, model_name, device):\n        pass\n    def post_process(self, response, model_name):",
        "detail": "tiny-universe-main.content.TinyEval.Eval.model.LLM",
        "documentation": {}
    },
    {
        "label": "internlm2Chat",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyEval.Eval.model.LLM",
        "description": "tiny-universe-main.content.TinyEval.Eval.model.LLM",
        "peekOfCode": "class internlm2Chat(BaseLLM):\n    def __init__(self, path: str, model_name: str = '', adapter_path: str = '') -> None:\n        super().__init__(path, model_name, adapter_path)  # 调用父类初始化函数并传入参数\n    def build_chat(self, prompt):\n        prompt = f'<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n'\n        return prompt\n    def post_process(self, response):\n        response = response.split(\"<|im_end|>\")[0]\n        return response\n    def load_model_and_tokenizer(self, path, device, adapter_path):",
        "detail": "tiny-universe-main.content.TinyEval.Eval.model.LLM",
        "documentation": {}
    },
    {
        "label": "Qwen2Chat",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyEval.Eval.model.LLM",
        "description": "tiny-universe-main.content.TinyEval.Eval.model.LLM",
        "peekOfCode": "class Qwen2Chat(BaseLLM):\n    def __init__(self, path: str, model_name: str = '', adapter_path: str = '') -> None:\n        super().__init__(path, model_name, adapter_path)  # 调用父类初始化函数并传入参数\n    def build_chat(self, prompt, instruct=None):\n        if instruct is None:\n            instruct = 'You are a helpful assistant.'\n        prompt = f'<|im_start|>system\\n{instruct}<im_end>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n'\n        return prompt\n    def load_model_and_tokenizer(self, path, device, adapter_path):\n        model = AutoModelForCausalLM.from_pretrained(path, trust_remote_code=True, torch_dtype=torch.bfloat16).to(device)",
        "detail": "tiny-universe-main.content.TinyEval.Eval.model.LLM",
        "documentation": {}
    },
    {
        "label": "normalize_zh_aswer",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "description": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "peekOfCode": "def normalize_zh_aswer(s):\n    \"\"\"小写化,删除标点,删除空格\"\"\"\n    def white_space_fix(text):\n        return \"\".join(text.split())\n    def remove_punc(text):\n        cn_punctuation = \"！？｡。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.\"\n        all_punctuation = set(string.punctuation + cn_punctuation)\n        return ''.join(ch for ch in text if ch not in all_punctuation)\n    def lower(text):\n        return text.lower()",
        "detail": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "documentation": {}
    },
    {
        "label": "normalize_en_answer",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "description": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "peekOfCode": "def normalize_en_answer(s):\n    \"\"\"小写化,删除标点,删除冠词和多余空白.\"\"\"\n    def remove_articles(text):\n        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n    def white_space_fix(text):\n        return \" \".join(text.split())\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \"\".join(ch for ch in text if ch not in exclude)\n    def lower(text):",
        "detail": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "documentation": {}
    },
    {
        "label": "classification_score",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "description": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "peekOfCode": "def classification_score(prediction, ground_truth, **kwargs):\n    em_match_list = []\n    all_classes = kwargs[\"all_classes\"]\n    for class_name in all_classes:\n        if class_name in prediction:                                   # 总类别里面的类别是否在预测中出现\n            em_match_list.append(class_name)\n    for match_term in em_match_list:\n        if match_term in ground_truth and match_term != ground_truth:  # 如果预测中的类别在答案中出现，但是不是答案  'two step'--'step'\n            em_match_list.remove(match_term)\n    if ground_truth in em_match_list:",
        "detail": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "documentation": {}
    },
    {
        "label": "rouge_score",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "description": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "peekOfCode": "def rouge_score(prediction, ground_truth, **kwargs):\n    rouge = Rouge()\n    try:\n        scores = rouge.get_scores([prediction], [ground_truth], avg=True)\n    except:\n        return 0.0\n    return scores[\"rouge-l\"][\"f\"]\ndef rouge_zh_score(prediction, ground_truth, **kwargs):\n    prediction = \" \".join(list(jieba.cut(prediction, cut_all=False)))\n    ground_truth = \" \".join(list(jieba.cut(ground_truth, cut_all=False))) ",
        "detail": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "documentation": {}
    },
    {
        "label": "rouge_zh_score",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "description": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "peekOfCode": "def rouge_zh_score(prediction, ground_truth, **kwargs):\n    prediction = \" \".join(list(jieba.cut(prediction, cut_all=False)))\n    ground_truth = \" \".join(list(jieba.cut(ground_truth, cut_all=False))) \n    score = rouge_score(prediction, ground_truth)\n    return score\ndef f1_score(prediction, ground_truth, **kwargs):\n    # Counter以dict的形式存储各个句子对应的词与其对应个数,&操作符返回两个Counter中共同的元素的键值对\n    common = Counter(prediction) & Counter(ground_truth)  \n    num_same = sum(common.values())                       # 显示prediction与gt的共同元素的个数\n    if num_same == 0:",
        "detail": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "documentation": {}
    },
    {
        "label": "f1_score",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "description": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "peekOfCode": "def f1_score(prediction, ground_truth, **kwargs):\n    # Counter以dict的形式存储各个句子对应的词与其对应个数,&操作符返回两个Counter中共同的元素的键值对\n    common = Counter(prediction) & Counter(ground_truth)  \n    num_same = sum(common.values())                       # 显示prediction与gt的共同元素的个数\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(prediction)          # 即模型预测正确的样本数量与总预测样本数量的比值\n    recall = 1.0 * num_same / len(ground_truth)           # 模型正确预测的样本数量与总实际样本数量的比值\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1",
        "detail": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "documentation": {}
    },
    {
        "label": "qa_f1_score",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "description": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "peekOfCode": "def qa_f1_score(prediction, ground_truth, **kwargs):\n    normalized_prediction = normalize_en_answer(prediction)\n    normalized_ground_truth = normalize_en_answer(ground_truth)\n    prediction_tokens = normalized_prediction.split()\n    ground_truth_tokens = normalized_ground_truth.split()\n    return f1_score(prediction_tokens, ground_truth_tokens)\ndef qa_f1_zh_score(prediction, ground_truth, **kwargs):\n    prediction_tokens = list(jieba.cut(prediction, cut_all=False))\n    ground_truth_tokens = list(jieba.cut(ground_truth, cut_all=False))\n    prediction_tokens_norm = [normalize_zh_aswer(t) for t in prediction_tokens]",
        "detail": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "documentation": {}
    },
    {
        "label": "qa_f1_zh_score",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "description": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "peekOfCode": "def qa_f1_zh_score(prediction, ground_truth, **kwargs):\n    prediction_tokens = list(jieba.cut(prediction, cut_all=False))\n    ground_truth_tokens = list(jieba.cut(ground_truth, cut_all=False))\n    prediction_tokens_norm = [normalize_zh_aswer(t) for t in prediction_tokens]\n    ground_truth_tokens_norm = [normalize_zh_aswer(t) for t in ground_truth_tokens]\n    prediction_tokens = [t for t in prediction_tokens_norm if len(t) > 0]\n    ground_truth_tokens = [t for t in ground_truth_tokens_norm if len(t) > 0]\n    return f1_score(prediction_tokens, ground_truth_tokens)\ndef GAOKAO_math(prediction, ground_truth, **kwargs):\n    score = 0",
        "detail": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "documentation": {}
    },
    {
        "label": "GAOKAO_math",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "description": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "peekOfCode": "def GAOKAO_math(prediction, ground_truth, **kwargs):\n    score = 0\n    # 判断是单选还是多选\n    if len(ground_truth) > 1:\n        # 多选\n        pattern = r\"[A-D]\"\n        matches = re.findall(pattern, prediction)\n        predicted_answer = ''\n        if matches:\n            # 从后往前匹配大写字母，且满足之间长度不超过10个字符的条件",
        "detail": "tiny-universe-main.content.TinyEval.Eval.metrics",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyEval.eval",
        "description": "tiny-universe-main.content.TinyEval.eval",
        "peekOfCode": "def parse_args(args=None):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model', type=str, default='Qwen2')\n    return parser.parse_args(args)\ndataset2metric = {\n    'multifieldqa_zh': qa_f1_zh_score,\n    'multi_news': rouge_score,\n    'trec': classification_score,\n    'custom_zh': rouge_zh_score,\n    \"GAOKAO_math\": GAOKAO_math",
        "detail": "tiny-universe-main.content.TinyEval.eval",
        "documentation": {}
    },
    {
        "label": "scorer",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyEval.eval",
        "description": "tiny-universe-main.content.TinyEval.eval",
        "peekOfCode": "def scorer(dataset, predictions, answers, all_classes):\n    total_score = 0.\n    for (prediction, ground_truths) in zip(predictions, answers):\n        score = 0.\n        if dataset in [\"trec\"]:\n            prediction = prediction.lstrip('\\n').split('\\n')[0]  # 格式抽取\n        if dataset in ['custom_zh', 'custom_en']:\n            score = max(score, dataset2metric[dataset](prediction, ground_truths, all_classes=all_classes))\n        else:\n            score = max(score, dataset2metric.get(dataset, dataset2metric[dataset])(prediction, ground_truths, all_classes=all_classes))",
        "detail": "tiny-universe-main.content.TinyEval.eval",
        "documentation": {}
    },
    {
        "label": "dataset2metric",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyEval.eval",
        "description": "tiny-universe-main.content.TinyEval.eval",
        "peekOfCode": "dataset2metric = {\n    'multifieldqa_zh': qa_f1_zh_score,\n    'multi_news': rouge_score,\n    'trec': classification_score,\n    'custom_zh': rouge_zh_score,\n    \"GAOKAO_math\": GAOKAO_math\n}\n# 计算得分\ndef scorer(dataset, predictions, answers, all_classes):\n    total_score = 0.",
        "detail": "tiny-universe-main.content.TinyEval.eval",
        "documentation": {}
    },
    {
        "label": "seed_everything",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyEval.inference",
        "description": "tiny-universe-main.content.TinyEval.inference",
        "peekOfCode": "def seed_everything(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n    torch.cuda.manual_seed_all(seed)\ndef parse_args(args=None):\n    parser = argparse.ArgumentParser()",
        "detail": "tiny-universe-main.content.TinyEval.inference",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyEval.inference",
        "description": "tiny-universe-main.content.TinyEval.inference",
        "peekOfCode": "def parse_args(args=None):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model', type=str, default='Qwen2')\n    return parser.parse_args(args)\nif __name__ == '__main__':\n    seed_everything(42)\n    args = parse_args()\n    model2path = json.load(open(\"Eval/config/model2path.json\", \"r\"))\n    model2maxlen = json.load(open(\"Eval/config/model2maxlen.json\", \"r\"))\n    adapter2path = json.load(open(\"Eval/config/adapter2path.json\", \"r\"))",
        "detail": "tiny-universe-main.content.TinyEval.inference",
        "documentation": {}
    },
    {
        "label": "load_qwen_vlm",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.ImgEvaluator",
        "description": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.ImgEvaluator",
        "peekOfCode": "def load_qwen_vlm(pretrained_model=\"./model/Qwen/Qwen2.5-VL-3B-Instruct\"):\n    min_pixels = 256 * 28 * 28\n    max_pixels = 1280 * 28 * 28\n    # default: Load the model on the available device(s)\n    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n        pretrained_model, torch_dtype=\"auto\", device_map=\"auto\"\n    )\n    # default processer\n    processor = AutoProcessor.from_pretrained(pretrained_model, min_pixels=min_pixels,\n                                              max_pixels=max_pixels)",
        "detail": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.ImgEvaluator",
        "documentation": {}
    },
    {
        "label": "run_qwen_vl",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.ImgEvaluator",
        "description": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.ImgEvaluator",
        "peekOfCode": "def run_qwen_vl(image_path,prompt,model,processor):\n    # 编码图片\n    with open(image_path, \"rb\") as image_file:\n        base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image\",",
        "detail": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.ImgEvaluator",
        "documentation": {}
    },
    {
        "label": "SDXLGenerator",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.ImgGenerator",
        "description": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.ImgGenerator",
        "peekOfCode": "class SDXLGenerator:\n    def __init__(self, prompt, output_path, steps=50, seed=0,\n                 use_image_guidance=False, image_path=None, ip_scale=0.5,\n                 sd_path=\"./model/stabilityai/stable-diffusion-xl-base-1.0\",\n                 adapter_path=\"./model/h94/IP-Adapter\"):\n        self.prompt = prompt\n        self.output_path = output_path\n        self.steps = steps\n        self.seed = seed\n        self.use_image_guidance = use_image_guidance",
        "detail": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.ImgGenerator",
        "documentation": {}
    },
    {
        "label": "get_clip_similarities",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.ImgRetrieval",
        "description": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.ImgRetrieval",
        "peekOfCode": "def get_clip_similarities(prompts, image_paths, embeddings_path=\"./datasets/vector_bases\", bs=2, k=5, device='cuda:0',\n                          model_path=\"./model/ViT-B-32.pt\"):\n    \"\"\"\n    Calculate similarity between text prompts and images using CLIP model.\n    Args:\n        prompts: List of text prompts to compare against images\n        image_paths: List of paths to images\n        embeddings_path: Directory to save/load precomputed image embeddings\n        bs: Batch size for processing images\n        k: Number of top similar images to return",
        "detail": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.ImgRetrieval",
        "documentation": {}
    },
    {
        "label": "load_qwen_llm",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.RewritePrompt",
        "description": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.RewritePrompt",
        "peekOfCode": "def load_qwen_llm(model_name = \"./model/Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\"):\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        torch_dtype=\"auto\",\n        device_map=\"auto\"\n    )\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    return model,tokenizer\ndef run_qwen_llm(prompt,model,tokenizer):\n    messages = [",
        "detail": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.RewritePrompt",
        "documentation": {}
    },
    {
        "label": "run_qwen_llm",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.RewritePrompt",
        "description": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.RewritePrompt",
        "peekOfCode": "def run_qwen_llm(prompt,model,tokenizer):\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )",
        "detail": "tiny-universe-main.content.TinyIMGRAG.IMGRAG.RewritePrompt",
        "documentation": {}
    },
    {
        "label": "available_models",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "peekOfCode": "def available_models() -> List[str]:\n    \"\"\"Returns the names of available CLIP models\"\"\"\n    return list(_MODELS.keys())\ndef load(name: str, device: Union[str, torch.device] = \"cuda\" if torch.cuda.is_available() else \"cpu\", jit: bool = False, download_root: str = None):\n    \"\"\"Load a CLIP model\n    Parameters\n    ----------\n    name : str\n        A model name listed by `clip.available_models()`, or the path to a model checkpoint containing the state_dict\n    device : Union[str, torch.device]",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "documentation": {}
    },
    {
        "label": "load",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "peekOfCode": "def load(name: str, device: Union[str, torch.device] = \"cuda\" if torch.cuda.is_available() else \"cpu\", jit: bool = False, download_root: str = None):\n    \"\"\"Load a CLIP model\n    Parameters\n    ----------\n    name : str\n        A model name listed by `clip.available_models()`, or the path to a model checkpoint containing the state_dict\n    device : Union[str, torch.device]\n        The device to put the loaded model\n    jit : bool\n        Whether to load the optimized JIT model or more hackable non-JIT model (default).",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "documentation": {}
    },
    {
        "label": "tokenize",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "peekOfCode": "def tokenize(texts: Union[str, List[str]], context_length: int = 77, truncate: bool = False) -> Union[torch.IntTensor, torch.LongTensor]:\n    \"\"\"\n    Returns the tokenized representation of given input string(s)\n    Parameters\n    ----------\n    texts : Union[str, List[str]]\n        An input string or a list of input strings to tokenize\n    context_length : int\n        The context length to use; all CLIP models use 77 as the context length\n    truncate: bool",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "peekOfCode": "__all__ = [\"available_models\", \"load\", \"tokenize\"]\n_tokenizer = _Tokenizer()\n_MODELS = {\n    \"RN50\": \"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\",\n    \"RN101\": \"https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt\",\n    \"RN50x4\": \"https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt\",\n    \"RN50x16\": \"https://openaipublic.azureedge.net/clip/models/52378b407f34354e150460fe41077663dd5b39c54cd0bfd2b27167a4a06ec9aa/RN50x16.pt\",\n    \"RN50x64\": \"https://openaipublic.azureedge.net/clip/models/be1cfb55d75a9666199fb2206c106743da0f6468c9d327f3e0d0a543a9919d9c/RN50x64.pt\",\n    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n    \"ViT-B/16\": \"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\",",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "documentation": {}
    },
    {
        "label": "_tokenizer",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "peekOfCode": "_tokenizer = _Tokenizer()\n_MODELS = {\n    \"RN50\": \"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\",\n    \"RN101\": \"https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt\",\n    \"RN50x4\": \"https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt\",\n    \"RN50x16\": \"https://openaipublic.azureedge.net/clip/models/52378b407f34354e150460fe41077663dd5b39c54cd0bfd2b27167a4a06ec9aa/RN50x16.pt\",\n    \"RN50x64\": \"https://openaipublic.azureedge.net/clip/models/be1cfb55d75a9666199fb2206c106743da0f6468c9d327f3e0d0a543a9919d9c/RN50x64.pt\",\n    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n    \"ViT-B/16\": \"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\",\n    \"ViT-L/14\": \"https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt\",",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "documentation": {}
    },
    {
        "label": "_MODELS",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "peekOfCode": "_MODELS = {\n    \"RN50\": \"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\",\n    \"RN101\": \"https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt\",\n    \"RN50x4\": \"https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt\",\n    \"RN50x16\": \"https://openaipublic.azureedge.net/clip/models/52378b407f34354e150460fe41077663dd5b39c54cd0bfd2b27167a4a06ec9aa/RN50x16.pt\",\n    \"RN50x64\": \"https://openaipublic.azureedge.net/clip/models/be1cfb55d75a9666199fb2206c106743da0f6468c9d327f3e0d0a543a9919d9c/RN50x64.pt\",\n    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n    \"ViT-B/16\": \"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\",\n    \"ViT-L/14\": \"https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt\",\n    \"ViT-L/14@336px\": \"https://openaipublic.azureedge.net/clip/models/3035c92b350959924f9f00213499208652fc7ea050643e8b385c2dac08641f02/ViT-L-14-336px.pt\",",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.clip",
        "documentation": {}
    },
    {
        "label": "Bottleneck",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "peekOfCode": "class Bottleneck(nn.Module):\n    expansion = 4\n    def __init__(self, inplanes, planes, stride=1):\n        super().__init__()\n        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "documentation": {}
    },
    {
        "label": "AttentionPool2d",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "peekOfCode": "class AttentionPool2d(nn.Module):\n    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n        super().__init__()\n        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n        self.num_heads = num_heads\n    def forward(self, x):",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "documentation": {}
    },
    {
        "label": "ModifiedResNet",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "peekOfCode": "class ModifiedResNet(nn.Module):\n    \"\"\"\n    A ResNet class that is similar to torchvision's but contains the following changes:\n    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n    - The final pooling layer is a QKV attention instead of an average pool\n    \"\"\"\n    def __init__(self, layers, output_dim, heads, input_resolution=224, width=64):\n        super().__init__()\n        self.output_dim = output_dim",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "documentation": {}
    },
    {
        "label": "LayerNorm",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "peekOfCode": "class LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        ret = super().forward(x.type(torch.float32))\n        return ret.type(orig_type)\nclass QuickGELU(nn.Module):\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\nclass ResidualAttentionBlock(nn.Module):",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "documentation": {}
    },
    {
        "label": "QuickGELU",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "peekOfCode": "class QuickGELU(nn.Module):\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\nclass ResidualAttentionBlock(nn.Module):\n    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(d_model, n_head)\n        self.ln_1 = LayerNorm(d_model)\n        self.mlp = nn.Sequential(OrderedDict([\n            (\"c_fc\", nn.Linear(d_model, d_model * 4)),",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "documentation": {}
    },
    {
        "label": "ResidualAttentionBlock",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "peekOfCode": "class ResidualAttentionBlock(nn.Module):\n    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(d_model, n_head)\n        self.ln_1 = LayerNorm(d_model)\n        self.mlp = nn.Sequential(OrderedDict([\n            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n            (\"gelu\", QuickGELU()),\n            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n        ]))",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "peekOfCode": "class Transformer(nn.Module):\n    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n        self.width = width\n        self.layers = layers\n        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n    def forward(self, x: torch.Tensor):\n        return self.resblocks(x)\nclass VisionTransformer(nn.Module):\n    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int):",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "documentation": {}
    },
    {
        "label": "VisionTransformer",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "peekOfCode": "class VisionTransformer(nn.Module):\n    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int):\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.output_dim = output_dim\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)\n        scale = width ** -0.5\n        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n        self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width))\n        self.ln_pre = LayerNorm(width)",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "documentation": {}
    },
    {
        "label": "CLIP",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "peekOfCode": "class CLIP(nn.Module):\n    def __init__(self,\n                 embed_dim: int,\n                 # vision\n                 image_resolution: int,\n                 vision_layers: Union[Tuple[int, int, int, int], int],\n                 vision_width: int,\n                 vision_patch_size: int,\n                 # text\n                 context_length: int,",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "documentation": {}
    },
    {
        "label": "convert_weights",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "peekOfCode": "def convert_weights(model: nn.Module):\n    \"\"\"Convert applicable model parameters to fp16\"\"\"\n    def _convert_weights_to_fp16(l):\n        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n            l.weight.data = l.weight.data.half()\n            if l.bias is not None:\n                l.bias.data = l.bias.data.half()\n        if isinstance(l, nn.MultiheadAttention):\n            for attr in [*[f\"{s}_proj_weight\" for s in [\"in\", \"q\", \"k\", \"v\"]], \"in_proj_bias\", \"bias_k\", \"bias_v\"]:\n                tensor = getattr(l, attr)",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "documentation": {}
    },
    {
        "label": "build_model",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "peekOfCode": "def build_model(state_dict: dict):\n    vit = \"visual.proj\" in state_dict\n    if vit:\n        vision_width = state_dict[\"visual.conv1.weight\"].shape[0]\n        vision_layers = len([k for k in state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n        vision_patch_size = state_dict[\"visual.conv1.weight\"].shape[-1]\n        grid_size = round((state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n        image_resolution = vision_patch_size * grid_size\n    else:\n        counts: list = [len(set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"visual.layer{b}\"))) for b in [1, 2, 3, 4]]",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.model",
        "documentation": {}
    },
    {
        "label": "SimpleTokenizer",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "peekOfCode": "class SimpleTokenizer(object):\n    def __init__(self, bpe_path: str = default_bpe()):\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')\n        merges = merges[1:49152-256-2+1]\n        merges = [tuple(merge.split()) for merge in merges]\n        vocab = list(bytes_to_unicode().values())\n        vocab = vocab + [v+'</w>' for v in vocab]\n        for merge in merges:",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "documentation": {}
    },
    {
        "label": "default_bpe",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "peekOfCode": "def default_bpe():\n    return os.path.join(os.path.dirname(os.path.abspath(__file__)), \"bpe_simple_vocab_16e6.txt.gz\")\n@lru_cache()\ndef bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "documentation": {}
    },
    {
        "label": "bytes_to_unicode",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "peekOfCode": "def bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "documentation": {}
    },
    {
        "label": "get_pairs",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "peekOfCode": "def get_pairs(word):\n    \"\"\"Return set of symbol pairs in a word.\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    \"\"\"\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "documentation": {}
    },
    {
        "label": "basic_clean",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "peekOfCode": "def basic_clean(text):\n    text = ftfy.fix_text(text)\n    text = html.unescape(html.unescape(text))\n    return text.strip()\ndef whitespace_clean(text):\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.strip()\n    return text\nclass SimpleTokenizer(object):\n    def __init__(self, bpe_path: str = default_bpe()):",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "documentation": {}
    },
    {
        "label": "whitespace_clean",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "peekOfCode": "def whitespace_clean(text):\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.strip()\n    return text\nclass SimpleTokenizer(object):\n    def __init__(self, bpe_path: str = default_bpe()):\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')\n        merges = merges[1:49152-256-2+1]",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.clip.simple_tokenizer",
        "documentation": {}
    },
    {
        "label": "test_consistency",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.tests.test_consistency",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.tests.test_consistency",
        "peekOfCode": "def test_consistency(model_name):\n    device = \"cpu\"\n    jit_model, transform = clip.load(model_name, device=device, jit=True)\n    py_model, _ = clip.load(model_name, device=device, jit=False)\n    image = transform(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)\n    text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n    with torch.no_grad():\n        logits_per_image, _ = jit_model(image, text)\n        jit_probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n        logits_per_image, _ = py_model(image, text)",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.tests.test_consistency",
        "documentation": {}
    },
    {
        "label": "tokenize",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.hubconf",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.hubconf",
        "peekOfCode": "def tokenize():\n    return _tokenize\n_entrypoints = {model_functions[model]: _create_hub_entrypoint(model) for model in _available_models()}\nglobals().update(_entrypoints)",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.hubconf",
        "documentation": {}
    },
    {
        "label": "dependencies",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.hubconf",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.hubconf",
        "peekOfCode": "dependencies = [\"torch\", \"torchvision\", \"ftfy\", \"regex\", \"tqdm\"]\n# For compatibility (cannot include special characters in function name)\nmodel_functions = { model: re.sub(f'[{string.punctuation}]', '_', model) for model in _available_models()}\ndef _create_hub_entrypoint(model):\n    def entrypoint(**kwargs):      \n        return _load(model, **kwargs)\n    entrypoint.__doc__ = f\"\"\"Loads the {model} CLIP model\n        Parameters\n        ----------\n        device : Union[str, torch.device]",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.hubconf",
        "documentation": {}
    },
    {
        "label": "model_functions",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.hubconf",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.hubconf",
        "peekOfCode": "model_functions = { model: re.sub(f'[{string.punctuation}]', '_', model) for model in _available_models()}\ndef _create_hub_entrypoint(model):\n    def entrypoint(**kwargs):      \n        return _load(model, **kwargs)\n    entrypoint.__doc__ = f\"\"\"Loads the {model} CLIP model\n        Parameters\n        ----------\n        device : Union[str, torch.device]\n            The device to put the loaded model\n        jit : bool",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.hubconf",
        "documentation": {}
    },
    {
        "label": "_entrypoints",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.hubconf",
        "description": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.hubconf",
        "peekOfCode": "_entrypoints = {model_functions[model]: _create_hub_entrypoint(model) for model in _available_models()}\nglobals().update(_entrypoints)",
        "detail": "tiny-universe-main.content.TinyIMGRAG.packages.CLIP-main.hubconf",
        "documentation": {}
    },
    {
        "label": "model_dir",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.download_model",
        "description": "tiny-universe-main.content.TinyIMGRAG.download_model",
        "peekOfCode": "model_dir = modelscope.snapshot_download('Qwen/Qwen2.5-VL-3B-Instruct', cache_dir='./model/', revision='master')\nmodel_dir = modelscope.snapshot_download('Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4', cache_dir='./model/', revision='master')\nmodel_dir = modelscope.snapshot_download('stabilityai/stable-diffusion-xl-base-1.0', cache_dir='./model/', revision='master')\nmodel_dir = huggingface_hub.snapshot_download(repo_id=\"h94/IP-Adapter\", local_dir=\"./model/\", max_workers=1)",
        "detail": "tiny-universe-main.content.TinyIMGRAG.download_model",
        "documentation": {}
    },
    {
        "label": "model_dir",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.download_model",
        "description": "tiny-universe-main.content.TinyIMGRAG.download_model",
        "peekOfCode": "model_dir = modelscope.snapshot_download('Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4', cache_dir='./model/', revision='master')\nmodel_dir = modelscope.snapshot_download('stabilityai/stable-diffusion-xl-base-1.0', cache_dir='./model/', revision='master')\nmodel_dir = huggingface_hub.snapshot_download(repo_id=\"h94/IP-Adapter\", local_dir=\"./model/\", max_workers=1)",
        "detail": "tiny-universe-main.content.TinyIMGRAG.download_model",
        "documentation": {}
    },
    {
        "label": "model_dir",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.download_model",
        "description": "tiny-universe-main.content.TinyIMGRAG.download_model",
        "peekOfCode": "model_dir = modelscope.snapshot_download('stabilityai/stable-diffusion-xl-base-1.0', cache_dir='./model/', revision='master')\nmodel_dir = huggingface_hub.snapshot_download(repo_id=\"h94/IP-Adapter\", local_dir=\"./model/\", max_workers=1)",
        "detail": "tiny-universe-main.content.TinyIMGRAG.download_model",
        "documentation": {}
    },
    {
        "label": "model_dir",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.download_model",
        "description": "tiny-universe-main.content.TinyIMGRAG.download_model",
        "peekOfCode": "model_dir = huggingface_hub.snapshot_download(repo_id=\"h94/IP-Adapter\", local_dir=\"./model/\", max_workers=1)",
        "detail": "tiny-universe-main.content.TinyIMGRAG.download_model",
        "documentation": {}
    },
    {
        "label": "ImageRAGPipeline",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyIMGRAG.main",
        "description": "tiny-universe-main.content.TinyIMGRAG.main",
        "peekOfCode": "class ImageRAGPipeline:\n    def __init__(self, base_output_dir=\"./datasets/results\"):\n        self.base_output_dir = base_output_dir\n        os.makedirs(self.base_output_dir, exist_ok=True)\n        # Initialize all models as None (lazy loading)\n        self.vl_model = None\n        self.vl_processor = None\n        self.llm_model = None\n        self.llm_tokenizer = None\n        self.clip_model = None",
        "detail": "tiny-universe-main.content.TinyIMGRAG.main",
        "documentation": {}
    },
    {
        "label": "ModelArgs",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyLLM.code.model",
        "description": "tiny-universe-main.content.TinyLLM.code.model",
        "peekOfCode": "class ModelArgs:\n    # 自定义超参数\n    dim: int = 288  # 模型维度\n    n_layers: int = 6  # Transformer层数\n    n_heads: int = 6  # 注意力机制的头数\n    n_kv_heads: Optional[int] = 6  # 键/值头数，如果未指定，则默认为n_heads\n    vocab_size: int = 32000  # 词汇表大小\n    hidden_dim: Optional[int] = None  # 隐藏层维度，如果未指定，则使用其他规则确定\n    multiple_of: int = 32  # MLP隐藏层大小是这个数的倍数\n    norm_eps: float = 1e-5  # 归一化层的epsilon值",
        "detail": "tiny-universe-main.content.TinyLLM.code.model",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyLLM.code.model",
        "description": "tiny-universe-main.content.TinyLLM.code.model",
        "peekOfCode": "class RMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float):\n        super().__init__()\n        # eps是为了防止除以0的情况\n        self.eps = eps\n        # weight是一个可学习的参数，全部初始化为1\n        self.weight = nn.Parameter(torch.ones(dim))\n    def _norm(self, x):\n        # 计算RMSNorm的核心部分\n        # x.pow(2).mean(-1, keepdim=True)计算了输入x的平方的均值",
        "detail": "tiny-universe-main.content.TinyLLM.code.model",
        "documentation": {}
    },
    {
        "label": "Attention",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyLLM.code.model",
        "description": "tiny-universe-main.content.TinyLLM.code.model",
        "peekOfCode": "class Attention(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        # 根据是否指定n_kv_heads，确定用于键（key）和值（value）的头的数量。\n        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n        # 确保总头数可以被键值头数整除。\n        assert args.n_heads % self.n_kv_heads == 0\n        # 模型并行处理大小，默认为1。\n        model_parallel_size = 1\n        # 本地计算头数，等于总头数除以模型并行处理大小。",
        "detail": "tiny-universe-main.content.TinyLLM.code.model",
        "documentation": {}
    },
    {
        "label": "MLP",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyLLM.code.model",
        "description": "tiny-universe-main.content.TinyLLM.code.model",
        "peekOfCode": "class MLP(nn.Module):\n    def __init__(self, dim: int, hidden_dim: int, multiple_of: int, dropout: float):\n        super().__init__()\n        # 如果没有指定隐藏层的维度，我们将其设置为输入维度的4倍\n        # 然后将其减少到2/3，最后确保它是multiple_of的倍数\n        if hidden_dim is None:\n            hidden_dim = 4 * dim\n            hidden_dim = int(2 * hidden_dim / 3)\n            hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n        # 定义第一层线性变换，从输入维度到隐藏维度",
        "detail": "tiny-universe-main.content.TinyLLM.code.model",
        "documentation": {}
    },
    {
        "label": "DecoderLayer",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyLLM.code.model",
        "description": "tiny-universe-main.content.TinyLLM.code.model",
        "peekOfCode": "class DecoderLayer(nn.Module):\n    def __init__(self, layer_id: int, args: ModelArgs):\n        super().__init__()\n        # 定义多头注意力的头数\n        self.n_heads = args.n_heads\n        # 定义输入维度\n        self.dim = args.dim\n        # 定义每个头的维度，等于输入维度除以头数\n        self.head_dim = args.dim // args.n_heads\n        # 定义LLaMA2Attention对象，用于进行多头注意力计算",
        "detail": "tiny-universe-main.content.TinyLLM.code.model",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyLLM.code.model",
        "description": "tiny-universe-main.content.TinyLLM.code.model",
        "peekOfCode": "class Transformer(nn.Module):\n    last_loss: Optional[torch.Tensor]\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        # 初始化模型参数\n        self.args = args\n        # 词汇表大小\n        self.vocab_size = args.vocab_size\n        # 层数\n        self.n_layers = args.n_layers",
        "detail": "tiny-universe-main.content.TinyLLM.code.model",
        "documentation": {}
    },
    {
        "label": "precompute_freqs_cis",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyLLM.code.model",
        "description": "tiny-universe-main.content.TinyLLM.code.model",
        "peekOfCode": "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n    # torch.arange(0, dim, 2)[: (dim // 2)].float()生成了一个从0开始，步长为2的序列，长度为dim的一半\n    # 然后每个元素除以dim，再取theta的倒数，得到频率\n    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n    # 生成一个从0到end的序列，长度为end\n    t = torch.arange(end, device=freqs.device)\n    # 计算外积，得到一个二维矩阵，每一行是t的元素乘以freqs的元素\n    freqs = torch.outer(t, freqs).float()\n    # 计算频率的余弦值，得到实部\n    freqs_cos = torch.cos(freqs)",
        "detail": "tiny-universe-main.content.TinyLLM.code.model",
        "documentation": {}
    },
    {
        "label": "reshape_for_broadcast",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyLLM.code.model",
        "description": "tiny-universe-main.content.TinyLLM.code.model",
        "peekOfCode": "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n    # 获取x的维度数\n    ndim = x.ndim\n    # 断言，确保1在x的维度范围内\n    assert 0 <= 1 < ndim\n    # 断言，确保freqs_cis的形状与x的第二维和最后一维相同\n    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n    # 构造一个新的形状，除了第二维和最后一维，其他维度都为1，这样做是为了能够将freqs_cis与x进行广播操作\n    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n    # 将freqs_cis调整为新的形状，并返回",
        "detail": "tiny-universe-main.content.TinyLLM.code.model",
        "documentation": {}
    },
    {
        "label": "apply_rotary_emb",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyLLM.code.model",
        "description": "tiny-universe-main.content.TinyLLM.code.model",
        "peekOfCode": "def apply_rotary_emb(\n    xq: torch.Tensor,\n    xk: torch.Tensor,\n    freqs_cos: torch.Tensor,\n    freqs_sin: torch.Tensor\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    # 将查询和键张量转换为浮点数，并重塑形状以分离实部和虚部\n    xq_r, xq_i = xq.float().reshape(xq.shape[:-1] + (-1, 2)).unbind(-1)\n    xk_r, xk_i = xk.float().reshape(xk.shape[:-1] + (-1, 2)).unbind(-1)\n    # 重新塑形频率张量以进行广播",
        "detail": "tiny-universe-main.content.TinyLLM.code.model",
        "documentation": {}
    },
    {
        "label": "repeat_kv",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyLLM.code.model",
        "description": "tiny-universe-main.content.TinyLLM.code.model",
        "peekOfCode": "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n    # 获取输入张量的形状：批量大小、序列长度、键/值对头的数量、每个头的维度大小\n    bs, slen, n_kv_heads, head_dim = x.shape\n    # 如果重复次数为1，则不需要重复，直接返回原始张量\n    if n_rep == 1:\n        return x\n    # 对张量进行扩展和重塑操作以重复键值对\n    return (\n        x[:, :, :, None, :]  # 在第四个维度（头的维度前）添加一个新的维度\n        .expand(bs, slen, n_kv_heads, n_rep, head_dim)  # 将新添加的维度扩展到n_rep大小，实现重复的效果",
        "detail": "tiny-universe-main.content.TinyLLM.code.model",
        "documentation": {}
    },
    {
        "label": "PretokDataset",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "description": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "peekOfCode": "class PretokDataset(torch.utils.data.IterableDataset):\n    \"\"\"从磁盘加载已预处理的分词数据，并将其以 PyTorch 张量的形式返回。\"\"\"\n    def __init__(self, split, max_seq_len, vocab_size, vocab_source):\n        \"\"\"\n        初始化数据集。\n        参数:\n        split: str, 数据集的分割方式（'train' 或 'test'）。\n        max_seq_len: int, 最大序列长度，用于生成输入输出序列。\n        vocab_size: int, 词汇表的大小。\n        vocab_source: str, 词汇表的来源（'llama2' 或 'custom'）。",
        "detail": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "documentation": {}
    },
    {
        "label": "Task",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "description": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "peekOfCode": "class Task:\n    @staticmethod\n    def iter_batches(batch_size, device, num_workers=0, **dataset_kwargs):\n        ds = PretokDataset(**dataset_kwargs)\n        dl = torch.utils.data.DataLoader(\n            ds, batch_size=batch_size, pin_memory=True, num_workers=num_workers\n        )\n        for x, y in dl:\n            x = x.to(device, non_blocking=True)\n            y = y.to(device, non_blocking=True)",
        "detail": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "documentation": {}
    },
    {
        "label": "process_shard",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "description": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "peekOfCode": "def process_shard(args, vocab_size, tokenizer_model_path):\n    \"\"\"\n    处理数据分片，将其中的文本进行分词并保存为二进制文件。\n    参数:\n    args: tuple, 包含分片ID和分片文件名\n    vocab_size: int, 词汇表大小，用于决定输出文件存储路径\n    \"\"\"\n    # 提取分片ID和文件名\n    shard_id, shard = args\n    # 初始化分词器",
        "detail": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "documentation": {}
    },
    {
        "label": "pretokenize",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "description": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "peekOfCode": "def pretokenize(vocab_size):\n    \"\"\"\n    预处理所有的数据分片，并将分词后的数据保存为二进制文件。\n    参数:\n    vocab_size: int, 词汇表大小，用于决定输出文件存储路径\n    \"\"\"\n    # 数据所在目录\n    data_dir = os.path.join(DATA_CACHE_DIR, \"TinyStories_all_data\")\n    # 获取所有JSON文件的文件名列表，并按字典序排序\n    shard_filenames = sorted(glob.glob(os.path.join(data_dir, \"*.json\")))",
        "detail": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "documentation": {}
    },
    {
        "label": "DATA_CACHE_DIR",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "description": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "peekOfCode": "DATA_CACHE_DIR = 'data'\nTOKENIZER_MODEL = \"./data/tok4096.model\"\n# 定义分片处理函数\ndef process_shard(args, vocab_size, tokenizer_model_path):\n    \"\"\"\n    处理数据分片，将其中的文本进行分词并保存为二进制文件。\n    参数:\n    args: tuple, 包含分片ID和分片文件名\n    vocab_size: int, 词汇表大小，用于决定输出文件存储路径\n    \"\"\"",
        "detail": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "documentation": {}
    },
    {
        "label": "TOKENIZER_MODEL",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "description": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "peekOfCode": "TOKENIZER_MODEL = \"./data/tok4096.model\"\n# 定义分片处理函数\ndef process_shard(args, vocab_size, tokenizer_model_path):\n    \"\"\"\n    处理数据分片，将其中的文本进行分词并保存为二进制文件。\n    参数:\n    args: tuple, 包含分片ID和分片文件名\n    vocab_size: int, 词汇表大小，用于决定输出文件存储路径\n    \"\"\"\n    # 提取分片ID和文件名",
        "detail": "tiny-universe-main.content.TinyLLM.code.preprocess",
        "documentation": {}
    },
    {
        "label": "TextGenerator",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyLLM.code.sample",
        "description": "tiny-universe-main.content.TinyLLM.code.sample",
        "peekOfCode": "class TextGenerator:\n    def __init__(self, \n                 checkpoint='output/ckpt.pt',  # 模型检查点路径\n                 tokenizer_model_path='tok4096.model',  # 分词器模型路径\n                 seed=1337,  # 随机种子，确保可重复性\n                 device=None,  # 设备，优先使用 CUDA，如果没有可用的 CUDA，则使用 CPU\n                 dtype=\"float32\"):  # 数据类型，默认为 float32，可以选择 float16 或 bfloat16\n        \"\"\"\n        初始化 TextGenerator 类，加载模型、设置设备和分词器等。\n        \"\"\"",
        "detail": "tiny-universe-main.content.TinyLLM.code.sample",
        "documentation": {}
    },
    {
        "label": "Tokenizer",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyLLM.code.tokenizer",
        "description": "tiny-universe-main.content.TinyLLM.code.tokenizer",
        "peekOfCode": "class Tokenizer:\n    def __init__(self, tokenizer_model=None):\n        \"\"\"\n        初始化分词器。加载预训练的SentencePiece模型，并设置一些特殊的token ID。\n        参数:\n        tokenizer_model: str, 可选，分词器模型的路径，如果不指定则使用默认路径 TOKENIZER_MODEL。\n        \"\"\"\n        # 如果提供了分词器模型路径，使用该路径；否则使用默认模型路径\n        model_path = tokenizer_model if tokenizer_model else TOKENIZER_MODEL\n        # 确保模型文件存在",
        "detail": "tiny-universe-main.content.TinyLLM.code.tokenizer",
        "documentation": {}
    },
    {
        "label": "TOKENIZER_MODEL",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.tokenizer",
        "description": "tiny-universe-main.content.TinyLLM.code.tokenizer",
        "peekOfCode": "TOKENIZER_MODEL = \"./data/tok4096.model\"\nclass Tokenizer:\n    def __init__(self, tokenizer_model=None):\n        \"\"\"\n        初始化分词器。加载预训练的SentencePiece模型，并设置一些特殊的token ID。\n        参数:\n        tokenizer_model: str, 可选，分词器模型的路径，如果不指定则使用默认路径 TOKENIZER_MODEL。\n        \"\"\"\n        # 如果提供了分词器模型路径，使用该路径；否则使用默认模型路径\n        model_path = tokenizer_model if tokenizer_model else TOKENIZER_MODEL",
        "detail": "tiny-universe-main.content.TinyLLM.code.tokenizer",
        "documentation": {}
    },
    {
        "label": "estimate_loss",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "def estimate_loss():\n    out = {}  # 用于存储训练集和验证集上的平均损失\n    model.eval()  # 将模型设置为评估模式，这会影响 dropout 和 batchnorm 等层的行为\n    for split in [\"train\", \"val\"]:  # 分别对训练集和验证集进行评估\n        batch_iter = iter_batches(split=split)  # 获取对应数据集的批次迭代器\n        losses = torch.zeros(eval_iters)  # 初始化一个张量用于存储多次迭代的损失，放在 CPU 上\n        for k in range(eval_iters):  # 进行多次迭代以计算平均损失\n            X, Y = next(batch_iter)  # 从迭代器中获取下一个批次的输入数据 X 和标签 Y\n            with ctx:  # 上下文管理器，可以是 torch.autocast()，用于自动混合精度训练\n                logits = model(X, Y)  # 前向传播，计算模型的输出",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "get_lr",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "def get_lr(it):\n    \"\"\"\n    根据当前的训练迭代步数 it 返回当前的学习率值。\n    学习率调整策略包括线性预热、余弦退火和最小学习率限制。\n    \"\"\"\n    # 1) 线性预热阶段，在 warmup_iters 之前，学习率线性增加到目标学习率\n    if it < warmup_iters:\n        return learning_rate * it / warmup_iters  # 预热阶段，学习率线性增长\n    # 2) 如果迭代步数超过 lr_decay_iters，返回最小学习率 min_lr\n    if it > lr_decay_iters:",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "out_dir",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "out_dir = \"output\"  # 模型输出保存路径\neval_interval = 2000  # 评估间隔步数\nlog_interval = 1  # 日志记录间隔步数\neval_iters = 100  # 每次评估时迭代的步数\neval_only = False  # 如果为True，脚本在第一次评估后立即退出\nalways_save_checkpoint = False  # 如果为True，在每次评估后总是保存检查点\ninit_from = \"scratch\"  # 可以选择从头开始训练（'scratch'）或从已有的检查点恢复（'resume'）\n# 数据配置\nbatch_size = 8  # 每个微批次的样本数量，如果使用梯度累积，实际批次大小将更大\nmax_seq_len = 256  # 最大序列长度",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "eval_interval",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "eval_interval = 2000  # 评估间隔步数\nlog_interval = 1  # 日志记录间隔步数\neval_iters = 100  # 每次评估时迭代的步数\neval_only = False  # 如果为True，脚本在第一次评估后立即退出\nalways_save_checkpoint = False  # 如果为True，在每次评估后总是保存检查点\ninit_from = \"scratch\"  # 可以选择从头开始训练（'scratch'）或从已有的检查点恢复（'resume'）\n# 数据配置\nbatch_size = 8  # 每个微批次的样本数量，如果使用梯度累积，实际批次大小将更大\nmax_seq_len = 256  # 最大序列长度\nvocab_size = 4096  # 自定义词汇表大小",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "log_interval",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "log_interval = 1  # 日志记录间隔步数\neval_iters = 100  # 每次评估时迭代的步数\neval_only = False  # 如果为True，脚本在第一次评估后立即退出\nalways_save_checkpoint = False  # 如果为True，在每次评估后总是保存检查点\ninit_from = \"scratch\"  # 可以选择从头开始训练（'scratch'）或从已有的检查点恢复（'resume'）\n# 数据配置\nbatch_size = 8  # 每个微批次的样本数量，如果使用梯度累积，实际批次大小将更大\nmax_seq_len = 256  # 最大序列长度\nvocab_size = 4096  # 自定义词汇表大小\n# 模型配置",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "eval_iters",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "eval_iters = 100  # 每次评估时迭代的步数\neval_only = False  # 如果为True，脚本在第一次评估后立即退出\nalways_save_checkpoint = False  # 如果为True，在每次评估后总是保存检查点\ninit_from = \"scratch\"  # 可以选择从头开始训练（'scratch'）或从已有的检查点恢复（'resume'）\n# 数据配置\nbatch_size = 8  # 每个微批次的样本数量，如果使用梯度累积，实际批次大小将更大\nmax_seq_len = 256  # 最大序列长度\nvocab_size = 4096  # 自定义词汇表大小\n# 模型配置\ndim = 288  # 模型的隐藏层维度",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "eval_only",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "eval_only = False  # 如果为True，脚本在第一次评估后立即退出\nalways_save_checkpoint = False  # 如果为True，在每次评估后总是保存检查点\ninit_from = \"scratch\"  # 可以选择从头开始训练（'scratch'）或从已有的检查点恢复（'resume'）\n# 数据配置\nbatch_size = 8  # 每个微批次的样本数量，如果使用梯度累积，实际批次大小将更大\nmax_seq_len = 256  # 最大序列长度\nvocab_size = 4096  # 自定义词汇表大小\n# 模型配置\ndim = 288  # 模型的隐藏层维度\nn_layers = 8  # Transformer的层数",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "always_save_checkpoint",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "always_save_checkpoint = False  # 如果为True，在每次评估后总是保存检查点\ninit_from = \"scratch\"  # 可以选择从头开始训练（'scratch'）或从已有的检查点恢复（'resume'）\n# 数据配置\nbatch_size = 8  # 每个微批次的样本数量，如果使用梯度累积，实际批次大小将更大\nmax_seq_len = 256  # 最大序列长度\nvocab_size = 4096  # 自定义词汇表大小\n# 模型配置\ndim = 288  # 模型的隐藏层维度\nn_layers = 8  # Transformer的层数\nn_heads = 8  # 注意力头的数量",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "init_from",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "init_from = \"scratch\"  # 可以选择从头开始训练（'scratch'）或从已有的检查点恢复（'resume'）\n# 数据配置\nbatch_size = 8  # 每个微批次的样本数量，如果使用梯度累积，实际批次大小将更大\nmax_seq_len = 256  # 最大序列长度\nvocab_size = 4096  # 自定义词汇表大小\n# 模型配置\ndim = 288  # 模型的隐藏层维度\nn_layers = 8  # Transformer的层数\nn_heads = 8  # 注意力头的数量\nn_kv_heads = 4  # 模型分组",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "batch_size = 8  # 每个微批次的样本数量，如果使用梯度累积，实际批次大小将更大\nmax_seq_len = 256  # 最大序列长度\nvocab_size = 4096  # 自定义词汇表大小\n# 模型配置\ndim = 288  # 模型的隐藏层维度\nn_layers = 8  # Transformer的层数\nn_heads = 8  # 注意力头的数量\nn_kv_heads = 4  # 模型分组\nmultiple_of = 32  # 在某些层的维度必须是该数的倍数\ndropout = 0.0  # Dropout概率",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "max_seq_len",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "max_seq_len = 256  # 最大序列长度\nvocab_size = 4096  # 自定义词汇表大小\n# 模型配置\ndim = 288  # 模型的隐藏层维度\nn_layers = 8  # Transformer的层数\nn_heads = 8  # 注意力头的数量\nn_kv_heads = 4  # 模型分组\nmultiple_of = 32  # 在某些层的维度必须是该数的倍数\ndropout = 0.0  # Dropout概率\n# AdamW优化器配置",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "vocab_size",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "vocab_size = 4096  # 自定义词汇表大小\n# 模型配置\ndim = 288  # 模型的隐藏层维度\nn_layers = 8  # Transformer的层数\nn_heads = 8  # 注意力头的数量\nn_kv_heads = 4  # 模型分组\nmultiple_of = 32  # 在某些层的维度必须是该数的倍数\ndropout = 0.0  # Dropout概率\n# AdamW优化器配置\ngradient_accumulation_steps = 4  # 梯度累积步数，用于模拟更大的批次",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "dim",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "dim = 288  # 模型的隐藏层维度\nn_layers = 8  # Transformer的层数\nn_heads = 8  # 注意力头的数量\nn_kv_heads = 4  # 模型分组\nmultiple_of = 32  # 在某些层的维度必须是该数的倍数\ndropout = 0.0  # Dropout概率\n# AdamW优化器配置\ngradient_accumulation_steps = 4  # 梯度累积步数，用于模拟更大的批次\nlearning_rate = 5e-4  # 最大学习率\nmax_iters = 100000  # 总的训练迭代次数",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "n_layers",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "n_layers = 8  # Transformer的层数\nn_heads = 8  # 注意力头的数量\nn_kv_heads = 4  # 模型分组\nmultiple_of = 32  # 在某些层的维度必须是该数的倍数\ndropout = 0.0  # Dropout概率\n# AdamW优化器配置\ngradient_accumulation_steps = 4  # 梯度累积步数，用于模拟更大的批次\nlearning_rate = 5e-4  # 最大学习率\nmax_iters = 100000  # 总的训练迭代次数\nweight_decay = 1e-1  # 权重衰减系数",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "n_heads",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "n_heads = 8  # 注意力头的数量\nn_kv_heads = 4  # 模型分组\nmultiple_of = 32  # 在某些层的维度必须是该数的倍数\ndropout = 0.0  # Dropout概率\n# AdamW优化器配置\ngradient_accumulation_steps = 4  # 梯度累积步数，用于模拟更大的批次\nlearning_rate = 5e-4  # 最大学习率\nmax_iters = 100000  # 总的训练迭代次数\nweight_decay = 1e-1  # 权重衰减系数\nbeta1 = 0.9  # AdamW优化器的β1参数",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "n_kv_heads",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "n_kv_heads = 4  # 模型分组\nmultiple_of = 32  # 在某些层的维度必须是该数的倍数\ndropout = 0.0  # Dropout概率\n# AdamW优化器配置\ngradient_accumulation_steps = 4  # 梯度累积步数，用于模拟更大的批次\nlearning_rate = 5e-4  # 最大学习率\nmax_iters = 100000  # 总的训练迭代次数\nweight_decay = 1e-1  # 权重衰减系数\nbeta1 = 0.9  # AdamW优化器的β1参数\nbeta2 = 0.95  # AdamW优化器的β2参数",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "multiple_of",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "multiple_of = 32  # 在某些层的维度必须是该数的倍数\ndropout = 0.0  # Dropout概率\n# AdamW优化器配置\ngradient_accumulation_steps = 4  # 梯度累积步数，用于模拟更大的批次\nlearning_rate = 5e-4  # 最大学习率\nmax_iters = 100000  # 总的训练迭代次数\nweight_decay = 1e-1  # 权重衰减系数\nbeta1 = 0.9  # AdamW优化器的β1参数\nbeta2 = 0.95  # AdamW优化器的β2参数\ngrad_clip = 1.0  # 梯度裁剪阈值，0表示不裁剪",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "dropout",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "dropout = 0.0  # Dropout概率\n# AdamW优化器配置\ngradient_accumulation_steps = 4  # 梯度累积步数，用于模拟更大的批次\nlearning_rate = 5e-4  # 最大学习率\nmax_iters = 100000  # 总的训练迭代次数\nweight_decay = 1e-1  # 权重衰减系数\nbeta1 = 0.9  # AdamW优化器的β1参数\nbeta2 = 0.95  # AdamW优化器的β2参数\ngrad_clip = 1.0  # 梯度裁剪阈值，0表示不裁剪\n# 学习率衰减配置",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "gradient_accumulation_steps",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "gradient_accumulation_steps = 4  # 梯度累积步数，用于模拟更大的批次\nlearning_rate = 5e-4  # 最大学习率\nmax_iters = 100000  # 总的训练迭代次数\nweight_decay = 1e-1  # 权重衰减系数\nbeta1 = 0.9  # AdamW优化器的β1参数\nbeta2 = 0.95  # AdamW优化器的β2参数\ngrad_clip = 1.0  # 梯度裁剪阈值，0表示不裁剪\n# 学习率衰减配置\ndecay_lr = True  # 是否启用学习率衰减\nwarmup_iters = 1000  # 学习率预热的步数",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "learning_rate",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "learning_rate = 5e-4  # 最大学习率\nmax_iters = 100000  # 总的训练迭代次数\nweight_decay = 1e-1  # 权重衰减系数\nbeta1 = 0.9  # AdamW优化器的β1参数\nbeta2 = 0.95  # AdamW优化器的β2参数\ngrad_clip = 1.0  # 梯度裁剪阈值，0表示不裁剪\n# 学习率衰减配置\ndecay_lr = True  # 是否启用学习率衰减\nwarmup_iters = 1000  # 学习率预热的步数\n# 系统设置",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "max_iters",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "max_iters = 100000  # 总的训练迭代次数\nweight_decay = 1e-1  # 权重衰减系数\nbeta1 = 0.9  # AdamW优化器的β1参数\nbeta2 = 0.95  # AdamW优化器的β2参数\ngrad_clip = 1.0  # 梯度裁剪阈值，0表示不裁剪\n# 学习率衰减配置\ndecay_lr = True  # 是否启用学习率衰减\nwarmup_iters = 1000  # 学习率预热的步数\n# 系统设置\ndevice = \"cuda:0\"  # 设备选择：'cpu'，'cuda'，'cuda:0'等",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "weight_decay",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "weight_decay = 1e-1  # 权重衰减系数\nbeta1 = 0.9  # AdamW优化器的β1参数\nbeta2 = 0.95  # AdamW优化器的β2参数\ngrad_clip = 1.0  # 梯度裁剪阈值，0表示不裁剪\n# 学习率衰减配置\ndecay_lr = True  # 是否启用学习率衰减\nwarmup_iters = 1000  # 学习率预热的步数\n# 系统设置\ndevice = \"cuda:0\"  # 设备选择：'cpu'，'cuda'，'cuda:0'等\ndtype = \"bfloat16\"  # 数据类型：'float32'，'bfloat16'，'float16'",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "beta1",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "beta1 = 0.9  # AdamW优化器的β1参数\nbeta2 = 0.95  # AdamW优化器的β2参数\ngrad_clip = 1.0  # 梯度裁剪阈值，0表示不裁剪\n# 学习率衰减配置\ndecay_lr = True  # 是否启用学习率衰减\nwarmup_iters = 1000  # 学习率预热的步数\n# 系统设置\ndevice = \"cuda:0\"  # 设备选择：'cpu'，'cuda'，'cuda:0'等\ndtype = \"bfloat16\"  # 数据类型：'float32'，'bfloat16'，'float16'\n# -----------------------------------------------------------------------------",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "beta2",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "beta2 = 0.95  # AdamW优化器的β2参数\ngrad_clip = 1.0  # 梯度裁剪阈值，0表示不裁剪\n# 学习率衰减配置\ndecay_lr = True  # 是否启用学习率衰减\nwarmup_iters = 1000  # 学习率预热的步数\n# 系统设置\ndevice = \"cuda:0\"  # 设备选择：'cpu'，'cuda'，'cuda:0'等\ndtype = \"bfloat16\"  # 数据类型：'float32'，'bfloat16'，'float16'\n# -----------------------------------------------------------------------------\n# 获取配置参数的键值对，便于后续的日志记录",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "grad_clip",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "grad_clip = 1.0  # 梯度裁剪阈值，0表示不裁剪\n# 学习率衰减配置\ndecay_lr = True  # 是否启用学习率衰减\nwarmup_iters = 1000  # 学习率预热的步数\n# 系统设置\ndevice = \"cuda:0\"  # 设备选择：'cpu'，'cuda'，'cuda:0'等\ndtype = \"bfloat16\"  # 数据类型：'float32'，'bfloat16'，'float16'\n# -----------------------------------------------------------------------------\n# 获取配置参数的键值对，便于后续的日志记录\nconfig_keys = [",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "decay_lr",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "decay_lr = True  # 是否启用学习率衰减\nwarmup_iters = 1000  # 学习率预热的步数\n# 系统设置\ndevice = \"cuda:0\"  # 设备选择：'cpu'，'cuda'，'cuda:0'等\ndtype = \"bfloat16\"  # 数据类型：'float32'，'bfloat16'，'float16'\n# -----------------------------------------------------------------------------\n# 获取配置参数的键值对，便于后续的日志记录\nconfig_keys = [\n    k\n    for k, v in globals().items()",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "warmup_iters",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "warmup_iters = 1000  # 学习率预热的步数\n# 系统设置\ndevice = \"cuda:0\"  # 设备选择：'cpu'，'cuda'，'cuda:0'等\ndtype = \"bfloat16\"  # 数据类型：'float32'，'bfloat16'，'float16'\n# -----------------------------------------------------------------------------\n# 获取配置参数的键值对，便于后续的日志记录\nconfig_keys = [\n    k\n    for k, v in globals().items()\n    if not k.startswith(\"_\") and isinstance(v, (int, float, bool, str))",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "device = \"cuda:0\"  # 设备选择：'cpu'，'cuda'，'cuda:0'等\ndtype = \"bfloat16\"  # 数据类型：'float32'，'bfloat16'，'float16'\n# -----------------------------------------------------------------------------\n# 获取配置参数的键值对，便于后续的日志记录\nconfig_keys = [\n    k\n    for k, v in globals().items()\n    if not k.startswith(\"_\") and isinstance(v, (int, float, bool, str))\n]\nconfig = {k: globals()[k] for k in config_keys}  # 保存配置到字典中，便于日志记录",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "dtype",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "dtype = \"bfloat16\"  # 数据类型：'float32'，'bfloat16'，'float16'\n# -----------------------------------------------------------------------------\n# 获取配置参数的键值对，便于后续的日志记录\nconfig_keys = [\n    k\n    for k, v in globals().items()\n    if not k.startswith(\"_\") and isinstance(v, (int, float, bool, str))\n]\nconfig = {k: globals()[k] for k in config_keys}  # 保存配置到字典中，便于日志记录\n# -----------------------------------------------------------------------------",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "config_keys",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "config_keys = [\n    k\n    for k, v in globals().items()\n    if not k.startswith(\"_\") and isinstance(v, (int, float, bool, str))\n]\nconfig = {k: globals()[k] for k in config_keys}  # 保存配置到字典中，便于日志记录\n# -----------------------------------------------------------------------------\n# 固定一些超参数的默认值\nlr_decay_iters = max_iters  # 学习率衰减步数，设置为等于最大迭代步数\nmin_lr = 0.0  # 最小学习率，建议为学习率的十分之一",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "config = {k: globals()[k] for k in config_keys}  # 保存配置到字典中，便于日志记录\n# -----------------------------------------------------------------------------\n# 固定一些超参数的默认值\nlr_decay_iters = max_iters  # 学习率衰减步数，设置为等于最大迭代步数\nmin_lr = 0.0  # 最小学习率，建议为学习率的十分之一\nvocab_source = 'custom'  # 词汇表来源\nmaster_process = True  # 用于区分主进程\nseed_offset = 0  # 随机种子偏移量\nddp_world_size = 1  # 分布式数据并行的世界大小\ntokens_per_iter = batch_size * max_seq_len  # 每次迭代处理的token数",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "lr_decay_iters",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "lr_decay_iters = max_iters  # 学习率衰减步数，设置为等于最大迭代步数\nmin_lr = 0.0  # 最小学习率，建议为学习率的十分之一\nvocab_source = 'custom'  # 词汇表来源\nmaster_process = True  # 用于区分主进程\nseed_offset = 0  # 随机种子偏移量\nddp_world_size = 1  # 分布式数据并行的世界大小\ntokens_per_iter = batch_size * max_seq_len  # 每次迭代处理的token数\n# 设置随机种子，确保可重复性\ntorch.manual_seed(1337 + seed_offset)\ntorch.backends.cuda.matmul.allow_tf32 = True  # 允许在matmul上使用tf32",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "min_lr",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "min_lr = 0.0  # 最小学习率，建议为学习率的十分之一\nvocab_source = 'custom'  # 词汇表来源\nmaster_process = True  # 用于区分主进程\nseed_offset = 0  # 随机种子偏移量\nddp_world_size = 1  # 分布式数据并行的世界大小\ntokens_per_iter = batch_size * max_seq_len  # 每次迭代处理的token数\n# 设置随机种子，确保可重复性\ntorch.manual_seed(1337 + seed_offset)\ntorch.backends.cuda.matmul.allow_tf32 = True  # 允许在matmul上使用tf32\ntorch.backends.cudnn.allow_tf32 = True  # 允许在cudnn上使用tf32",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "vocab_source",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "vocab_source = 'custom'  # 词汇表来源\nmaster_process = True  # 用于区分主进程\nseed_offset = 0  # 随机种子偏移量\nddp_world_size = 1  # 分布式数据并行的世界大小\ntokens_per_iter = batch_size * max_seq_len  # 每次迭代处理的token数\n# 设置随机种子，确保可重复性\ntorch.manual_seed(1337 + seed_offset)\ntorch.backends.cuda.matmul.allow_tf32 = True  # 允许在matmul上使用tf32\ntorch.backends.cudnn.allow_tf32 = True  # 允许在cudnn上使用tf32\ndevice_type = \"cuda\" if \"cuda\" in device else \"cpu\"  # 用于自动选择设备类型",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "master_process",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "master_process = True  # 用于区分主进程\nseed_offset = 0  # 随机种子偏移量\nddp_world_size = 1  # 分布式数据并行的世界大小\ntokens_per_iter = batch_size * max_seq_len  # 每次迭代处理的token数\n# 设置随机种子，确保可重复性\ntorch.manual_seed(1337 + seed_offset)\ntorch.backends.cuda.matmul.allow_tf32 = True  # 允许在matmul上使用tf32\ntorch.backends.cudnn.allow_tf32 = True  # 允许在cudnn上使用tf32\ndevice_type = \"cuda\" if \"cuda\" in device else \"cpu\"  # 用于自动选择设备类型\nptdtype = torch.float16  # 设置训练时使用的数据类型",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "seed_offset",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "seed_offset = 0  # 随机种子偏移量\nddp_world_size = 1  # 分布式数据并行的世界大小\ntokens_per_iter = batch_size * max_seq_len  # 每次迭代处理的token数\n# 设置随机种子，确保可重复性\ntorch.manual_seed(1337 + seed_offset)\ntorch.backends.cuda.matmul.allow_tf32 = True  # 允许在matmul上使用tf32\ntorch.backends.cudnn.allow_tf32 = True  # 允许在cudnn上使用tf32\ndevice_type = \"cuda\" if \"cuda\" in device else \"cpu\"  # 用于自动选择设备类型\nptdtype = torch.float16  # 设置训练时使用的数据类型\n# 混合精度训练相关",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "ddp_world_size",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "ddp_world_size = 1  # 分布式数据并行的世界大小\ntokens_per_iter = batch_size * max_seq_len  # 每次迭代处理的token数\n# 设置随机种子，确保可重复性\ntorch.manual_seed(1337 + seed_offset)\ntorch.backends.cuda.matmul.allow_tf32 = True  # 允许在matmul上使用tf32\ntorch.backends.cudnn.allow_tf32 = True  # 允许在cudnn上使用tf32\ndevice_type = \"cuda\" if \"cuda\" in device else \"cpu\"  # 用于自动选择设备类型\nptdtype = torch.float16  # 设置训练时使用的数据类型\n# 混合精度训练相关\nctx = (",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "tokens_per_iter",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "tokens_per_iter = batch_size * max_seq_len  # 每次迭代处理的token数\n# 设置随机种子，确保可重复性\ntorch.manual_seed(1337 + seed_offset)\ntorch.backends.cuda.matmul.allow_tf32 = True  # 允许在matmul上使用tf32\ntorch.backends.cudnn.allow_tf32 = True  # 允许在cudnn上使用tf32\ndevice_type = \"cuda\" if \"cuda\" in device else \"cpu\"  # 用于自动选择设备类型\nptdtype = torch.float16  # 设置训练时使用的数据类型\n# 混合精度训练相关\nctx = (\n    nullcontext()",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "torch.backends.cuda.matmul.allow_tf32",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "torch.backends.cuda.matmul.allow_tf32 = True  # 允许在matmul上使用tf32\ntorch.backends.cudnn.allow_tf32 = True  # 允许在cudnn上使用tf32\ndevice_type = \"cuda\" if \"cuda\" in device else \"cpu\"  # 用于自动选择设备类型\nptdtype = torch.float16  # 设置训练时使用的数据类型\n# 混合精度训练相关\nctx = (\n    nullcontext()\n    if device_type == \"cpu\"\n    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n)",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "torch.backends.cudnn.allow_tf32",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "torch.backends.cudnn.allow_tf32 = True  # 允许在cudnn上使用tf32\ndevice_type = \"cuda\" if \"cuda\" in device else \"cpu\"  # 用于自动选择设备类型\nptdtype = torch.float16  # 设置训练时使用的数据类型\n# 混合精度训练相关\nctx = (\n    nullcontext()\n    if device_type == \"cpu\"\n    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n)\n# 为特定任务设置批次迭代器 iter_batches",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "device_type",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "device_type = \"cuda\" if \"cuda\" in device else \"cpu\"  # 用于自动选择设备类型\nptdtype = torch.float16  # 设置训练时使用的数据类型\n# 混合精度训练相关\nctx = (\n    nullcontext()\n    if device_type == \"cpu\"\n    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n)\n# 为特定任务设置批次迭代器 iter_batches\niter_batches = partial(",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "ptdtype",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "ptdtype = torch.float16  # 设置训练时使用的数据类型\n# 混合精度训练相关\nctx = (\n    nullcontext()\n    if device_type == \"cpu\"\n    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n)\n# 为特定任务设置批次迭代器 iter_batches\niter_batches = partial(\n    Task.iter_batches,  # 调用 Task 类中的 iter_batches 方法",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "ctx",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "ctx = (\n    nullcontext()\n    if device_type == \"cpu\"\n    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n)\n# 为特定任务设置批次迭代器 iter_batches\niter_batches = partial(\n    Task.iter_batches,  # 调用 Task 类中的 iter_batches 方法\n    batch_size=batch_size,  # 每个批次的样本数量\n    max_seq_len=max_seq_len,  # 每个序列的最大长度",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "iter_batches",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "iter_batches = partial(\n    Task.iter_batches,  # 调用 Task 类中的 iter_batches 方法\n    batch_size=batch_size,  # 每个批次的样本数量\n    max_seq_len=max_seq_len,  # 每个序列的最大长度\n    vocab_size=vocab_size,  # 词汇表大小\n    vocab_source=vocab_source,  # 词汇表来源（如 llama2 或 custom）\n    device=device,  # 运行模型的设备（如 GPU 或 CPU）\n    num_workers=0,  # 用于数据加载的 worker 数量，0 表示在主线程中加载\n)\n# 训练迭代数初始化",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "iter_num",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "iter_num = 0  # 记录当前迭代数\n# 验证集上的最好损失初始值设置为一个极大值，用于后续模型验证时对比更新\nbest_val_loss = 1e9  # 设置初始的最佳验证损失为非常大的值，以便在训练中更新\n# 模型初始化参数设置\nmodel_args = dict(\n    dim=dim,  # 模型的隐藏层维度\n    n_layers=n_layers,  # Transformer 的层数\n    n_heads=n_heads,  # 多头注意力机制中的头数\n    n_kv_heads=n_kv_heads,  # 分组数（可能是用于并行化或其他优化目的）\n    vocab_size=vocab_size,  # 词汇表大小",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "best_val_loss",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "best_val_loss = 1e9  # 设置初始的最佳验证损失为非常大的值，以便在训练中更新\n# 模型初始化参数设置\nmodel_args = dict(\n    dim=dim,  # 模型的隐藏层维度\n    n_layers=n_layers,  # Transformer 的层数\n    n_heads=n_heads,  # 多头注意力机制中的头数\n    n_kv_heads=n_kv_heads,  # 分组数（可能是用于并行化或其他优化目的）\n    vocab_size=vocab_size,  # 词汇表大小\n    multiple_of=multiple_of,  # 用于调整某些维度的参数，确保其为特定数的倍数\n    max_seq_len=max_seq_len,  # 最大序列长度",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "model_args",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "model_args = dict(\n    dim=dim,  # 模型的隐藏层维度\n    n_layers=n_layers,  # Transformer 的层数\n    n_heads=n_heads,  # 多头注意力机制中的头数\n    n_kv_heads=n_kv_heads,  # 分组数（可能是用于并行化或其他优化目的）\n    vocab_size=vocab_size,  # 词汇表大小\n    multiple_of=multiple_of,  # 用于调整某些维度的参数，确保其为特定数的倍数\n    max_seq_len=max_seq_len,  # 最大序列长度\n    dropout=dropout,  # dropout 概率，用于防止过拟合\n)",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "gptconf",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "gptconf = ModelArgs(**model_args)\nmodel = Transformer(gptconf)\nmodel.to(device)\n# 初始化 GradScaler，用于自动混合精度训练（AMP）\n# 如果 enabled=False，表示禁用混合精度，scaler 将不起作用\nscaler = torch.cuda.amp.GradScaler(enabled=(dtype == \"float16\"))\n# 优化器初始化，调用模型的 configure_optimizers 方法\noptimizer = model.configure_optimizers(\n    weight_decay,  # 权重衰减（L2 正则化）\n    learning_rate,  # 学习率",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "model = Transformer(gptconf)\nmodel.to(device)\n# 初始化 GradScaler，用于自动混合精度训练（AMP）\n# 如果 enabled=False，表示禁用混合精度，scaler 将不起作用\nscaler = torch.cuda.amp.GradScaler(enabled=(dtype == \"float16\"))\n# 优化器初始化，调用模型的 configure_optimizers 方法\noptimizer = model.configure_optimizers(\n    weight_decay,  # 权重衰减（L2 正则化）\n    learning_rate,  # 学习率\n    (beta1, beta2),  # Adam 优化器中的 beta1 和 beta2 参数",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "scaler",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == \"float16\"))\n# 优化器初始化，调用模型的 configure_optimizers 方法\noptimizer = model.configure_optimizers(\n    weight_decay,  # 权重衰减（L2 正则化）\n    learning_rate,  # 学习率\n    (beta1, beta2),  # Adam 优化器中的 beta1 和 beta2 参数\n    device_type  # 当前训练设备（如 GPU 或 CPU）\n)\n# 定义评估损失的流程\n@torch.no_grad()  # 使用 no_grad 装饰器，确保在评估过程中不计算梯度，从而节省内存",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "optimizer",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "optimizer = model.configure_optimizers(\n    weight_decay,  # 权重衰减（L2 正则化）\n    learning_rate,  # 学习率\n    (beta1, beta2),  # Adam 优化器中的 beta1 和 beta2 参数\n    device_type  # 当前训练设备（如 GPU 或 CPU）\n)\n# 定义评估损失的流程\n@torch.no_grad()  # 使用 no_grad 装饰器，确保在评估过程中不计算梯度，从而节省内存\ndef estimate_loss():\n    out = {}  # 用于存储训练集和验证集上的平均损失",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "train_batch_iter",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "train_batch_iter = iter_batches(split=\"train\")\nX, Y = next(train_batch_iter)  # 获取第一个批次的数据\nt0 = time.time()  # 记录开始时间\nlocal_iter_num = 0  # 本进程中的迭代次数\nraw_model = model  # 如果使用了分布式数据并行 (DDP)，需要解包模型\nrunning_mfu = -1.0  # 初始化模型浮点运算利用率\nos.makedirs(out_dir, exist_ok=True)\nwhile True:\n    # 或许当前step的学习率\n    lr = get_lr(iter_num) if decay_lr else learning_rate",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "t0",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "t0 = time.time()  # 记录开始时间\nlocal_iter_num = 0  # 本进程中的迭代次数\nraw_model = model  # 如果使用了分布式数据并行 (DDP)，需要解包模型\nrunning_mfu = -1.0  # 初始化模型浮点运算利用率\nos.makedirs(out_dir, exist_ok=True)\nwhile True:\n    # 或许当前step的学习率\n    lr = get_lr(iter_num) if decay_lr else learning_rate\n    # 更新优化器中的学习率\n    for param_group in optimizer.param_groups:",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "local_iter_num",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "local_iter_num = 0  # 本进程中的迭代次数\nraw_model = model  # 如果使用了分布式数据并行 (DDP)，需要解包模型\nrunning_mfu = -1.0  # 初始化模型浮点运算利用率\nos.makedirs(out_dir, exist_ok=True)\nwhile True:\n    # 或许当前step的学习率\n    lr = get_lr(iter_num) if decay_lr else learning_rate\n    # 更新优化器中的学习率\n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = lr",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "raw_model",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "raw_model = model  # 如果使用了分布式数据并行 (DDP)，需要解包模型\nrunning_mfu = -1.0  # 初始化模型浮点运算利用率\nos.makedirs(out_dir, exist_ok=True)\nwhile True:\n    # 或许当前step的学习率\n    lr = get_lr(iter_num) if decay_lr else learning_rate\n    # 更新优化器中的学习率\n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = lr\n    # 在指定的评估间隔进行模型评估和保存检查点",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "running_mfu",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train",
        "description": "tiny-universe-main.content.TinyLLM.code.train",
        "peekOfCode": "running_mfu = -1.0  # 初始化模型浮点运算利用率\nos.makedirs(out_dir, exist_ok=True)\nwhile True:\n    # 或许当前step的学习率\n    lr = get_lr(iter_num) if decay_lr else learning_rate\n    # 更新优化器中的学习率\n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = lr\n    # 在指定的评估间隔进行模型评估和保存检查点\n    if iter_num % eval_interval == 0 and master_process:",
        "detail": "tiny-universe-main.content.TinyLLM.code.train",
        "documentation": {}
    },
    {
        "label": "download_file",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "description": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "peekOfCode": "def download_file(url: str, fname: str, chunk_size=1024):\n    \"\"\"发送HTTP GET请求以流式方式获取文件\"\"\"\n    resp = requests.get(url, stream=True)\n    # 获取文件的总大小（以字节为单位），默认为0如果没有提供'content-length'头信息\n    total = int(resp.headers.get(\"content-length\", 0))\n    # 以写二进制模式打开一个文件以保存下载的内容\n    with open(fname, \"wb\") as file, tqdm(\n        desc=fname,           # 进度条前面的描述信息（通常是文件名）\n        total=total,          # 总的字节数，用于设置进度条的总长度\n        unit=\"iB\",            # 进度条的单位，'iB'代表二进制字节",
        "detail": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "documentation": {}
    },
    {
        "label": "download",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "description": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "peekOfCode": "def download():\n    \"\"\"在DATA_CACHE_DIR中创建目录，如果目录不存在则创建\"\"\"\n    os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n    # 定义TinyStories数据集的下载URL和保存的文件名\n    data_url = \"https://www.modelscope.cn/datasets/AI-ModelScope/TinyStories/resolve/master/TinyStories_all_data.tar.gz\"\n    data_filename = os.path.join(DATA_CACHE_DIR, \"TinyStories_all_data.tar.gz\")\n    # 检查数据集是否已经下载，如果没有下载则进行下载\n    if not os.path.exists(data_filename):\n        print(f\"Downloading {data_url} to {data_filename}...\")\n        download_file(data_url, data_filename)  # 使用之前定义的download_file函数进行下载",
        "detail": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "documentation": {}
    },
    {
        "label": "load_text_from_files",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "description": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "peekOfCode": "def load_text_from_files(path):\n    path_list = glob.glob(path)\n    text_data = []\n    for file_path in path_list:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            text_data.extend(file.readlines())\n    return text_data\ndef batch_iterator(text_data, batch_size=648):\n    for i in range(0, len(text_data), batch_size):\n        yield text_data[i:i + batch_size]",
        "detail": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "documentation": {}
    },
    {
        "label": "batch_iterator",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "description": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "peekOfCode": "def batch_iterator(text_data, batch_size=648):\n    for i in range(0, len(text_data), batch_size):\n        yield text_data[i:i + batch_size]\ndef train_vocab(vocab_size: int=32000, num_shards: int=20):\n    \"\"\"\n    vocab_size: int, 词汇表的大小，决定分词器的词汇量。\n    num_shards: int, 用于加快词汇表训练的效率，指定要处理的分片数量。\n    \"\"\"\n    # 确保词汇表大小为正数\n    assert vocab_size > 0, \"Vocab size must be positive\"",
        "detail": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "documentation": {}
    },
    {
        "label": "train_vocab",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "description": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "peekOfCode": "def train_vocab(vocab_size: int=32000, num_shards: int=20):\n    \"\"\"\n    vocab_size: int, 词汇表的大小，决定分词器的词汇量。\n    num_shards: int, 用于加快词汇表训练的效率，指定要处理的分片数量。\n    \"\"\"\n    # 确保词汇表大小为正数\n    assert vocab_size > 0, \"Vocab size must be positive\"\n    # SentencePiece 模型的前缀路径，将用于保存分词器\n    prefix = os.path.join(DATA_CACHE_DIR, f\"tok{vocab_size}\")\n    # 1) 将多个分片中的文本导出为单个文本文件 tiny.txt",
        "detail": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "documentation": {}
    },
    {
        "label": "DATA_CACHE_DIR",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "description": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "peekOfCode": "DATA_CACHE_DIR = 'data'\ndef download_file(url: str, fname: str, chunk_size=1024):\n    \"\"\"发送HTTP GET请求以流式方式获取文件\"\"\"\n    resp = requests.get(url, stream=True)\n    # 获取文件的总大小（以字节为单位），默认为0如果没有提供'content-length'头信息\n    total = int(resp.headers.get(\"content-length\", 0))\n    # 以写二进制模式打开一个文件以保存下载的内容\n    with open(fname, \"wb\") as file, tqdm(\n        desc=fname,           # 进度条前面的描述信息（通常是文件名）\n        total=total,          # 总的字节数，用于设置进度条的总长度",
        "detail": "tiny-universe-main.content.TinyLLM.code.train_vocab",
        "documentation": {}
    },
    {
        "label": "BaseEmbeddings",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "description": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "peekOfCode": "class BaseEmbeddings:\n    \"\"\"\n    Base class for embeddings\n    \"\"\"\n    def __init__(self, path: str, is_api: bool) -> None:\n        self.path = path\n        self.is_api = is_api\n    def get_embedding(self, text: str, model: str) -> List[float]:\n        raise NotImplementedError\n    @classmethod",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "documentation": {}
    },
    {
        "label": "OpenAIEmbedding",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "description": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "peekOfCode": "class OpenAIEmbedding(BaseEmbeddings):\n    \"\"\"\n    class for OpenAI embeddings\n    \"\"\"\n    def __init__(self, path: str = '', is_api: bool = True) -> None:\n        super().__init__(path, is_api)\n        if self.is_api:\n            from openai import OpenAI\n            self.client = OpenAI()\n            self.client.api_key = os.getenv(\"OPENAI_API_KEY\")",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "documentation": {}
    },
    {
        "label": "JinaEmbedding",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "description": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "peekOfCode": "class JinaEmbedding(BaseEmbeddings):\n    \"\"\"\n    class for Jina embeddings\n    \"\"\"\n    def __init__(self, path: str = 'jinaai/jina-embeddings-v2-base-zh', is_api: bool = False) -> None:\n        super().__init__(path, is_api)\n        self._model = self.load_model()\n    def get_embedding(self, text: str) -> List[float]:\n        return self._model.encode([text])[0].tolist()\n    def load_model(self):",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "documentation": {}
    },
    {
        "label": "ZhipuEmbedding",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "description": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "peekOfCode": "class ZhipuEmbedding(BaseEmbeddings):\n    \"\"\"\n    class for Zhipu embeddings\n    \"\"\"\n    def __init__(self, path: str = '', is_api: bool = True) -> None:\n        super().__init__(path, is_api)\n        if self.is_api:\n            from zhipuai import ZhipuAI\n            self.client = ZhipuAI(api_key=os.getenv(\"ZHIPUAI_API_KEY\")) \n    def get_embedding(self, text: str) -> List[float]:",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "documentation": {}
    },
    {
        "label": "DashscopeEmbedding",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "description": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "peekOfCode": "class DashscopeEmbedding(BaseEmbeddings):\n    \"\"\"\n    class for Dashscope embeddings\n    \"\"\"\n    def __init__(self, path: str = '', is_api: bool = True) -> None:\n        super().__init__(path, is_api)\n        if self.is_api:\n            import dashscope\n            dashscope.api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n            self.client = dashscope.TextEmbedding",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "documentation": {}
    },
    {
        "label": "os.environ['CURL_CA_BUNDLE']",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "description": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "peekOfCode": "os.environ['CURL_CA_BUNDLE'] = ''\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv())\nclass BaseEmbeddings:\n    \"\"\"\n    Base class for embeddings\n    \"\"\"\n    def __init__(self, path: str, is_api: bool) -> None:\n        self.path = path\n        self.is_api = is_api",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "documentation": {}
    },
    {
        "label": "_",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "description": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "peekOfCode": "_ = load_dotenv(find_dotenv())\nclass BaseEmbeddings:\n    \"\"\"\n    Base class for embeddings\n    \"\"\"\n    def __init__(self, path: str, is_api: bool) -> None:\n        self.path = path\n        self.is_api = is_api\n    def get_embedding(self, text: str, model: str) -> List[float]:\n        raise NotImplementedError",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.Embeddings",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "description": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "peekOfCode": "class BaseModel:\n    def __init__(self, path: str = '') -> None:\n        self.path = path\n    def chat(self, prompt: str, history: List[dict], content: str) -> str:\n        pass\n    def load_model(self):\n        pass\nclass OpenAIChat(BaseModel):\n    def __init__(self, path: str = '', model: str = \"gpt-3.5-turbo-1106\") -> None:\n        super().__init__(path)",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "documentation": {}
    },
    {
        "label": "OpenAIChat",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "description": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "peekOfCode": "class OpenAIChat(BaseModel):\n    def __init__(self, path: str = '', model: str = \"gpt-3.5-turbo-1106\") -> None:\n        super().__init__(path)\n        self.model = model\n    def chat(self, prompt: str, history: List[dict], content: str) -> str:\n        from openai import OpenAI\n        client = OpenAI()\n        client.api_key = os.getenv(\"OPENAI_API_KEY\")   \n        client.base_url = os.getenv(\"OPENAI_BASE_URL\")\n        history.append({'role': 'user', 'content': PROMPT_TEMPLATE['RAG_PROMPT_TEMPALTE'].format(question=prompt, context=content)})",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "documentation": {}
    },
    {
        "label": "InternLMChat",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "description": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "peekOfCode": "class InternLMChat(BaseModel):\n    def __init__(self, path: str = '') -> None:\n        super().__init__(path)\n        self.load_model()\n    def chat(self, prompt: str, history: List = [], content: str='') -> str:\n        prompt = PROMPT_TEMPLATE['InternLM_PROMPT_TEMPALTE'].format(question=prompt, context=content)\n        response, history = self.model.chat(self.tokenizer, prompt, history)\n        return response\n    def load_model(self):\n        import torch",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "documentation": {}
    },
    {
        "label": "DashscopeChat",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "description": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "peekOfCode": "class DashscopeChat(BaseModel):\n    def __init__(self, path: str = '', model: str = \"qwen-turbo\") -> None:\n        super().__init__(path)\n        self.model = model\n    def chat(self, prompt: str, history: List[Dict], content: str) -> str:\n        import dashscope\n        dashscope.api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n        history.append({'role': 'user', 'content': PROMPT_TEMPLATE['RAG_PROMPT_TEMPALTE'].format(question=prompt, context=content)})\n        response = dashscope.Generation.call(\n            model=self.model,",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "documentation": {}
    },
    {
        "label": "ZhipuChat",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "description": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "peekOfCode": "class ZhipuChat(BaseModel):\n    def __init__(self, path: str = '', model: str = \"glm-4\") -> None:\n        super().__init__(path)\n        from zhipuai import ZhipuAI\n        self.client = ZhipuAI(api_key=os.getenv(\"ZHIPUAI_API_KEY\"))\n        self.model = model\n    def chat(self, prompt: str, history: List[Dict], content: str) -> str:\n        history.append({'role': 'user', 'content': PROMPT_TEMPLATE['RAG_PROMPT_TEMPALTE'].format(question=prompt, context=content)})\n        response = self.client.chat.completions.create(\n            model=self.model,",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "documentation": {}
    },
    {
        "label": "PROMPT_TEMPLATE",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "description": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "peekOfCode": "PROMPT_TEMPLATE = dict(\n    RAG_PROMPT_TEMPALTE=\"\"\"使用以上下文来回答用户的问题。如果你不知道答案，就说你不知道。总是使用中文回答。\n        问题: {question}\n        可参考的上下文：\n        ···\n        {context}\n        ···\n        如果给定的上下文无法让你做出回答，请回答数据库中没有这个内容，你不知道。\n        有用的回答:\"\"\",\n    InternLM_PROMPT_TEMPALTE=\"\"\"先对上下文进行内容总结,再使用上下文来回答用户的问题。如果你不知道答案，就说你不知道。总是使用中文回答。",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.LLM",
        "documentation": {}
    },
    {
        "label": "ReadFiles",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.utils",
        "description": "tiny-universe-main.content.TinyRAG.RAG.utils",
        "peekOfCode": "class ReadFiles:\n    \"\"\"\n    class to read files\n    \"\"\"\n    def __init__(self, path: str) -> None:\n        self._path = path\n        self.file_list = self.get_files()\n    def get_files(self):\n        # args：dir_path，目标文件夹路径\n        file_list = []",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.utils",
        "documentation": {}
    },
    {
        "label": "Documents",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.utils",
        "description": "tiny-universe-main.content.TinyRAG.RAG.utils",
        "peekOfCode": "class Documents:\n    \"\"\"\n        获取已分好类的json格式文档\n    \"\"\"\n    def __init__(self, path: str = '') -> None:\n        self.path = path\n    def get_content(self):\n        with open(self.path, mode='r', encoding='utf-8') as f:\n            content = json.load(f)\n        return content",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.utils",
        "documentation": {}
    },
    {
        "label": "enc",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.utils",
        "description": "tiny-universe-main.content.TinyRAG.RAG.utils",
        "peekOfCode": "enc = tiktoken.get_encoding(\"cl100k_base\")\nclass ReadFiles:\n    \"\"\"\n    class to read files\n    \"\"\"\n    def __init__(self, path: str) -> None:\n        self._path = path\n        self.file_list = self.get_files()\n    def get_files(self):\n        # args：dir_path，目标文件夹路径",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.utils",
        "documentation": {}
    },
    {
        "label": "VectorStore",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyRAG.RAG.VectorBase",
        "description": "tiny-universe-main.content.TinyRAG.RAG.VectorBase",
        "peekOfCode": "class VectorStore:\n    def __init__(self, document: List[str] = ['']) -> None:\n        self.document = document\n    def get_vector(self, EmbeddingModel: BaseEmbeddings) -> List[List[float]]:\n        self.vectors = []\n        for doc in tqdm(self.document, desc=\"Calculating embeddings\"):\n            self.vectors.append(EmbeddingModel.get_embedding(doc))\n        return self.vectors\n    def persist(self, path: str = 'storage'):\n        if not os.path.exists(path):",
        "detail": "tiny-universe-main.content.TinyRAG.RAG.VectorBase",
        "documentation": {}
    },
    {
        "label": "model_dir",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyRAG.down_model",
        "description": "tiny-universe-main.content.TinyRAG.down_model",
        "peekOfCode": "model_dir = snapshot_download('Shanghai_AI_Laboratory/internlm2-chat-7b', cache_dir='/root/autodl-tmp/', revision='master')\nmodel_dir = snapshot_download('jinaai/jina-embeddings-v2-base-zh', cache_dir='/root/autodl-tmp/', revision='master')",
        "detail": "tiny-universe-main.content.TinyRAG.down_model",
        "documentation": {}
    },
    {
        "label": "model_dir",
        "kind": 5,
        "importPath": "tiny-universe-main.content.TinyRAG.down_model",
        "description": "tiny-universe-main.content.TinyRAG.down_model",
        "peekOfCode": "model_dir = snapshot_download('jinaai/jina-embeddings-v2-base-zh', cache_dir='/root/autodl-tmp/', revision='master')",
        "detail": "tiny-universe-main.content.TinyRAG.down_model",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttention",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "description": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "peekOfCode": "class MultiHeadAttention(nn.Module):\n    def __init__(self, config, is_causal=False):\n        # 构造函数\n        # config: 配置对象\n        super().__init__()\n        # 隐藏层维度必须是头数的整数倍\n        assert config.n_embd % config.n_head == 0\n        # Wq, Wk, Wv 参数矩阵，每个参数矩阵为 n_embd x n_embd\n        self.c_attns = nn.ModuleList([nn.Linear(config.n_embd, config.n_embd, bias=config.bias) for _ in range(3)])\n        # 输出的线性层，维度为 n_embd x n_embd",
        "detail": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "documentation": {}
    },
    {
        "label": "MLP",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "description": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "peekOfCode": "class MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        # Transformer 的全连接模块有两个线性层，中间加了一个 RELU 激活函数\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n        self.relu    = nn.ReLU()\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n        self.dropout = nn.Dropout(config.dropout)\n    def forward(self, x):\n        x = self.c_fc(x)",
        "detail": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "documentation": {}
    },
    {
        "label": "LayerNorm",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "description": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "peekOfCode": "class LayerNorm(nn.Module):\n    # 在 Pytorch 的 LayerNorm 基础上添加了偏置，因为 Pytorch 的 LayerNorm 不支持偏置为 None\n    def __init__(self, ndim, bias):\n        super().__init__()\n        # 初始化参数和偏置\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n    def forward(self, input):\n        # 直接调用 Pytorch 的 LayerNorm\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)",
        "detail": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "documentation": {}
    },
    {
        "label": "EncoderLayer",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "description": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "peekOfCode": "class EncoderLayer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        # 一个 Layer 中有两个 LayerNorm，分别在 Attention 之前和 MLP 之前\n        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n        # Encoder 不需要掩码，传入 is_causal=False\n        self.attn = MultiHeadAttention(config, is_causal=False)\n        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n        self.mlp = MLP(config)\n    def forward(self, x):",
        "detail": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "description": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "peekOfCode": "class Encoder(nn.Module):\n    def __init__(self, config):\n        super(Encoder, self).__init__() \n        # 一个 Encoder 由 N 个 Encoder Layer 组成\n        self.layers = nn.ModuleList([EncoderLayer(config) for _ in range(config.n_layer)])\n        self.norm = LayerNorm(config.n_embd, bias=config.bias)\n    def forward(self, x):\n        \"分别通过 N 层 Encoder Layer\"\n        for layer in self.layers:\n            x = layer(x)",
        "detail": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "documentation": {}
    },
    {
        "label": "DecoderLayer",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "description": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "peekOfCode": "class DecoderLayer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        # 一个 Layer 中有三个 LayerNorm，分别在 Mask Attention 之前、Self Attention 之前和 MLP 之前\n        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n        # Decoder 的第一个部分是 Mask Attention，传入 is_causal=True\n        self.m_attn = MultiHeadAttention(config, is_causal=True)\n        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n        # Decoder 的第二个部分是 类似于 Encoder 的 Attention，传入 is_causal=False\n        self.attn = MultiHeadAttention(config, is_causal=False)",
        "detail": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "description": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "peekOfCode": "class Decoder(nn.Module):\n    def __init__(self, config):\n        super(Decoder, self).__init__() \n        # 一个 Decoder 由 N 个 Decoder Layer 组成\n        self.layers = nn.ModuleList([DecoderLayer(config) for _ in range(config.n_layer)])\n        self.norm = LayerNorm(config.n_embd, bias=config.bias)\n    def forward(self, x, enc_out):\n        \"Pass the input (and mask) through each layer in turn.\"\n        for layer in self.layers:\n            x = layer(x, enc_out)",
        "detail": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "documentation": {}
    },
    {
        "label": "PositionalEncoding",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "description": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "peekOfCode": "class PositionalEncoding(nn.Module):\n    # 在输入上加入了位置编码\n    def __init__(self, config):\n        super(PositionalEncoding, self).__init__()\n        # Dropout 层\n        self.dropout = nn.Dropout(p=config.dropout)\n        # block size 是序列的最大长度\n        pe = torch.zeros(config.block_size, config.n_embd)\n        position = torch.arange(0, config.block_size).unsqueeze(1)\n        div_term = torch.exp(",
        "detail": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "kind": 6,
        "importPath": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "description": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "peekOfCode": "class Transformer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        # 必须输入词表大小和 block size\n        assert config.vocab_size is not None\n        assert config.block_size is not None\n        self.config = config\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = PositionalEncoding(config),",
        "detail": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "documentation": {}
    },
    {
        "label": "attention",
        "kind": 2,
        "importPath": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "description": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "peekOfCode": "def attention(q, k, v, dropout_module = None, is_causal=False, dropout=None, mask=None):\n    # 计算 QK^T / sqrt(d_k)，维度为 (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n    # 如果是解码器的 Casual LM，需要 mask 掉右上角的元素\n    if is_causal:\n        # 这里截取到序列长度，因为有些序列可能比 block_size 短\n        att = att.masked_fill(mask[:,:,:k.size(-2),:k.size(-2)] == 0, float('-inf'))\n    # 计算 softmax，维度为 (B, nh, T, T)\n    att = F.softmax(att, dim=-1)\n    # Attention Dropout",
        "detail": "tiny-universe-main.content.TinyTransformer.tiny_transformer",
        "documentation": {}
    }
]